🚀 Starting PTQ Experiment: qdrop + lsq + mobilenet_v2
==========================================
Parameters:
  Model: mobilenet_v2
  Advanced Mode: qdrop
  Quant Model: lsq
  Weight Bits: 4
  Activation Bits: 2
  Alpha: 0.5
  Clusters: 16
  PCA dim: 50
  Batch Size: 64
  Calib Batches: 32
  Logits Batches: 10
  Alpha List: 0.2 0.4 0.6 0.8 1.0
  Clusters List: 8 16 32 64
  PCA Dim List: 25 50 100
==========================================
🔄 Running experiment...
Time: Mon Aug 18 11:13:30 AM CEST 2025
------------------------------------------
2025-08-18 11:13:59,862 | INFO | Environment: {'torch': '2.8.0+cu128', 'torchvision': '0.23.0+cu128', 'cuda_available': True, 'device_name': 'NVIDIA L40S', 'capability': '8.9', 'cudnn_version': 91002}
2025-08-18 11:13:59,862 | INFO | ▶ START: load fp32 model (torchvision weights API)
2025-08-18 11:14:00,374 | INFO | Model: mobilenet_v2 | Weights: MobileNet_V2_Weights.IMAGENET1K_V2 | Params: 3.50M | Ref acc@1=None
2025-08-18 11:14:00,374 | INFO | ✔ END: load fp32 model (torchvision weights API) (elapsed 0.51s)
2025-08-18 11:14:00,374 | INFO | ▶ START: build & check loaders
2025-08-18 11:14:00,390 | INFO | Val structure looks OK (1000 synset folders).
2025-08-18 11:14:00,404 | INFO | Train structure looks OK (1000 synset folders).
2025-08-18 11:14:25,031 | INFO | ImageNet train: 1281167 images, 1000 classes
2025-08-18 11:14:26,925 | INFO | ImageNet val: 50000 images, 1000 classes
2025-08-18 11:14:26,925 | INFO | Val dataset size: 50000 | batch_size=64 | calib_batches=32
2025-08-18 11:14:33,753 | INFO | [SANITY] Batch[0] stats: mean=-0.2055, std=1.1229, min=-2.118, max=2.640
2025-08-18 11:14:33,754 | INFO | ✔ END: build & check loaders (elapsed 33.38s)
2025-08-18 11:14:33,761 | INFO | ▶ START: prepare_by_platform(Academic)
2025-08-18 11:14:33,762 | INFO | [Academic extra_config] {'extra_qconfig_dict': {'w_observer': 'MinMaxObserver', 'a_observer': 'EMAMinMaxObserver', 'w_fakequantize': 'AdaRoundFakeQuantize', 'a_fakequantize': 'FixedFakeQuantize', 'w_qscheme': {'bit': 4, 'symmetry': True, 'per_channel': True, 'pot_scale': False}, 'a_qscheme': {'bit': 2, 'symmetry': False, 'per_channel': False, 'pot_scale': False}}}
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MinMaxObserver Params: Symmetric: True / Bitwidth: 4 / Per channel: True / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: FixedFakeQuantize Params: {}
    Oberver:      EMAMinMaxObserver Params: Symmetric: False / Bitwidth: 2 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer classifier.1 to 8 bit.
[MQBENCH] INFO: Set layer features.0.0 to 8 bit.
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_1_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_1_conv_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_2_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_2_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_2_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_3_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_3_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_4_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_4_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_4_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_5_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_5_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_6_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_6_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_7_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_7_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_7_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_8_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_8_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_9_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_9_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_4_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_10_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_10_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_5_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_11_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_11_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_11_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_12_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_12_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_6_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_13_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_13_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_7_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_14_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_14_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_14_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_15_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_15_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_8_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_16_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_16_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_9_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_17_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_17_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_17_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Set classifier_0 post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant classifier_0_post_act_fake_quantizer
2025-08-18 11:14:34,416 | INFO | Modules (total): 213 -> 425
2025-08-18 11:14:34,417 | INFO | 'Quantish' modules detected after prepare: 212
2025-08-18 11:14:34,417 | INFO | ▶ START: calibration (enable_calibration + forward)
[MQBENCH] INFO: Enable observer and Disable quantize.
2025-08-18 11:14:43,585 | INFO | [CALIB] step=1/32 seen=64 (7.0 img/s)
2025-08-18 11:14:44,095 | INFO | [CALIB] step=10/32 seen=640 (66.1 img/s)
2025-08-18 11:14:47,122 | INFO | [CALIB] step=20/32 seen=1280 (100.8 img/s)
2025-08-18 11:14:49,006 | INFO | [CALIB] step=30/32 seen=1920 (131.6 img/s)
2025-08-18 11:14:52,667 | INFO | [CALIB] total images seen: 2048
2025-08-18 11:14:52,667 | INFO | ✔ END: calibration (enable_calibration + forward) (elapsed 18.25s)
2025-08-18 11:14:52,667 | INFO | ▶ START: advanced PTQ reconstruction
2025-08-18 11:14:55,271 | INFO | [ADV] cfg={'pattern': 'block', 'scale_lr': 4e-05, 'warm_up': 0.2, 'weight': 0.01, 'max_count': 20000, 'b_range': [20, 2], 'keep_gpu': True, 'round_mode': 'learned_hard_sigmoid', 'prob': 1.0}
2025-08-18 11:14:55,271 | INFO | [ADV] stacked tensors: 32 | total calib images: 2048
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare block reconstruction for features_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [x_post_act_fake_quantizer, features_0_0, features_0_1, features_0_2, features_0_2_post_act_fake_quantizer, features_1_conv_0_0, features_1_conv_0_1, features_1_conv_0_2, features_1_conv_0_2_post_act_fake_quantizer, features_1_conv_1, features_1_conv_2, features_1_conv_2_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, x_post_act_fake_quantizer):
    features_0_0 = getattr(getattr(self.features, "0"), "0")(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None
    features_0_1 = getattr(getattr(self.features, "0"), "1")(features_0_0);  features_0_0 = None
    features_0_2 = getattr(getattr(self.features, "0"), "2")(features_0_1);  features_0_1 = None
    features_0_2_post_act_fake_quantizer = self.features_0_2_post_act_fake_quantizer(features_0_2);  features_0_2 = None
    features_1_conv_0_0 = getattr(getattr(getattr(self.features, "1").conv, "0"), "0")(features_0_2_post_act_fake_quantizer);  features_0_2_post_act_fake_quantizer = None
    features_1_conv_0_1 = getattr(getattr(getattr(self.features, "1").conv, "0"), "1")(features_1_conv_0_0);  features_1_conv_0_0 = None
    features_1_conv_0_2 = getattr(getattr(getattr(self.features, "1").conv, "0"), "2")(features_1_conv_0_1);  features_1_conv_0_1 = None
    features_1_conv_0_2_post_act_fake_quantizer = self.features_1_conv_0_2_post_act_fake_quantizer(features_1_conv_0_2);  features_1_conv_0_2 = None
    features_1_conv_1 = getattr(getattr(self.features, "1").conv, "1")(features_1_conv_0_2_post_act_fake_quantizer);  features_1_conv_0_2_post_act_fake_quantizer = None
    features_1_conv_2 = getattr(getattr(self.features, "1").conv, "2")(features_1_conv_1);  features_1_conv_1 = None
    features_1_conv_2_post_act_fake_quantizer = self.features_1_conv_2_post_act_fake_quantizer(features_1_conv_2);  features_1_conv_2 = None
    return features_1_conv_2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_1_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_1_conv_2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
2025-08-18 11:15:05,613 | WARNING | /home/alz07xz/project/kmeans_results/MQBench/mqbench/advanced_ptq.py:220: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)
  float(total_loss), float(rec_loss), float(round_loss), b, self.count))

[MQBENCH] INFO: Total loss:	625.493 (rec:625.493, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	567.058 (rec:567.058, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	683.615 (rec:683.615, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	699.665 (rec:699.665, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	492.685 (rec:492.685, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	405.756 (rec:405.756, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	654.846 (rec:654.846, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	650.349 (rec:643.012, round:7.336)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	626.658 (rec:621.222, round:5.436)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	563.626 (rec:558.802, round:4.824)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	743.390 (rec:738.924, round:4.465)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	678.248 (rec:674.069, round:4.179)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	741.825 (rec:737.919, round:3.906)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	740.586 (rec:736.906, round:3.680)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	582.878 (rec:579.324, round:3.554)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	820.086 (rec:816.744, round:3.342)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	615.990 (rec:612.804, round:3.186)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	735.312 (rec:732.265, round:3.048)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	535.415 (rec:532.468, round:2.947)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	684.358 (rec:681.489, round:2.869)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	730.057 (rec:727.252, round:2.805)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	759.952 (rec:757.227, round:2.725)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	532.852 (rec:530.217, round:2.635)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	619.516 (rec:616.944, round:2.572)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	764.340 (rec:761.831, round:2.509)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	699.605 (rec:697.150, round:2.455)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	772.719 (rec:770.322, round:2.397)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	559.494 (rec:557.155, round:2.339)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	820.944 (rec:818.645, round:2.299)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	689.908 (rec:687.654, round:2.254)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	746.827 (rec:744.617, round:2.211)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	809.243 (rec:807.076, round:2.167)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	618.060 (rec:615.933, round:2.128)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	454.022 (rec:451.940, round:2.082)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	456.845 (rec:454.812, round:2.033)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	582.561 (rec:580.584, round:1.978)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	390.954 (rec:389.047, round:1.907)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	572.688 (rec:570.865, round:1.824)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	615.434 (rec:613.818, round:1.616)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	735.359 (rec:734.056, round:1.303)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_2_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_1_conv_2_post_act_fake_quantizer, features_2_conv_0_0, features_2_conv_0_1, features_2_conv_0_2, features_2_conv_0_2_post_act_fake_quantizer, features_2_conv_1_0, features_2_conv_1_1, features_2_conv_1_2, features_2_conv_1_2_post_act_fake_quantizer, features_2_conv_2, features_2_conv_3, features_2_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_1_conv_2_post_act_fake_quantizer):
    features_2_conv_0_0 = getattr(getattr(getattr(self.features, "2").conv, "0"), "0")(features_1_conv_2_post_act_fake_quantizer);  features_1_conv_2_post_act_fake_quantizer = None
    features_2_conv_0_1 = getattr(getattr(getattr(self.features, "2").conv, "0"), "1")(features_2_conv_0_0);  features_2_conv_0_0 = None
    features_2_conv_0_2 = getattr(getattr(getattr(self.features, "2").conv, "0"), "2")(features_2_conv_0_1);  features_2_conv_0_1 = None
    features_2_conv_0_2_post_act_fake_quantizer = self.features_2_conv_0_2_post_act_fake_quantizer(features_2_conv_0_2);  features_2_conv_0_2 = None
    features_2_conv_1_0 = getattr(getattr(getattr(self.features, "2").conv, "1"), "0")(features_2_conv_0_2_post_act_fake_quantizer);  features_2_conv_0_2_post_act_fake_quantizer = None
    features_2_conv_1_1 = getattr(getattr(getattr(self.features, "2").conv, "1"), "1")(features_2_conv_1_0);  features_2_conv_1_0 = None
    features_2_conv_1_2 = getattr(getattr(getattr(self.features, "2").conv, "1"), "2")(features_2_conv_1_1);  features_2_conv_1_1 = None
    features_2_conv_1_2_post_act_fake_quantizer = self.features_2_conv_1_2_post_act_fake_quantizer(features_2_conv_1_2);  features_2_conv_1_2 = None
    features_2_conv_2 = getattr(getattr(self.features, "2").conv, "2")(features_2_conv_1_2_post_act_fake_quantizer);  features_2_conv_1_2_post_act_fake_quantizer = None
    features_2_conv_3 = getattr(getattr(self.features, "2").conv, "3")(features_2_conv_2);  features_2_conv_2 = None
    features_2_conv_3_post_act_fake_quantizer = self.features_2_conv_3_post_act_fake_quantizer(features_2_conv_3);  features_2_conv_3 = None
    return features_2_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_2_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_2_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_2_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	881.387 (rec:881.387, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	1155.129 (rec:1155.129, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	954.436 (rec:954.436, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	1119.723 (rec:1119.723, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	1170.451 (rec:1170.451, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	983.367 (rec:983.367, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	1262.865 (rec:1262.865, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	1346.790 (rec:1316.838, round:29.953)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	1089.618 (rec:1065.010, round:24.608)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	1068.476 (rec:1046.528, round:21.948)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	1332.371 (rec:1312.490, round:19.881)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	1164.257 (rec:1146.073, round:18.184)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	1342.786 (rec:1325.909, round:16.877)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	1089.549 (rec:1073.551, round:15.998)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	1173.942 (rec:1158.655, round:15.288)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	1197.182 (rec:1182.550, round:14.632)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1410.538 (rec:1396.482, round:14.056)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	1393.806 (rec:1380.287, round:13.518)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	1145.092 (rec:1132.051, round:13.041)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	1452.294 (rec:1439.609, round:12.684)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	1047.035 (rec:1034.708, round:12.327)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	1208.401 (rec:1196.437, round:11.964)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	1546.790 (rec:1535.132, round:11.659)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1283.424 (rec:1272.056, round:11.369)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	1046.870 (rec:1035.805, round:11.065)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	1182.740 (rec:1171.999, round:10.741)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	1340.246 (rec:1329.819, round:10.427)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	1450.048 (rec:1439.884, round:10.164)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	1168.751 (rec:1158.816, round:9.935)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	1190.607 (rec:1180.917, round:9.690)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	1548.085 (rec:1538.633, round:9.452)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	1130.068 (rec:1120.897, round:9.171)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	1073.772 (rec:1064.863, round:8.909)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	1076.224 (rec:1067.590, round:8.634)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	1320.342 (rec:1312.001, round:8.340)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	1038.501 (rec:1030.458, round:8.043)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1195.444 (rec:1187.684, round:7.760)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	1375.973 (rec:1368.532, round:7.440)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	1233.859 (rec:1227.006, round:6.853)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	1295.429 (rec:1289.759, round:5.671)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_3_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_2_conv_3_post_act_fake_quantizer, features_3_conv_0_0, features_3_conv_0_1, features_3_conv_0_2, features_3_conv_0_2_post_act_fake_quantizer, features_3_conv_1_0, features_3_conv_1_1, features_3_conv_1_2, features_3_conv_1_2_post_act_fake_quantizer, features_3_conv_2, features_3_conv_3, add, add_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_2_conv_3_post_act_fake_quantizer):
    features_3_conv_0_0 = getattr(getattr(getattr(self.features, "3").conv, "0"), "0")(features_2_conv_3_post_act_fake_quantizer)
    features_3_conv_0_1 = getattr(getattr(getattr(self.features, "3").conv, "0"), "1")(features_3_conv_0_0);  features_3_conv_0_0 = None
    features_3_conv_0_2 = getattr(getattr(getattr(self.features, "3").conv, "0"), "2")(features_3_conv_0_1);  features_3_conv_0_1 = None
    features_3_conv_0_2_post_act_fake_quantizer = self.features_3_conv_0_2_post_act_fake_quantizer(features_3_conv_0_2);  features_3_conv_0_2 = None
    features_3_conv_1_0 = getattr(getattr(getattr(self.features, "3").conv, "1"), "0")(features_3_conv_0_2_post_act_fake_quantizer);  features_3_conv_0_2_post_act_fake_quantizer = None
    features_3_conv_1_1 = getattr(getattr(getattr(self.features, "3").conv, "1"), "1")(features_3_conv_1_0);  features_3_conv_1_0 = None
    features_3_conv_1_2 = getattr(getattr(getattr(self.features, "3").conv, "1"), "2")(features_3_conv_1_1);  features_3_conv_1_1 = None
    features_3_conv_1_2_post_act_fake_quantizer = self.features_3_conv_1_2_post_act_fake_quantizer(features_3_conv_1_2);  features_3_conv_1_2 = None
    features_3_conv_2 = getattr(getattr(self.features, "3").conv, "2")(features_3_conv_1_2_post_act_fake_quantizer);  features_3_conv_1_2_post_act_fake_quantizer = None
    features_3_conv_3 = getattr(getattr(self.features, "3").conv, "3")(features_3_conv_2);  features_3_conv_2 = None
    add = features_2_conv_3_post_act_fake_quantizer + features_3_conv_3;  features_2_conv_3_post_act_fake_quantizer = features_3_conv_3 = None
    add_post_act_fake_quantizer = self.add_post_act_fake_quantizer(add);  add = None
    return add_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_3_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_3_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	5661.283 (rec:5661.283, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	4639.248 (rec:4639.248, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	3955.245 (rec:3955.245, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	3879.285 (rec:3879.285, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	2961.106 (rec:2961.106, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	3467.468 (rec:3467.468, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	2501.013 (rec:2501.013, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	2518.753 (rec:2467.584, round:51.170)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	2668.752 (rec:2626.532, round:42.220)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	2534.809 (rec:2497.558, round:37.251)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	2549.467 (rec:2516.171, round:33.296)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2762.384 (rec:2732.426, round:29.957)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	2823.931 (rec:2796.844, round:27.087)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2889.292 (rec:2864.776, round:24.516)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	2445.383 (rec:2423.078, round:22.305)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	2449.342 (rec:2428.731, round:20.611)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	2256.450 (rec:2237.326, round:19.124)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	2225.317 (rec:2207.574, round:17.743)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	2753.131 (rec:2736.557, round:16.574)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	2227.106 (rec:2211.485, round:15.621)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	2219.578 (rec:2204.893, round:14.685)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	2678.430 (rec:2664.611, round:13.819)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	2869.950 (rec:2856.734, round:13.216)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	2710.834 (rec:2698.210, round:12.624)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	2166.346 (rec:2154.229, round:12.117)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	2273.463 (rec:2261.859, round:11.604)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	2284.731 (rec:2273.637, round:11.095)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	2605.198 (rec:2594.547, round:10.651)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	2557.563 (rec:2547.309, round:10.254)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	2931.792 (rec:2921.859, round:9.934)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	2256.294 (rec:2246.704, round:9.590)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	2492.807 (rec:2483.552, round:9.255)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	2908.733 (rec:2899.810, round:8.923)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	2790.183 (rec:2781.596, round:8.587)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	2786.398 (rec:2778.152, round:8.245)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	2679.035 (rec:2671.131, round:7.904)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	2473.840 (rec:2466.277, round:7.563)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2189.788 (rec:2182.600, round:7.187)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	2547.931 (rec:2541.409, round:6.522)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	2487.999 (rec:2482.818, round:5.180)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_4_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_post_act_fake_quantizer, features_4_conv_0_0, features_4_conv_0_1, features_4_conv_0_2, features_4_conv_0_2_post_act_fake_quantizer, features_4_conv_1_0, features_4_conv_1_1, features_4_conv_1_2, features_4_conv_1_2_post_act_fake_quantizer, features_4_conv_2, features_4_conv_3, features_4_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_post_act_fake_quantizer):
    features_4_conv_0_0 = getattr(getattr(getattr(self.features, "4").conv, "0"), "0")(add_post_act_fake_quantizer);  add_post_act_fake_quantizer = None
    features_4_conv_0_1 = getattr(getattr(getattr(self.features, "4").conv, "0"), "1")(features_4_conv_0_0);  features_4_conv_0_0 = None
    features_4_conv_0_2 = getattr(getattr(getattr(self.features, "4").conv, "0"), "2")(features_4_conv_0_1);  features_4_conv_0_1 = None
    features_4_conv_0_2_post_act_fake_quantizer = self.features_4_conv_0_2_post_act_fake_quantizer(features_4_conv_0_2);  features_4_conv_0_2 = None
    features_4_conv_1_0 = getattr(getattr(getattr(self.features, "4").conv, "1"), "0")(features_4_conv_0_2_post_act_fake_quantizer);  features_4_conv_0_2_post_act_fake_quantizer = None
    features_4_conv_1_1 = getattr(getattr(getattr(self.features, "4").conv, "1"), "1")(features_4_conv_1_0);  features_4_conv_1_0 = None
    features_4_conv_1_2 = getattr(getattr(getattr(self.features, "4").conv, "1"), "2")(features_4_conv_1_1);  features_4_conv_1_1 = None
    features_4_conv_1_2_post_act_fake_quantizer = self.features_4_conv_1_2_post_act_fake_quantizer(features_4_conv_1_2);  features_4_conv_1_2 = None
    features_4_conv_2 = getattr(getattr(self.features, "4").conv, "2")(features_4_conv_1_2_post_act_fake_quantizer);  features_4_conv_1_2_post_act_fake_quantizer = None
    features_4_conv_3 = getattr(getattr(self.features, "4").conv, "3")(features_4_conv_2);  features_4_conv_2 = None
    features_4_conv_3_post_act_fake_quantizer = self.features_4_conv_3_post_act_fake_quantizer(features_4_conv_3);  features_4_conv_3 = None
    return features_4_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_4_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_4_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_4_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	1615.766 (rec:1615.766, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	1523.965 (rec:1523.965, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	1527.824 (rec:1527.824, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	1308.766 (rec:1308.766, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	1303.689 (rec:1303.689, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	1314.065 (rec:1314.065, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	1250.760 (rec:1250.760, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	1409.634 (rec:1341.240, round:68.394)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	1490.976 (rec:1433.069, round:57.907)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	1396.025 (rec:1343.362, round:52.663)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	1389.429 (rec:1341.005, round:48.424)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	1278.874 (rec:1234.011, round:44.863)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	1289.157 (rec:1247.716, round:41.442)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	1347.237 (rec:1308.679, round:38.558)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	1334.108 (rec:1297.997, round:36.111)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	1481.347 (rec:1447.258, round:34.088)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1358.422 (rec:1326.198, round:32.225)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	1511.318 (rec:1480.619, round:30.700)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	1469.507 (rec:1440.226, round:29.281)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	1291.353 (rec:1263.330, round:28.023)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	1330.755 (rec:1303.866, round:26.889)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	1279.023 (rec:1253.257, round:25.766)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	1368.160 (rec:1343.403, round:24.757)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1504.161 (rec:1480.182, round:23.979)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	1361.457 (rec:1338.277, round:23.181)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	1456.121 (rec:1433.711, round:22.410)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	1254.111 (rec:1232.475, round:21.636)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	1369.495 (rec:1348.610, round:20.885)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	1343.167 (rec:1322.932, round:20.234)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	1365.251 (rec:1345.706, round:19.546)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	1321.199 (rec:1302.363, round:18.836)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	1247.566 (rec:1229.372, round:18.194)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	1305.029 (rec:1287.481, round:17.549)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	1338.705 (rec:1321.787, round:16.918)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	1316.653 (rec:1300.389, round:16.265)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	1282.530 (rec:1266.938, round:15.592)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1356.996 (rec:1342.155, round:14.842)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	1358.244 (rec:1344.289, round:13.955)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	1307.149 (rec:1294.466, round:12.683)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	1328.653 (rec:1318.069, round:10.584)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_5_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_4_conv_3_post_act_fake_quantizer, features_5_conv_0_0, features_5_conv_0_1, features_5_conv_0_2, features_5_conv_0_2_post_act_fake_quantizer, features_5_conv_1_0, features_5_conv_1_1, features_5_conv_1_2, features_5_conv_1_2_post_act_fake_quantizer, features_5_conv_2, features_5_conv_3, add_1, add_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_4_conv_3_post_act_fake_quantizer):
    features_5_conv_0_0 = getattr(getattr(getattr(self.features, "5").conv, "0"), "0")(features_4_conv_3_post_act_fake_quantizer)
    features_5_conv_0_1 = getattr(getattr(getattr(self.features, "5").conv, "0"), "1")(features_5_conv_0_0);  features_5_conv_0_0 = None
    features_5_conv_0_2 = getattr(getattr(getattr(self.features, "5").conv, "0"), "2")(features_5_conv_0_1);  features_5_conv_0_1 = None
    features_5_conv_0_2_post_act_fake_quantizer = self.features_5_conv_0_2_post_act_fake_quantizer(features_5_conv_0_2);  features_5_conv_0_2 = None
    features_5_conv_1_0 = getattr(getattr(getattr(self.features, "5").conv, "1"), "0")(features_5_conv_0_2_post_act_fake_quantizer);  features_5_conv_0_2_post_act_fake_quantizer = None
    features_5_conv_1_1 = getattr(getattr(getattr(self.features, "5").conv, "1"), "1")(features_5_conv_1_0);  features_5_conv_1_0 = None
    features_5_conv_1_2 = getattr(getattr(getattr(self.features, "5").conv, "1"), "2")(features_5_conv_1_1);  features_5_conv_1_1 = None
    features_5_conv_1_2_post_act_fake_quantizer = self.features_5_conv_1_2_post_act_fake_quantizer(features_5_conv_1_2);  features_5_conv_1_2 = None
    features_5_conv_2 = getattr(getattr(self.features, "5").conv, "2")(features_5_conv_1_2_post_act_fake_quantizer);  features_5_conv_1_2_post_act_fake_quantizer = None
    features_5_conv_3 = getattr(getattr(self.features, "5").conv, "3")(features_5_conv_2);  features_5_conv_2 = None
    add_1 = features_4_conv_3_post_act_fake_quantizer + features_5_conv_3;  features_4_conv_3_post_act_fake_quantizer = features_5_conv_3 = None
    add_1_post_act_fake_quantizer = self.add_1_post_act_fake_quantizer(add_1);  add_1 = None
    return add_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_5_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_5_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	2950.184 (rec:2950.184, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	2688.038 (rec:2688.038, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	2433.796 (rec:2433.796, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	2424.408 (rec:2424.408, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	2374.628 (rec:2374.628, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	2338.033 (rec:2338.033, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	2275.669 (rec:2275.669, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	2385.504 (rec:2300.870, round:84.634)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	2356.488 (rec:2293.978, round:62.510)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	2336.448 (rec:2282.621, round:53.828)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	2324.466 (rec:2276.810, round:47.656)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2352.864 (rec:2309.679, round:43.185)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	2350.225 (rec:2311.285, round:38.940)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2287.401 (rec:2251.642, round:35.759)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	2163.529 (rec:2130.267, round:33.262)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	2378.651 (rec:2347.569, round:31.082)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	2228.464 (rec:2199.223, round:29.241)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	2215.176 (rec:2187.509, round:27.667)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	2219.820 (rec:2193.662, round:26.159)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	2211.195 (rec:2186.422, round:24.772)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	2371.501 (rec:2347.775, round:23.726)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	2483.319 (rec:2460.630, round:22.689)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	2324.547 (rec:2302.895, round:21.652)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	2278.678 (rec:2258.051, round:20.627)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	2194.438 (rec:2174.666, round:19.772)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	2228.367 (rec:2209.358, round:19.009)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	2361.262 (rec:2343.074, round:18.188)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	2341.741 (rec:2324.372, round:17.370)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	2332.535 (rec:2315.909, round:16.627)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	2243.970 (rec:2228.113, round:15.857)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	2309.424 (rec:2294.229, round:15.195)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	2179.698 (rec:2165.209, round:14.490)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	2420.592 (rec:2406.796, round:13.795)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	2151.764 (rec:2138.645, round:13.119)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	2263.811 (rec:2251.309, round:12.502)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	2297.214 (rec:2285.345, round:11.869)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	2487.368 (rec:2476.191, round:11.177)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2141.435 (rec:2130.949, round:10.486)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	2276.214 (rec:2266.825, round:9.388)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	2238.859 (rec:2231.562, round:7.297)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_6_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_1_post_act_fake_quantizer, features_6_conv_0_0, features_6_conv_0_1, features_6_conv_0_2, features_6_conv_0_2_post_act_fake_quantizer, features_6_conv_1_0, features_6_conv_1_1, features_6_conv_1_2, features_6_conv_1_2_post_act_fake_quantizer, features_6_conv_2, features_6_conv_3, add_2, add_2_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_1_post_act_fake_quantizer):
    features_6_conv_0_0 = getattr(getattr(getattr(self.features, "6").conv, "0"), "0")(add_1_post_act_fake_quantizer)
    features_6_conv_0_1 = getattr(getattr(getattr(self.features, "6").conv, "0"), "1")(features_6_conv_0_0);  features_6_conv_0_0 = None
    features_6_conv_0_2 = getattr(getattr(getattr(self.features, "6").conv, "0"), "2")(features_6_conv_0_1);  features_6_conv_0_1 = None
    features_6_conv_0_2_post_act_fake_quantizer = self.features_6_conv_0_2_post_act_fake_quantizer(features_6_conv_0_2);  features_6_conv_0_2 = None
    features_6_conv_1_0 = getattr(getattr(getattr(self.features, "6").conv, "1"), "0")(features_6_conv_0_2_post_act_fake_quantizer);  features_6_conv_0_2_post_act_fake_quantizer = None
    features_6_conv_1_1 = getattr(getattr(getattr(self.features, "6").conv, "1"), "1")(features_6_conv_1_0);  features_6_conv_1_0 = None
    features_6_conv_1_2 = getattr(getattr(getattr(self.features, "6").conv, "1"), "2")(features_6_conv_1_1);  features_6_conv_1_1 = None
    features_6_conv_1_2_post_act_fake_quantizer = self.features_6_conv_1_2_post_act_fake_quantizer(features_6_conv_1_2);  features_6_conv_1_2 = None
    features_6_conv_2 = getattr(getattr(self.features, "6").conv, "2")(features_6_conv_1_2_post_act_fake_quantizer);  features_6_conv_1_2_post_act_fake_quantizer = None
    features_6_conv_3 = getattr(getattr(self.features, "6").conv, "3")(features_6_conv_2);  features_6_conv_2 = None
    add_2 = add_1_post_act_fake_quantizer + features_6_conv_3;  add_1_post_act_fake_quantizer = features_6_conv_3 = None
    add_2_post_act_fake_quantizer = self.add_2_post_act_fake_quantizer(add_2);  add_2 = None
    return add_2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_6_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_6_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	3961.501 (rec:3961.501, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	3752.890 (rec:3752.890, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	3585.042 (rec:3585.042, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	3738.132 (rec:3738.132, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	3442.901 (rec:3442.901, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	3446.896 (rec:3446.896, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	3337.434 (rec:3337.434, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	3444.226 (rec:3369.902, round:74.325)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	3329.107 (rec:3278.223, round:50.884)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	3361.257 (rec:3319.104, round:42.153)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	3340.273 (rec:3304.323, round:35.950)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	3181.900 (rec:3150.275, round:31.626)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	3599.104 (rec:3570.824, round:28.280)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	3527.642 (rec:3502.080, round:25.562)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	3422.987 (rec:3399.751, round:23.236)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	3320.691 (rec:3299.304, round:21.387)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	3323.328 (rec:3303.522, round:19.807)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	3456.909 (rec:3438.568, round:18.341)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	3454.092 (rec:3437.010, round:17.082)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	3422.174 (rec:3406.125, round:16.049)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	3305.618 (rec:3290.476, round:15.142)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	3278.862 (rec:3264.524, round:14.337)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	3118.681 (rec:3105.179, round:13.502)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	3459.957 (rec:3447.177, round:12.780)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	3413.955 (rec:3401.811, round:12.144)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	3361.207 (rec:3349.645, round:11.562)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	3262.308 (rec:3251.358, round:10.949)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	3559.377 (rec:3548.861, round:10.516)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	3110.730 (rec:3100.668, round:10.062)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	3500.680 (rec:3491.114, round:9.567)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	3304.552 (rec:3295.407, round:9.145)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	3273.603 (rec:3264.790, round:8.814)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	3389.952 (rec:3381.492, round:8.460)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	3160.012 (rec:3151.889, round:8.123)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	3194.307 (rec:3186.559, round:7.747)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	3157.927 (rec:3150.564, round:7.362)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	3436.977 (rec:3429.960, round:7.018)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	3409.508 (rec:3402.859, round:6.649)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	3169.640 (rec:3163.758, round:5.882)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	3485.296 (rec:3481.136, round:4.159)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_7_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_2_post_act_fake_quantizer, features_7_conv_0_0, features_7_conv_0_1, features_7_conv_0_2, features_7_conv_0_2_post_act_fake_quantizer, features_7_conv_1_0, features_7_conv_1_1, features_7_conv_1_2, features_7_conv_1_2_post_act_fake_quantizer, features_7_conv_2, features_7_conv_3, features_7_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_2_post_act_fake_quantizer):
    features_7_conv_0_0 = getattr(getattr(getattr(self.features, "7").conv, "0"), "0")(add_2_post_act_fake_quantizer);  add_2_post_act_fake_quantizer = None
    features_7_conv_0_1 = getattr(getattr(getattr(self.features, "7").conv, "0"), "1")(features_7_conv_0_0);  features_7_conv_0_0 = None
    features_7_conv_0_2 = getattr(getattr(getattr(self.features, "7").conv, "0"), "2")(features_7_conv_0_1);  features_7_conv_0_1 = None
    features_7_conv_0_2_post_act_fake_quantizer = self.features_7_conv_0_2_post_act_fake_quantizer(features_7_conv_0_2);  features_7_conv_0_2 = None
    features_7_conv_1_0 = getattr(getattr(getattr(self.features, "7").conv, "1"), "0")(features_7_conv_0_2_post_act_fake_quantizer);  features_7_conv_0_2_post_act_fake_quantizer = None
    features_7_conv_1_1 = getattr(getattr(getattr(self.features, "7").conv, "1"), "1")(features_7_conv_1_0);  features_7_conv_1_0 = None
    features_7_conv_1_2 = getattr(getattr(getattr(self.features, "7").conv, "1"), "2")(features_7_conv_1_1);  features_7_conv_1_1 = None
    features_7_conv_1_2_post_act_fake_quantizer = self.features_7_conv_1_2_post_act_fake_quantizer(features_7_conv_1_2);  features_7_conv_1_2 = None
    features_7_conv_2 = getattr(getattr(self.features, "7").conv, "2")(features_7_conv_1_2_post_act_fake_quantizer);  features_7_conv_1_2_post_act_fake_quantizer = None
    features_7_conv_3 = getattr(getattr(self.features, "7").conv, "3")(features_7_conv_2);  features_7_conv_2 = None
    features_7_conv_3_post_act_fake_quantizer = self.features_7_conv_3_post_act_fake_quantizer(features_7_conv_3);  features_7_conv_3 = None
    return features_7_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_7_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_7_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_7_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	2676.280 (rec:2676.280, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	2299.325 (rec:2299.325, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	2114.504 (rec:2114.504, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	2123.943 (rec:2123.943, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	2017.654 (rec:2017.654, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	1972.752 (rec:1972.752, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	1988.141 (rec:1988.141, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	1995.536 (rec:1842.612, round:152.923)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	2057.710 (rec:1927.597, round:130.112)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	2033.728 (rec:1914.797, round:118.931)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	2060.180 (rec:1949.871, round:110.309)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2103.030 (rec:1999.870, round:103.161)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	1903.449 (rec:1805.965, round:97.484)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2037.143 (rec:1944.487, round:92.655)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	1997.222 (rec:1909.028, round:88.194)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	1866.900 (rec:1782.565, round:84.335)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1969.043 (rec:1887.807, round:81.236)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	2054.065 (rec:1975.746, round:78.319)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	1988.535 (rec:1912.952, round:75.583)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	1860.693 (rec:1787.583, round:73.110)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	1949.302 (rec:1878.628, round:70.674)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	1965.274 (rec:1896.862, round:68.412)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	1938.345 (rec:1871.956, round:66.390)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1967.891 (rec:1903.524, round:64.367)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	1963.391 (rec:1901.077, round:62.314)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	1931.804 (rec:1871.538, round:60.266)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	1960.520 (rec:1902.089, round:58.431)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	1903.284 (rec:1846.680, round:56.603)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	1967.959 (rec:1913.200, round:54.759)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	2065.757 (rec:2012.746, round:53.012)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	1911.813 (rec:1860.519, round:51.294)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	1938.445 (rec:1888.869, round:49.576)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	1931.982 (rec:1884.188, round:47.794)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	1966.812 (rec:1920.869, round:45.943)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	1887.128 (rec:1843.053, round:44.074)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	1827.938 (rec:1785.798, round:42.140)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1962.540 (rec:1922.559, round:39.981)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	1844.222 (rec:1806.632, round:37.590)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	1873.577 (rec:1839.027, round:34.550)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	1917.608 (rec:1887.472, round:30.136)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_8_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_7_conv_3_post_act_fake_quantizer, features_8_conv_0_0, features_8_conv_0_1, features_8_conv_0_2, features_8_conv_0_2_post_act_fake_quantizer, features_8_conv_1_0, features_8_conv_1_1, features_8_conv_1_2, features_8_conv_1_2_post_act_fake_quantizer, features_8_conv_2, features_8_conv_3, add_3, add_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_7_conv_3_post_act_fake_quantizer):
    features_8_conv_0_0 = getattr(getattr(getattr(self.features, "8").conv, "0"), "0")(features_7_conv_3_post_act_fake_quantizer)
    features_8_conv_0_1 = getattr(getattr(getattr(self.features, "8").conv, "0"), "1")(features_8_conv_0_0);  features_8_conv_0_0 = None
    features_8_conv_0_2 = getattr(getattr(getattr(self.features, "8").conv, "0"), "2")(features_8_conv_0_1);  features_8_conv_0_1 = None
    features_8_conv_0_2_post_act_fake_quantizer = self.features_8_conv_0_2_post_act_fake_quantizer(features_8_conv_0_2);  features_8_conv_0_2 = None
    features_8_conv_1_0 = getattr(getattr(getattr(self.features, "8").conv, "1"), "0")(features_8_conv_0_2_post_act_fake_quantizer);  features_8_conv_0_2_post_act_fake_quantizer = None
    features_8_conv_1_1 = getattr(getattr(getattr(self.features, "8").conv, "1"), "1")(features_8_conv_1_0);  features_8_conv_1_0 = None
    features_8_conv_1_2 = getattr(getattr(getattr(self.features, "8").conv, "1"), "2")(features_8_conv_1_1);  features_8_conv_1_1 = None
    features_8_conv_1_2_post_act_fake_quantizer = self.features_8_conv_1_2_post_act_fake_quantizer(features_8_conv_1_2);  features_8_conv_1_2 = None
    features_8_conv_2 = getattr(getattr(self.features, "8").conv, "2")(features_8_conv_1_2_post_act_fake_quantizer);  features_8_conv_1_2_post_act_fake_quantizer = None
    features_8_conv_3 = getattr(getattr(self.features, "8").conv, "3")(features_8_conv_2);  features_8_conv_2 = None
    add_3 = features_7_conv_3_post_act_fake_quantizer + features_8_conv_3;  features_7_conv_3_post_act_fake_quantizer = features_8_conv_3 = None
    add_3_post_act_fake_quantizer = self.add_3_post_act_fake_quantizer(add_3);  add_3 = None
    return add_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_8_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_8_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	2991.506 (rec:2991.506, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	3022.135 (rec:3022.135, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	2921.146 (rec:2921.146, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	2930.407 (rec:2930.407, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	3058.360 (rec:3058.360, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	2910.417 (rec:2910.417, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	2825.654 (rec:2825.654, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	3416.150 (rec:3068.514, round:347.636)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	3089.595 (rec:2839.692, round:249.903)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	3125.176 (rec:2904.913, round:220.263)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	3023.298 (rec:2823.880, round:199.418)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	3269.260 (rec:3087.522, round:181.737)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	3087.065 (rec:2919.969, round:167.096)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	3210.526 (rec:3055.991, round:154.535)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	3187.903 (rec:3044.482, round:143.420)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	3218.825 (rec:3085.312, round:133.513)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	3100.783 (rec:2976.172, round:124.611)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	2989.008 (rec:2872.075, round:116.933)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	2901.342 (rec:2791.472, round:109.870)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	2909.450 (rec:2806.400, round:103.050)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	2965.908 (rec:2868.696, round:97.212)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	2908.015 (rec:2816.153, round:91.862)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	3175.364 (rec:3088.773, round:86.591)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	2936.486 (rec:2854.578, round:81.908)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	3089.749 (rec:3012.448, round:77.300)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	3001.664 (rec:2928.742, round:72.922)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	3007.649 (rec:2938.713, round:68.936)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	2959.266 (rec:2894.097, round:65.169)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	3050.977 (rec:2989.364, round:61.613)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	3057.519 (rec:2999.421, round:58.098)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	2935.167 (rec:2880.385, round:54.782)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	2935.753 (rec:2884.234, round:51.519)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	2908.292 (rec:2860.017, round:48.275)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	2852.689 (rec:2807.604, round:45.084)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	2984.251 (rec:2942.362, round:41.889)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	2839.769 (rec:2800.790, round:38.980)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	2948.589 (rec:2912.664, round:35.925)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2930.520 (rec:2897.811, round:32.710)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	2839.655 (rec:2811.075, round:28.579)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	2957.633 (rec:2935.454, round:22.179)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_9_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_3_post_act_fake_quantizer, features_9_conv_0_0, features_9_conv_0_1, features_9_conv_0_2, features_9_conv_0_2_post_act_fake_quantizer, features_9_conv_1_0, features_9_conv_1_1, features_9_conv_1_2, features_9_conv_1_2_post_act_fake_quantizer, features_9_conv_2, features_9_conv_3, add_4, add_4_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_3_post_act_fake_quantizer):
    features_9_conv_0_0 = getattr(getattr(getattr(self.features, "9").conv, "0"), "0")(add_3_post_act_fake_quantizer)
    features_9_conv_0_1 = getattr(getattr(getattr(self.features, "9").conv, "0"), "1")(features_9_conv_0_0);  features_9_conv_0_0 = None
    features_9_conv_0_2 = getattr(getattr(getattr(self.features, "9").conv, "0"), "2")(features_9_conv_0_1);  features_9_conv_0_1 = None
    features_9_conv_0_2_post_act_fake_quantizer = self.features_9_conv_0_2_post_act_fake_quantizer(features_9_conv_0_2);  features_9_conv_0_2 = None
    features_9_conv_1_0 = getattr(getattr(getattr(self.features, "9").conv, "1"), "0")(features_9_conv_0_2_post_act_fake_quantizer);  features_9_conv_0_2_post_act_fake_quantizer = None
    features_9_conv_1_1 = getattr(getattr(getattr(self.features, "9").conv, "1"), "1")(features_9_conv_1_0);  features_9_conv_1_0 = None
    features_9_conv_1_2 = getattr(getattr(getattr(self.features, "9").conv, "1"), "2")(features_9_conv_1_1);  features_9_conv_1_1 = None
    features_9_conv_1_2_post_act_fake_quantizer = self.features_9_conv_1_2_post_act_fake_quantizer(features_9_conv_1_2);  features_9_conv_1_2 = None
    features_9_conv_2 = getattr(getattr(self.features, "9").conv, "2")(features_9_conv_1_2_post_act_fake_quantizer);  features_9_conv_1_2_post_act_fake_quantizer = None
    features_9_conv_3 = getattr(getattr(self.features, "9").conv, "3")(features_9_conv_2);  features_9_conv_2 = None
    add_4 = add_3_post_act_fake_quantizer + features_9_conv_3;  add_3_post_act_fake_quantizer = features_9_conv_3 = None
    add_4_post_act_fake_quantizer = self.add_4_post_act_fake_quantizer(add_4);  add_4 = None
    return add_4_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_9_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_9_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_4_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	3551.740 (rec:3551.740, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	3448.988 (rec:3448.988, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	3629.961 (rec:3629.961, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	3593.319 (rec:3593.319, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	3365.286 (rec:3365.286, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	3426.225 (rec:3426.225, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	3422.934 (rec:3422.934, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	3616.745 (rec:3298.004, round:318.741)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	3624.450 (rec:3401.455, round:222.994)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	3500.488 (rec:3306.248, round:194.240)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	3754.896 (rec:3580.764, round:174.133)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	3534.513 (rec:3376.624, round:157.889)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	3512.577 (rec:3368.440, round:144.137)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	3609.249 (rec:3475.960, round:133.289)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	3566.308 (rec:3442.405, round:123.903)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	3696.799 (rec:3581.678, round:115.120)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	3552.232 (rec:3444.607, round:107.625)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	3636.230 (rec:3535.247, round:100.982)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	3372.704 (rec:3277.842, round:94.863)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	3387.601 (rec:3298.434, round:89.168)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	3473.012 (rec:3389.192, round:83.820)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	3493.294 (rec:3413.917, round:79.377)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	3539.302 (rec:3463.990, round:75.312)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	3452.760 (rec:3381.335, round:71.425)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	3376.459 (rec:3308.590, round:67.870)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	3611.973 (rec:3547.573, round:64.400)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	3431.170 (rec:3369.955, round:61.214)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	3542.131 (rec:3483.995, round:58.136)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	3469.243 (rec:3414.237, round:55.006)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	3459.473 (rec:3407.328, round:52.145)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	3461.261 (rec:3411.752, round:49.509)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	3536.915 (rec:3489.954, round:46.961)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	3406.748 (rec:3362.442, round:44.306)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	3288.861 (rec:3246.962, round:41.899)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	3506.632 (rec:3467.275, round:39.356)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	3430.766 (rec:3394.114, round:36.652)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	3433.512 (rec:3399.690, round:33.822)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	3495.707 (rec:3464.684, round:31.023)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	3402.633 (rec:3375.305, round:27.328)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	3524.521 (rec:3503.174, round:21.347)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_10_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_4_post_act_fake_quantizer, features_10_conv_0_0, features_10_conv_0_1, features_10_conv_0_2, features_10_conv_0_2_post_act_fake_quantizer, features_10_conv_1_0, features_10_conv_1_1, features_10_conv_1_2, features_10_conv_1_2_post_act_fake_quantizer, features_10_conv_2, features_10_conv_3, add_5, add_5_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_4_post_act_fake_quantizer):
    features_10_conv_0_0 = getattr(getattr(getattr(self.features, "10").conv, "0"), "0")(add_4_post_act_fake_quantizer)
    features_10_conv_0_1 = getattr(getattr(getattr(self.features, "10").conv, "0"), "1")(features_10_conv_0_0);  features_10_conv_0_0 = None
    features_10_conv_0_2 = getattr(getattr(getattr(self.features, "10").conv, "0"), "2")(features_10_conv_0_1);  features_10_conv_0_1 = None
    features_10_conv_0_2_post_act_fake_quantizer = self.features_10_conv_0_2_post_act_fake_quantizer(features_10_conv_0_2);  features_10_conv_0_2 = None
    features_10_conv_1_0 = getattr(getattr(getattr(self.features, "10").conv, "1"), "0")(features_10_conv_0_2_post_act_fake_quantizer);  features_10_conv_0_2_post_act_fake_quantizer = None
    features_10_conv_1_1 = getattr(getattr(getattr(self.features, "10").conv, "1"), "1")(features_10_conv_1_0);  features_10_conv_1_0 = None
    features_10_conv_1_2 = getattr(getattr(getattr(self.features, "10").conv, "1"), "2")(features_10_conv_1_1);  features_10_conv_1_1 = None
    features_10_conv_1_2_post_act_fake_quantizer = self.features_10_conv_1_2_post_act_fake_quantizer(features_10_conv_1_2);  features_10_conv_1_2 = None
    features_10_conv_2 = getattr(getattr(self.features, "10").conv, "2")(features_10_conv_1_2_post_act_fake_quantizer);  features_10_conv_1_2_post_act_fake_quantizer = None
    features_10_conv_3 = getattr(getattr(self.features, "10").conv, "3")(features_10_conv_2);  features_10_conv_2 = None
    add_5 = add_4_post_act_fake_quantizer + features_10_conv_3;  add_4_post_act_fake_quantizer = features_10_conv_3 = None
    add_5_post_act_fake_quantizer = self.add_5_post_act_fake_quantizer(add_5);  add_5 = None
    return add_5_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_10_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_10_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_5_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	4527.479 (rec:4527.479, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	4609.241 (rec:4609.241, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	4329.488 (rec:4329.488, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	4315.515 (rec:4315.515, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	4032.854 (rec:4032.854, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	4097.238 (rec:4097.238, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	3970.686 (rec:3970.686, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	4573.690 (rec:4245.666, round:328.024)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	4120.203 (rec:3885.958, round:234.245)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	4201.496 (rec:3996.464, round:205.032)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	4042.671 (rec:3856.851, round:185.820)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	4446.666 (rec:4276.817, round:169.848)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	4129.847 (rec:3973.648, round:156.199)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	4144.208 (rec:3999.324, round:144.884)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	4189.086 (rec:4053.990, round:135.096)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	4101.754 (rec:3975.582, round:126.172)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	3922.701 (rec:3803.807, round:118.894)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	4204.524 (rec:4092.576, round:111.948)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	4076.584 (rec:3971.073, round:105.511)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	4045.404 (rec:3945.902, round:99.502)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	4012.280 (rec:3918.032, round:94.248)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	4223.222 (rec:4134.009, round:89.213)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	3912.550 (rec:3828.202, round:84.348)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	4019.977 (rec:3939.805, round:80.172)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	3937.384 (rec:3861.023, round:76.361)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	4109.320 (rec:4036.709, round:72.612)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	4111.640 (rec:4042.615, round:69.025)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	4200.628 (rec:4134.771, round:65.857)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	4028.401 (rec:3965.719, round:62.682)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	4038.781 (rec:3979.118, round:59.663)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	4018.104 (rec:3961.570, round:56.534)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	3958.847 (rec:3905.283, round:53.564)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	3873.785 (rec:3823.105, round:50.680)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	3867.810 (rec:3819.965, round:47.845)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	4240.142 (rec:4195.059, round:45.083)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	4240.741 (rec:4198.429, round:42.312)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	3884.699 (rec:3845.298, round:39.402)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	4078.329 (rec:4042.003, round:36.325)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	3819.815 (rec:3787.670, round:32.144)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	4219.917 (rec:4194.233, round:25.685)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_11_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_5_post_act_fake_quantizer, features_11_conv_0_0, features_11_conv_0_1, features_11_conv_0_2, features_11_conv_0_2_post_act_fake_quantizer, features_11_conv_1_0, features_11_conv_1_1, features_11_conv_1_2, features_11_conv_1_2_post_act_fake_quantizer, features_11_conv_2, features_11_conv_3, features_11_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_5_post_act_fake_quantizer):
    features_11_conv_0_0 = getattr(getattr(getattr(self.features, "11").conv, "0"), "0")(add_5_post_act_fake_quantizer);  add_5_post_act_fake_quantizer = None
    features_11_conv_0_1 = getattr(getattr(getattr(self.features, "11").conv, "0"), "1")(features_11_conv_0_0);  features_11_conv_0_0 = None
    features_11_conv_0_2 = getattr(getattr(getattr(self.features, "11").conv, "0"), "2")(features_11_conv_0_1);  features_11_conv_0_1 = None
    features_11_conv_0_2_post_act_fake_quantizer = self.features_11_conv_0_2_post_act_fake_quantizer(features_11_conv_0_2);  features_11_conv_0_2 = None
    features_11_conv_1_0 = getattr(getattr(getattr(self.features, "11").conv, "1"), "0")(features_11_conv_0_2_post_act_fake_quantizer);  features_11_conv_0_2_post_act_fake_quantizer = None
    features_11_conv_1_1 = getattr(getattr(getattr(self.features, "11").conv, "1"), "1")(features_11_conv_1_0);  features_11_conv_1_0 = None
    features_11_conv_1_2 = getattr(getattr(getattr(self.features, "11").conv, "1"), "2")(features_11_conv_1_1);  features_11_conv_1_1 = None
    features_11_conv_1_2_post_act_fake_quantizer = self.features_11_conv_1_2_post_act_fake_quantizer(features_11_conv_1_2);  features_11_conv_1_2 = None
    features_11_conv_2 = getattr(getattr(self.features, "11").conv, "2")(features_11_conv_1_2_post_act_fake_quantizer);  features_11_conv_1_2_post_act_fake_quantizer = None
    features_11_conv_3 = getattr(getattr(self.features, "11").conv, "3")(features_11_conv_2);  features_11_conv_2 = None
    features_11_conv_3_post_act_fake_quantizer = self.features_11_conv_3_post_act_fake_quantizer(features_11_conv_3);  features_11_conv_3 = None
    return features_11_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_11_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_11_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_11_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	2271.865 (rec:2271.865, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	2069.871 (rec:2069.871, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	2041.192 (rec:2041.192, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	1866.962 (rec:1866.962, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	1744.036 (rec:1744.036, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	1823.853 (rec:1823.853, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	1809.002 (rec:1809.002, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	2370.667 (rec:1889.261, round:481.406)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	2090.382 (rec:1723.480, round:366.901)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	2069.737 (rec:1742.343, round:327.394)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	2226.438 (rec:1928.175, round:298.262)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2013.259 (rec:1737.866, round:275.393)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	1986.145 (rec:1729.683, round:256.462)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2023.826 (rec:1783.570, round:240.257)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	2069.070 (rec:1843.040, round:226.029)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	2053.288 (rec:1839.743, round:213.545)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1919.496 (rec:1716.762, round:202.735)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	2165.210 (rec:1972.477, round:192.733)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	2031.971 (rec:1848.449, round:183.521)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	2048.322 (rec:1873.362, round:174.960)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	2040.054 (rec:1872.936, round:167.119)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	1924.189 (rec:1764.655, round:159.534)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	2076.700 (rec:1924.233, round:152.467)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1867.545 (rec:1721.855, round:145.690)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	1986.112 (rec:1846.846, round:139.266)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	1808.758 (rec:1675.438, round:133.320)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	1996.409 (rec:1868.894, round:127.515)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	2052.932 (rec:1930.949, round:121.983)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	1947.812 (rec:1831.505, round:116.307)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	2046.948 (rec:1936.011, round:110.937)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	1753.500 (rec:1647.546, round:105.955)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	1932.791 (rec:1832.102, round:100.689)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	1856.739 (rec:1761.351, round:95.388)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	1748.776 (rec:1658.727, round:90.049)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	1929.319 (rec:1844.679, round:84.640)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	2007.954 (rec:1928.891, round:79.062)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1750.745 (rec:1677.452, round:73.293)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2052.902 (rec:1985.820, round:67.082)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	1975.201 (rec:1915.769, round:59.431)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	1975.823 (rec:1926.811, round:49.012)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_12_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_11_conv_3_post_act_fake_quantizer, features_12_conv_0_0, features_12_conv_0_1, features_12_conv_0_2, features_12_conv_0_2_post_act_fake_quantizer, features_12_conv_1_0, features_12_conv_1_1, features_12_conv_1_2, features_12_conv_1_2_post_act_fake_quantizer, features_12_conv_2, features_12_conv_3, add_6, add_6_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_11_conv_3_post_act_fake_quantizer):
    features_12_conv_0_0 = getattr(getattr(getattr(self.features, "12").conv, "0"), "0")(features_11_conv_3_post_act_fake_quantizer)
    features_12_conv_0_1 = getattr(getattr(getattr(self.features, "12").conv, "0"), "1")(features_12_conv_0_0);  features_12_conv_0_0 = None
    features_12_conv_0_2 = getattr(getattr(getattr(self.features, "12").conv, "0"), "2")(features_12_conv_0_1);  features_12_conv_0_1 = None
    features_12_conv_0_2_post_act_fake_quantizer = self.features_12_conv_0_2_post_act_fake_quantizer(features_12_conv_0_2);  features_12_conv_0_2 = None
    features_12_conv_1_0 = getattr(getattr(getattr(self.features, "12").conv, "1"), "0")(features_12_conv_0_2_post_act_fake_quantizer);  features_12_conv_0_2_post_act_fake_quantizer = None
    features_12_conv_1_1 = getattr(getattr(getattr(self.features, "12").conv, "1"), "1")(features_12_conv_1_0);  features_12_conv_1_0 = None
    features_12_conv_1_2 = getattr(getattr(getattr(self.features, "12").conv, "1"), "2")(features_12_conv_1_1);  features_12_conv_1_1 = None
    features_12_conv_1_2_post_act_fake_quantizer = self.features_12_conv_1_2_post_act_fake_quantizer(features_12_conv_1_2);  features_12_conv_1_2 = None
    features_12_conv_2 = getattr(getattr(self.features, "12").conv, "2")(features_12_conv_1_2_post_act_fake_quantizer);  features_12_conv_1_2_post_act_fake_quantizer = None
    features_12_conv_3 = getattr(getattr(self.features, "12").conv, "3")(features_12_conv_2);  features_12_conv_2 = None
    add_6 = features_11_conv_3_post_act_fake_quantizer + features_12_conv_3;  features_11_conv_3_post_act_fake_quantizer = features_12_conv_3 = None
    add_6_post_act_fake_quantizer = self.add_6_post_act_fake_quantizer(add_6);  add_6 = None
    return add_6_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_12_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_12_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_6_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	2529.352 (rec:2529.352, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	2613.544 (rec:2613.544, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	2818.997 (rec:2818.997, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	2656.884 (rec:2656.884, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	2737.944 (rec:2737.944, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	2577.597 (rec:2577.597, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	2617.873 (rec:2617.873, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	3571.810 (rec:2754.152, round:817.658)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	3293.287 (rec:2732.476, round:560.812)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	3382.997 (rec:2882.405, round:500.592)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	3066.043 (rec:2608.922, round:457.121)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	3058.381 (rec:2638.198, round:420.183)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	3016.796 (rec:2627.940, round:388.856)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	3088.881 (rec:2726.921, round:361.960)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	3203.598 (rec:2866.575, round:337.023)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	3110.244 (rec:2795.084, round:315.159)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	3163.538 (rec:2868.884, round:294.654)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	2965.967 (rec:2689.916, round:276.051)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	2902.932 (rec:2643.854, round:259.078)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	2740.638 (rec:2497.040, round:243.599)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	2808.943 (rec:2579.603, round:229.340)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	2914.091 (rec:2698.191, round:215.899)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	2670.140 (rec:2466.046, round:204.094)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	2768.242 (rec:2575.979, round:192.264)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	3105.522 (rec:2924.353, round:181.169)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	3034.176 (rec:2863.509, round:170.667)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	2894.520 (rec:2733.790, round:160.731)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	2773.568 (rec:2622.235, round:151.333)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	2577.985 (rec:2435.843, round:142.142)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	2998.577 (rec:2865.636, round:132.941)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	2692.493 (rec:2568.437, round:124.057)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	2724.248 (rec:2608.734, round:115.514)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	2793.655 (rec:2685.960, round:107.695)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	2784.017 (rec:2684.576, round:99.441)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	2794.278 (rec:2702.931, round:91.346)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	2549.586 (rec:2466.299, round:83.287)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	2775.473 (rec:2700.224, round:75.249)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2867.963 (rec:2801.110, round:66.853)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	2929.902 (rec:2872.858, round:57.044)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	2970.004 (rec:2926.143, round:43.861)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_13_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_6_post_act_fake_quantizer, features_13_conv_0_0, features_13_conv_0_1, features_13_conv_0_2, features_13_conv_0_2_post_act_fake_quantizer, features_13_conv_1_0, features_13_conv_1_1, features_13_conv_1_2, features_13_conv_1_2_post_act_fake_quantizer, features_13_conv_2, features_13_conv_3, add_7, add_7_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_6_post_act_fake_quantizer):
    features_13_conv_0_0 = getattr(getattr(getattr(self.features, "13").conv, "0"), "0")(add_6_post_act_fake_quantizer)
    features_13_conv_0_1 = getattr(getattr(getattr(self.features, "13").conv, "0"), "1")(features_13_conv_0_0);  features_13_conv_0_0 = None
    features_13_conv_0_2 = getattr(getattr(getattr(self.features, "13").conv, "0"), "2")(features_13_conv_0_1);  features_13_conv_0_1 = None
    features_13_conv_0_2_post_act_fake_quantizer = self.features_13_conv_0_2_post_act_fake_quantizer(features_13_conv_0_2);  features_13_conv_0_2 = None
    features_13_conv_1_0 = getattr(getattr(getattr(self.features, "13").conv, "1"), "0")(features_13_conv_0_2_post_act_fake_quantizer);  features_13_conv_0_2_post_act_fake_quantizer = None
    features_13_conv_1_1 = getattr(getattr(getattr(self.features, "13").conv, "1"), "1")(features_13_conv_1_0);  features_13_conv_1_0 = None
    features_13_conv_1_2 = getattr(getattr(getattr(self.features, "13").conv, "1"), "2")(features_13_conv_1_1);  features_13_conv_1_1 = None
    features_13_conv_1_2_post_act_fake_quantizer = self.features_13_conv_1_2_post_act_fake_quantizer(features_13_conv_1_2);  features_13_conv_1_2 = None
    features_13_conv_2 = getattr(getattr(self.features, "13").conv, "2")(features_13_conv_1_2_post_act_fake_quantizer);  features_13_conv_1_2_post_act_fake_quantizer = None
    features_13_conv_3 = getattr(getattr(self.features, "13").conv, "3")(features_13_conv_2);  features_13_conv_2 = None
    add_7 = add_6_post_act_fake_quantizer + features_13_conv_3;  add_6_post_act_fake_quantizer = features_13_conv_3 = None
    add_7_post_act_fake_quantizer = self.add_7_post_act_fake_quantizer(add_7);  add_7 = None
    return add_7_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_13_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_13_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_7_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	3795.333 (rec:3795.333, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	4084.666 (rec:4084.666, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	4088.703 (rec:4088.703, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	4103.967 (rec:4103.967, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	4042.546 (rec:4042.546, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	4171.040 (rec:4171.040, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	4149.288 (rec:4149.288, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	4661.512 (rec:3835.298, round:826.214)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	4326.216 (rec:3745.974, round:580.242)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	4462.836 (rec:3942.137, round:520.700)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	4133.394 (rec:3657.011, round:476.382)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	4280.372 (rec:3839.163, round:441.209)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	4386.332 (rec:3976.590, round:409.743)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	4367.646 (rec:3986.025, round:381.621)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	4417.894 (rec:4062.118, round:355.776)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	4373.466 (rec:4040.002, round:333.463)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	4196.039 (rec:3882.536, round:313.503)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	4119.293 (rec:3824.046, round:295.247)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	4194.485 (rec:3915.299, round:279.186)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	3901.926 (rec:3637.678, round:264.248)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	3912.440 (rec:3662.626, round:249.814)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	4375.341 (rec:4138.586, round:236.755)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	4189.201 (rec:3965.339, round:223.862)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	3953.119 (rec:3740.962, round:212.157)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	3924.401 (rec:3723.239, round:201.162)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	4228.223 (rec:4037.443, round:190.780)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	3853.732 (rec:3672.982, round:180.750)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	3757.751 (rec:3586.952, round:170.799)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	3614.436 (rec:3453.152, round:161.284)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	3972.449 (rec:3819.859, round:152.590)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	3830.229 (rec:3686.366, round:143.863)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	3952.798 (rec:3817.598, round:135.200)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	3821.631 (rec:3694.788, round:126.843)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	4037.470 (rec:3918.972, round:118.498)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	3861.127 (rec:3750.699, round:110.428)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	3508.321 (rec:3405.942, round:102.379)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	3831.626 (rec:3737.842, round:93.785)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	3967.158 (rec:3882.247, round:84.911)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	3602.157 (rec:3528.047, round:74.110)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	3646.106 (rec:3586.794, round:59.312)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_14_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_7_post_act_fake_quantizer, features_14_conv_0_0, features_14_conv_0_1, features_14_conv_0_2, features_14_conv_0_2_post_act_fake_quantizer, features_14_conv_1_0, features_14_conv_1_1, features_14_conv_1_2, features_14_conv_1_2_post_act_fake_quantizer, features_14_conv_2, features_14_conv_3, features_14_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_7_post_act_fake_quantizer):
    features_14_conv_0_0 = getattr(getattr(getattr(self.features, "14").conv, "0"), "0")(add_7_post_act_fake_quantizer);  add_7_post_act_fake_quantizer = None
    features_14_conv_0_1 = getattr(getattr(getattr(self.features, "14").conv, "0"), "1")(features_14_conv_0_0);  features_14_conv_0_0 = None
    features_14_conv_0_2 = getattr(getattr(getattr(self.features, "14").conv, "0"), "2")(features_14_conv_0_1);  features_14_conv_0_1 = None
    features_14_conv_0_2_post_act_fake_quantizer = self.features_14_conv_0_2_post_act_fake_quantizer(features_14_conv_0_2);  features_14_conv_0_2 = None
    features_14_conv_1_0 = getattr(getattr(getattr(self.features, "14").conv, "1"), "0")(features_14_conv_0_2_post_act_fake_quantizer);  features_14_conv_0_2_post_act_fake_quantizer = None
    features_14_conv_1_1 = getattr(getattr(getattr(self.features, "14").conv, "1"), "1")(features_14_conv_1_0);  features_14_conv_1_0 = None
    features_14_conv_1_2 = getattr(getattr(getattr(self.features, "14").conv, "1"), "2")(features_14_conv_1_1);  features_14_conv_1_1 = None
    features_14_conv_1_2_post_act_fake_quantizer = self.features_14_conv_1_2_post_act_fake_quantizer(features_14_conv_1_2);  features_14_conv_1_2 = None
    features_14_conv_2 = getattr(getattr(self.features, "14").conv, "2")(features_14_conv_1_2_post_act_fake_quantizer);  features_14_conv_1_2_post_act_fake_quantizer = None
    features_14_conv_3 = getattr(getattr(self.features, "14").conv, "3")(features_14_conv_2);  features_14_conv_2 = None
    features_14_conv_3_post_act_fake_quantizer = self.features_14_conv_3_post_act_fake_quantizer(features_14_conv_3);  features_14_conv_3 = None
    return features_14_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_14_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_14_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_14_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	1784.573 (rec:1784.573, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	1760.792 (rec:1760.792, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	1776.611 (rec:1776.611, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	1968.852 (rec:1968.852, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	1774.884 (rec:1774.884, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	1685.587 (rec:1685.587, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	1691.158 (rec:1691.158, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	3072.455 (rec:1815.904, round:1256.551)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	2682.090 (rec:1703.213, round:978.876)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	2671.908 (rec:1765.955, round:905.953)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	2550.926 (rec:1700.028, round:850.898)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2600.815 (rec:1797.396, round:803.419)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	2505.171 (rec:1744.607, round:760.564)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2424.652 (rec:1702.098, round:722.553)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	2415.448 (rec:1727.243, round:688.205)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	2468.950 (rec:1812.488, round:656.462)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	2408.164 (rec:1781.429, round:626.735)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	2522.609 (rec:1923.487, round:599.122)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	2347.517 (rec:1773.751, round:573.767)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	2262.265 (rec:1712.756, round:549.509)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	2417.761 (rec:1891.087, round:526.674)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	2297.162 (rec:1791.627, round:505.535)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	2197.441 (rec:1712.468, round:484.974)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	2251.050 (rec:1785.642, round:465.408)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	2264.885 (rec:1817.227, round:447.658)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	2292.751 (rec:1862.716, round:430.035)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	2226.813 (rec:1814.530, round:412.284)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	2267.123 (rec:1871.571, round:395.552)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	2160.065 (rec:1780.730, round:379.335)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	2087.497 (rec:1724.484, round:363.013)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	2176.866 (rec:1829.624, round:347.243)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	2135.035 (rec:1803.531, round:331.504)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	2020.196 (rec:1704.982, round:315.214)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	2107.789 (rec:1808.476, round:299.313)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	1979.550 (rec:1697.288, round:282.262)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	1960.741 (rec:1695.803, round:264.938)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	2170.734 (rec:1924.418, round:246.316)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2040.753 (rec:1814.860, round:225.893)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	1976.757 (rec:1774.444, round:202.313)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	1993.645 (rec:1820.884, round:172.761)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_15_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_14_conv_3_post_act_fake_quantizer, features_15_conv_0_0, features_15_conv_0_1, features_15_conv_0_2, features_15_conv_0_2_post_act_fake_quantizer, features_15_conv_1_0, features_15_conv_1_1, features_15_conv_1_2, features_15_conv_1_2_post_act_fake_quantizer, features_15_conv_2, features_15_conv_3, add_8, add_8_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_14_conv_3_post_act_fake_quantizer):
    features_15_conv_0_0 = getattr(getattr(getattr(self.features, "15").conv, "0"), "0")(features_14_conv_3_post_act_fake_quantizer)
    features_15_conv_0_1 = getattr(getattr(getattr(self.features, "15").conv, "0"), "1")(features_15_conv_0_0);  features_15_conv_0_0 = None
    features_15_conv_0_2 = getattr(getattr(getattr(self.features, "15").conv, "0"), "2")(features_15_conv_0_1);  features_15_conv_0_1 = None
    features_15_conv_0_2_post_act_fake_quantizer = self.features_15_conv_0_2_post_act_fake_quantizer(features_15_conv_0_2);  features_15_conv_0_2 = None
    features_15_conv_1_0 = getattr(getattr(getattr(self.features, "15").conv, "1"), "0")(features_15_conv_0_2_post_act_fake_quantizer);  features_15_conv_0_2_post_act_fake_quantizer = None
    features_15_conv_1_1 = getattr(getattr(getattr(self.features, "15").conv, "1"), "1")(features_15_conv_1_0);  features_15_conv_1_0 = None
    features_15_conv_1_2 = getattr(getattr(getattr(self.features, "15").conv, "1"), "2")(features_15_conv_1_1);  features_15_conv_1_1 = None
    features_15_conv_1_2_post_act_fake_quantizer = self.features_15_conv_1_2_post_act_fake_quantizer(features_15_conv_1_2);  features_15_conv_1_2 = None
    features_15_conv_2 = getattr(getattr(self.features, "15").conv, "2")(features_15_conv_1_2_post_act_fake_quantizer);  features_15_conv_1_2_post_act_fake_quantizer = None
    features_15_conv_3 = getattr(getattr(self.features, "15").conv, "3")(features_15_conv_2);  features_15_conv_2 = None
    add_8 = features_14_conv_3_post_act_fake_quantizer + features_15_conv_3;  features_14_conv_3_post_act_fake_quantizer = features_15_conv_3 = None
    add_8_post_act_fake_quantizer = self.add_8_post_act_fake_quantizer(add_8);  add_8 = None
    return add_8_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_15_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_15_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_8_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	6298.903 (rec:6298.903, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	5807.217 (rec:5807.217, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	5382.306 (rec:5382.306, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	5439.584 (rec:5439.584, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	4986.992 (rec:4986.992, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	5030.152 (rec:5030.152, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	5029.276 (rec:5029.276, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	7485.368 (rec:4836.547, round:2648.821)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	6717.359 (rec:4663.830, round:2053.530)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	6636.794 (rec:4738.163, round:1898.631)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	6462.090 (rec:4682.424, round:1779.667)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	6214.745 (rec:4537.601, round:1677.144)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	6028.359 (rec:4447.988, round:1580.371)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	5874.722 (rec:4384.872, round:1489.850)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	5785.847 (rec:4380.350, round:1405.497)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	5699.462 (rec:4372.803, round:1326.659)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	5718.244 (rec:4467.054, round:1251.190)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	5589.398 (rec:4408.350, round:1181.049)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	5424.426 (rec:4311.664, round:1112.763)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	5599.485 (rec:4551.637, round:1047.849)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	5324.760 (rec:4337.330, round:987.430)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	5357.967 (rec:4427.228, round:930.739)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	5419.698 (rec:4542.785, round:876.913)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	5291.800 (rec:4466.328, round:825.472)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	5299.501 (rec:4523.642, round:775.860)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	5076.619 (rec:4348.415, round:728.204)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	5180.528 (rec:4497.577, round:682.951)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	5127.010 (rec:4487.284, round:639.726)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	4974.172 (rec:4375.405, round:598.768)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	5054.439 (rec:4496.261, round:558.179)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	4897.195 (rec:4379.018, round:518.178)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	4979.058 (rec:4499.201, round:479.857)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	4894.881 (rec:4451.543, round:443.337)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	4878.397 (rec:4471.644, round:406.754)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	4723.471 (rec:4352.975, round:370.496)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	4820.908 (rec:4485.890, round:335.017)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	4661.569 (rec:4363.080, round:298.490)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	4735.180 (rec:4474.211, round:260.969)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	4711.425 (rec:4490.624, round:220.801)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	4697.167 (rec:4522.901, round:174.265)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_16_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_8_post_act_fake_quantizer, features_16_conv_0_0, features_16_conv_0_1, features_16_conv_0_2, features_16_conv_0_2_post_act_fake_quantizer, features_16_conv_1_0, features_16_conv_1_1, features_16_conv_1_2, features_16_conv_1_2_post_act_fake_quantizer, features_16_conv_2, features_16_conv_3, add_9, add_9_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_8_post_act_fake_quantizer):
    features_16_conv_0_0 = getattr(getattr(getattr(self.features, "16").conv, "0"), "0")(add_8_post_act_fake_quantizer)
    features_16_conv_0_1 = getattr(getattr(getattr(self.features, "16").conv, "0"), "1")(features_16_conv_0_0);  features_16_conv_0_0 = None
    features_16_conv_0_2 = getattr(getattr(getattr(self.features, "16").conv, "0"), "2")(features_16_conv_0_1);  features_16_conv_0_1 = None
    features_16_conv_0_2_post_act_fake_quantizer = self.features_16_conv_0_2_post_act_fake_quantizer(features_16_conv_0_2);  features_16_conv_0_2 = None
    features_16_conv_1_0 = getattr(getattr(getattr(self.features, "16").conv, "1"), "0")(features_16_conv_0_2_post_act_fake_quantizer);  features_16_conv_0_2_post_act_fake_quantizer = None
    features_16_conv_1_1 = getattr(getattr(getattr(self.features, "16").conv, "1"), "1")(features_16_conv_1_0);  features_16_conv_1_0 = None
    features_16_conv_1_2 = getattr(getattr(getattr(self.features, "16").conv, "1"), "2")(features_16_conv_1_1);  features_16_conv_1_1 = None
    features_16_conv_1_2_post_act_fake_quantizer = self.features_16_conv_1_2_post_act_fake_quantizer(features_16_conv_1_2);  features_16_conv_1_2 = None
    features_16_conv_2 = getattr(getattr(self.features, "16").conv, "2")(features_16_conv_1_2_post_act_fake_quantizer);  features_16_conv_1_2_post_act_fake_quantizer = None
    features_16_conv_3 = getattr(getattr(self.features, "16").conv, "3")(features_16_conv_2);  features_16_conv_2 = None
    add_9 = add_8_post_act_fake_quantizer + features_16_conv_3;  add_8_post_act_fake_quantizer = features_16_conv_3 = None
    add_9_post_act_fake_quantizer = self.add_9_post_act_fake_quantizer(add_9);  add_9 = None
    return add_9_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_16_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_16_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_9_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	9546.952 (rec:9546.952, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	8328.104 (rec:8328.104, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	8502.116 (rec:8502.116, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	8130.133 (rec:8130.133, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	8357.430 (rec:8357.430, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	7800.986 (rec:7800.986, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	7573.588 (rec:7573.588, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	10270.039 (rec:7693.447, round:2576.592)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	9513.303 (rec:7477.893, round:2035.410)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	9190.622 (rec:7314.550, round:1876.072)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	9110.043 (rec:7354.215, round:1755.828)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	9102.445 (rec:7452.404, round:1650.041)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	9007.766 (rec:7453.698, round:1554.068)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	9244.185 (rec:7779.816, round:1464.368)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	8869.027 (rec:7486.522, round:1382.505)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	9142.715 (rec:7836.377, round:1306.337)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	9023.236 (rec:7787.519, round:1235.718)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	8974.961 (rec:7804.907, round:1170.054)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	8947.464 (rec:7838.406, round:1109.057)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	8860.146 (rec:7809.182, round:1050.964)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	8581.330 (rec:7585.738, round:995.592)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	8799.752 (rec:7855.951, round:943.801)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	7938.526 (rec:7044.398, round:894.128)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	8673.877 (rec:7826.407, round:847.470)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	8627.898 (rec:7825.270, round:802.629)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	8579.639 (rec:7820.468, round:759.171)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	7923.261 (rec:7205.067, round:718.194)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	8005.170 (rec:7326.474, round:678.697)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	7991.132 (rec:7349.754, round:641.378)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	8372.124 (rec:7767.833, round:604.291)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	7794.706 (rec:7226.515, round:568.191)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	8027.053 (rec:7493.927, round:533.126)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	8030.186 (rec:7531.435, round:498.751)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	8481.065 (rec:8016.713, round:464.352)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	7739.258 (rec:7309.179, round:430.079)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	7941.711 (rec:7546.083, round:395.628)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	7351.382 (rec:6991.375, round:360.007)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	8123.801 (rec:7801.365, round:322.436)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	8130.780 (rec:7850.103, round:280.677)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	8049.592 (rec:7818.515, round:231.077)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_17_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_9_post_act_fake_quantizer, features_17_conv_0_0, features_17_conv_0_1, features_17_conv_0_2, features_17_conv_0_2_post_act_fake_quantizer, features_17_conv_1_0, features_17_conv_1_1, features_17_conv_1_2, features_17_conv_1_2_post_act_fake_quantizer, features_17_conv_2, features_17_conv_3, features_17_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_9_post_act_fake_quantizer):
    features_17_conv_0_0 = getattr(getattr(getattr(self.features, "17").conv, "0"), "0")(add_9_post_act_fake_quantizer);  add_9_post_act_fake_quantizer = None
    features_17_conv_0_1 = getattr(getattr(getattr(self.features, "17").conv, "0"), "1")(features_17_conv_0_0);  features_17_conv_0_0 = None
    features_17_conv_0_2 = getattr(getattr(getattr(self.features, "17").conv, "0"), "2")(features_17_conv_0_1);  features_17_conv_0_1 = None
    features_17_conv_0_2_post_act_fake_quantizer = self.features_17_conv_0_2_post_act_fake_quantizer(features_17_conv_0_2);  features_17_conv_0_2 = None
    features_17_conv_1_0 = getattr(getattr(getattr(self.features, "17").conv, "1"), "0")(features_17_conv_0_2_post_act_fake_quantizer);  features_17_conv_0_2_post_act_fake_quantizer = None
    features_17_conv_1_1 = getattr(getattr(getattr(self.features, "17").conv, "1"), "1")(features_17_conv_1_0);  features_17_conv_1_0 = None
    features_17_conv_1_2 = getattr(getattr(getattr(self.features, "17").conv, "1"), "2")(features_17_conv_1_1);  features_17_conv_1_1 = None
    features_17_conv_1_2_post_act_fake_quantizer = self.features_17_conv_1_2_post_act_fake_quantizer(features_17_conv_1_2);  features_17_conv_1_2 = None
    features_17_conv_2 = getattr(getattr(self.features, "17").conv, "2")(features_17_conv_1_2_post_act_fake_quantizer);  features_17_conv_1_2_post_act_fake_quantizer = None
    features_17_conv_3 = getattr(getattr(self.features, "17").conv, "3")(features_17_conv_2);  features_17_conv_2 = None
    features_17_conv_3_post_act_fake_quantizer = self.features_17_conv_3_post_act_fake_quantizer(features_17_conv_3);  features_17_conv_3 = None
    return features_17_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_17_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_17_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_17_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	2960.826 (rec:2960.826, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	2877.087 (rec:2877.087, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	2889.104 (rec:2889.104, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	2627.549 (rec:2627.549, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	2565.028 (rec:2565.028, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	2549.598 (rec:2549.598, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	2661.707 (rec:2661.707, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	6702.525 (rec:2569.569, round:4132.957)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	6360.519 (rec:2971.175, round:3389.344)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	5669.363 (rec:2484.694, round:3184.669)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	5429.884 (rec:2392.950, round:3036.934)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	5370.513 (rec:2462.804, round:2907.709)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	5178.582 (rec:2390.185, round:2788.397)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	5180.086 (rec:2502.411, round:2677.675)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	5240.703 (rec:2668.835, round:2571.867)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	4946.686 (rec:2474.516, round:2472.169)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	4834.404 (rec:2459.836, round:2374.568)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	4824.613 (rec:2543.302, round:2281.311)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	4643.765 (rec:2451.443, round:2192.323)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	4791.932 (rec:2686.340, round:2105.592)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	4562.745 (rec:2539.588, round:2023.156)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	4327.506 (rec:2383.010, round:1944.496)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	4305.980 (rec:2439.198, round:1866.782)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	4183.535 (rec:2391.998, round:1791.537)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	4152.109 (rec:2434.839, round:1717.271)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	4206.515 (rec:2560.232, round:1646.283)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	4022.651 (rec:2445.899, round:1576.752)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	4193.775 (rec:2687.076, round:1506.699)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	4064.351 (rec:2626.501, round:1437.850)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	3908.203 (rec:2537.427, round:1370.776)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	4036.860 (rec:2733.134, round:1303.726)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	3818.103 (rec:2581.514, round:1236.589)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	3713.506 (rec:2544.741, round:1168.765)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	3622.748 (rec:2522.055, round:1100.693)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	3573.210 (rec:2542.882, round:1030.328)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	3420.738 (rec:2464.237, round:956.501)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	3394.279 (rec:2515.805, round:878.473)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	3246.521 (rec:2452.273, round:794.248)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	3378.851 (rec:2679.716, round:699.135)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	3061.454 (rec:2473.756, round:587.697)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_18_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_17_conv_3_post_act_fake_quantizer, features_18_0, features_18_1, features_18_2, adaptive_avg_pool2d, flatten, classifier_0, classifier_0_post_act_fake_quantizer, classifier_1]
[MQBENCH] INFO: 


def forward(self, features_17_conv_3_post_act_fake_quantizer):
    features_18_0 = getattr(getattr(self.features, "18"), "0")(features_17_conv_3_post_act_fake_quantizer);  features_17_conv_3_post_act_fake_quantizer = None
    features_18_1 = getattr(getattr(self.features, "18"), "1")(features_18_0);  features_18_0 = None
    features_18_2 = getattr(getattr(self.features, "18"), "2")(features_18_1);  features_18_1 = None
    adaptive_avg_pool2d = torch.nn.functional.adaptive_avg_pool2d(features_18_2, (1, 1));  features_18_2 = None
    flatten = torch.flatten(adaptive_avg_pool2d, 1);  adaptive_avg_pool2d = None
    classifier_0 = getattr(self.classifier, "0")(flatten);  flatten = None
    classifier_0_post_act_fake_quantizer = self.classifier_0_post_act_fake_quantizer(classifier_0);  classifier_0 = None
    classifier_1 = getattr(self.classifier, "1")(classifier_0_post_act_fake_quantizer);  classifier_0_post_act_fake_quantizer = None
    return classifier_1
    
[MQBENCH] INFO: learn the scale for classifier_0_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	346.950 (rec:346.950, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	470.021 (rec:470.021, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	338.338 (rec:338.338, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	419.259 (rec:419.259, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	481.044 (rec:481.044, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	451.117 (rec:451.117, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	387.596 (rec:387.596, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	15224.285 (rec:410.580, round:14813.706)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	9462.923 (rec:474.392, round:8988.531)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	8789.326 (rec:473.032, round:8316.294)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	8311.559 (rec:466.445, round:7845.113)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	7824.221 (rec:393.372, round:7430.849)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	7327.686 (rec:285.175, round:7042.511)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	7079.083 (rec:408.931, round:6670.152)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	6872.910 (rec:563.496, round:6309.414)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	6349.802 (rec:392.802, round:5957.000)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	6035.541 (rec:423.219, round:5612.321)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	5741.523 (rec:465.772, round:5275.752)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	5377.644 (rec:432.899, round:4944.745)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	5026.485 (rec:401.122, round:4625.364)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	4892.227 (rec:579.949, round:4312.277)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	4497.761 (rec:487.777, round:4009.983)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	4201.522 (rec:487.108, round:3714.413)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	3723.214 (rec:298.493, round:3424.720)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	3601.995 (rec:455.621, round:3146.374)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	3226.361 (rec:352.678, round:2873.682)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	3029.728 (rec:420.259, round:2609.469)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	2738.701 (rec:386.165, round:2352.536)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	2527.605 (rec:427.585, round:2100.020)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	2180.171 (rec:323.652, round:1856.519)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	1995.034 (rec:371.930, round:1623.104)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	1775.561 (rec:378.990, round:1396.571)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	1520.861 (rec:345.322, round:1175.539)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	1308.022 (rec:345.007, round:963.016)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	1115.330 (rec:358.648, round:756.682)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	863.755 (rec:296.996, round:566.759)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	818.746 (rec:424.460, round:394.287)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	572.655 (rec:324.468, round:248.186)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	430.942 (rec:297.478, round:133.464)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	442.907 (rec:381.058, round:61.849)	b=2.00	count=20000
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node x_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.1.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.1.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.1.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_1_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.1.conv.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.1.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_1_conv_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_2_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_2_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_2_conv_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_3_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_3_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_4_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_4_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_4_conv_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_5_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_5_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_6_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_6_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_7_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_7_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_7_conv_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_8_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_8_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_9_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_9_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_4_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_10_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_10_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_5_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_11_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_11_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_11_conv_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_12_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_12_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_6_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_13_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_13_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_7_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_14_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_14_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_14_conv_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_15_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_15_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_8_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_16_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_16_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_9_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_17_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_17_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_17_conv_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.18.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.18.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.18.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node classifier.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node classifier_0_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node classifier.1 in quant
2025-08-18 12:01:19,899 | INFO | ✔ END: advanced PTQ reconstruction (elapsed 2787.23s)
2025-08-18 12:01:20,140 | INFO | ▶ START: enable_quantization (simulate INT8)
[MQBENCH] INFO: Disable observer and Enable quantize.
2025-08-18 12:01:20,154 | INFO | ✔ END: enable_quantization (simulate INT8) (elapsed 0.01s)
2025-08-18 12:01:20,154 | INFO | ✔ END: prepare_by_platform(Academic) (elapsed 2806.39s)
2025-08-18 12:01:20,156 | INFO | ▶ START: evaluate INT8-sim
2025-08-18 12:01:22,937 | INFO | [EVAL_INT8] progress: 50 batches, running top1=4.62%
2025-08-18 12:01:27,765 | INFO | [EVAL_INT8] progress: 100 batches, running top1=2.33%
2025-08-18 12:01:32,365 | INFO | [EVAL_INT8] progress: 150 batches, running top1=1.55%
2025-08-18 12:01:39,700 | INFO | [EVAL_INT8] progress: 200 batches, running top1=1.16%
2025-08-18 12:01:47,938 | INFO | [EVAL_INT8] progress: 250 batches, running top1=0.93%
2025-08-18 12:01:58,960 | INFO | [EVAL_INT8] progress: 300 batches, running top1=0.78%
2025-08-18 12:02:08,574 | INFO | [EVAL_INT8] progress: 350 batches, running top1=0.67%
2025-08-18 12:02:16,451 | INFO | [EVAL_INT8] progress: 400 batches, running top1=0.59%
2025-08-18 12:02:20,732 | INFO | [EVAL_INT8] progress: 450 batches, running top1=0.52%
2025-08-18 12:02:24,588 | INFO | [EVAL_INT8] progress: 500 batches, running top1=0.47%
2025-08-18 12:02:29,649 | INFO | [EVAL_INT8] progress: 550 batches, running top1=0.43%
2025-08-18 12:02:34,102 | INFO | [EVAL_INT8] progress: 600 batches, running top1=0.39%
2025-08-18 12:02:38,209 | INFO | [EVAL_INT8] progress: 650 batches, running top1=0.36%
2025-08-18 12:02:44,484 | INFO | [EVAL_INT8] progress: 700 batches, running top1=0.33%
2025-08-18 12:02:50,889 | INFO | [EVAL_INT8] progress: 750 batches, running top1=0.31%
2025-08-18 12:02:55,434 | INFO | [EVAL_INT8] done: 782 batches in 95.28s, top1=0.30%
2025-08-18 12:02:55,434 | INFO | [PTQ][mobilenet_v2][Academic] [ADV] Top-1 = 0.30%
2025-08-18 12:02:55,434 | INFO | ✔ END: evaluate INT8-sim (elapsed 95.28s)
2025-08-18 12:02:55,434 | INFO | ▶ START: evaluate FP32 baseline
2025-08-18 12:02:57,961 | INFO | [EVAL_FP32] progress: 50 batches, running top1=78.28%
2025-08-18 12:03:00,250 | INFO | [EVAL_FP32] progress: 100 batches, running top1=79.39%
2025-08-18 12:03:02,200 | INFO | [EVAL_FP32] progress: 150 batches, running top1=78.90%
2025-08-18 12:03:04,155 | INFO | [EVAL_FP32] progress: 200 batches, running top1=78.18%
2025-08-18 12:03:06,413 | INFO | [EVAL_FP32] progress: 250 batches, running top1=78.02%
2025-08-18 12:03:08,567 | INFO | [EVAL_FP32] progress: 300 batches, running top1=78.35%
2025-08-18 12:03:10,475 | INFO | [EVAL_FP32] progress: 350 batches, running top1=77.30%
2025-08-18 12:03:12,493 | INFO | [EVAL_FP32] progress: 400 batches, running top1=75.82%
2025-08-18 12:03:14,537 | INFO | [EVAL_FP32] progress: 450 batches, running top1=75.21%
2025-08-18 12:03:16,633 | INFO | [EVAL_FP32] progress: 500 batches, running top1=74.27%
2025-08-18 12:03:18,746 | INFO | [EVAL_FP32] progress: 550 batches, running top1=73.65%
2025-08-18 12:03:20,680 | INFO | [EVAL_FP32] progress: 600 batches, running top1=73.09%
2025-08-18 12:03:22,649 | INFO | [EVAL_FP32] progress: 650 batches, running top1=72.71%
2025-08-18 12:03:24,586 | INFO | [EVAL_FP32] progress: 700 batches, running top1=72.16%
2025-08-18 12:03:26,469 | INFO | [EVAL_FP32] progress: 750 batches, running top1=72.19%
2025-08-18 12:03:27,717 | INFO | [EVAL_FP32] done: 782 batches in 32.28s, top1=72.15%
2025-08-18 12:03:27,717 | INFO | [FP32] Top-1 = 72.15% (expected ~None)
2025-08-18 12:03:27,717 | INFO | ✔ END: evaluate FP32 baseline (elapsed 32.28s)
2025-08-18 12:03:27,717 | INFO | ▶ START: extract model logits
2025-08-18 12:03:27,719 | INFO | Extracting logits from both models...

============================================================
BASELINE ACCURACIES (Before Clustering)
============================================================
  FP32 Model: 72.15%
  Baseline PTQ: 0.30%
  PTQ Degradation: 71.85%
============================================================
Extracting logits from quantized and full-precision models...
2025-08-18 12:03:29,491 | INFO | Processed 5 batches
2025-08-18 12:03:30,315 | INFO | Processed 10 batches
2025-08-18 12:03:32,241 | INFO | Extracted logits: Q=torch.Size([640, 1000]), FP=torch.Size([640, 1000])
Logits extraction complete.
Quantized logits shape: torch.Size([640, 1000])
Full-precision logits shape: torch.Size([640, 1000])
🔍 Parameter ranges to test:
  Alpha values: [0.2, 0.4, 0.6, 0.8, 1.0]
  Cluster numbers: [8, 16, 32, 64]
  PCA dimensions: [25, 50, 100]
  Total combinations: 60
🚀 Running all 60 combinations...

🔄 [1/60] Running with alpha=0.2, num_clusters=8, pca_dim=25
[Alpha=0.20] Top-1 Accuracy: 0.27%
[Alpha=0.20] Top-5 Accuracy: 1.12%
✅ Result: Top-1: 0.27%, Top-5: 1.12%

🔄 [2/60] Running with alpha=0.2, num_clusters=8, pca_dim=50
[Alpha=0.20] Top-1 Accuracy: 0.30%
[Alpha=0.20] Top-5 Accuracy: 1.12%
✅ Result: Top-1: 0.30%, Top-5: 1.12%

🔄 [3/60] Running with alpha=0.2, num_clusters=8, pca_dim=100
[Alpha=0.20] Top-1 Accuracy: 0.30%
[Alpha=0.20] Top-5 Accuracy: 1.11%
✅ Result: Top-1: 0.30%, Top-5: 1.11%

🔄 [4/60] Running with alpha=0.2, num_clusters=16, pca_dim=25
[Alpha=0.20] Top-1 Accuracy: 0.30%
[Alpha=0.20] Top-5 Accuracy: 1.06%
✅ Result: Top-1: 0.30%, Top-5: 1.06%

🔄 [5/60] Running with alpha=0.2, num_clusters=16, pca_dim=50
[Alpha=0.20] Top-1 Accuracy: 0.29%
[Alpha=0.20] Top-5 Accuracy: 1.08%
✅ Result: Top-1: 0.29%, Top-5: 1.08%
💾 Saving intermediate results... (5 total combinations)
Results saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_results_20250818_120744.csv
💾 Recovery checkpoint saved: qdrop_lsq_mobilenet_v2_20250818_111330/recovery_checkpoint.json

🔄 [6/60] Running with alpha=0.2, num_clusters=16, pca_dim=100
[Alpha=0.20] Top-1 Accuracy: 0.30%
[Alpha=0.20] Top-5 Accuracy: 1.08%
✅ Result: Top-1: 0.30%, Top-5: 1.08%

🔄 [7/60] Running with alpha=0.2, num_clusters=32, pca_dim=25
[Alpha=0.20] Top-1 Accuracy: 0.31%
[Alpha=0.20] Top-5 Accuracy: 1.08%
✅ Result: Top-1: 0.31%, Top-5: 1.08%

🔄 [8/60] Running with alpha=0.2, num_clusters=32, pca_dim=50
[Alpha=0.20] Top-1 Accuracy: 0.31%
[Alpha=0.20] Top-5 Accuracy: 1.05%
✅ Result: Top-1: 0.31%, Top-5: 1.05%

🔄 [9/60] Running with alpha=0.2, num_clusters=32, pca_dim=100
[Alpha=0.20] Top-1 Accuracy: 0.26%
[Alpha=0.20] Top-5 Accuracy: 1.08%
✅ Result: Top-1: 0.26%, Top-5: 1.08%

🔄 [10/60] Running with alpha=0.2, num_clusters=64, pca_dim=25
[Alpha=0.20] Top-1 Accuracy: 0.28%
[Alpha=0.20] Top-5 Accuracy: 1.03%
✅ Result: Top-1: 0.28%, Top-5: 1.03%
💾 Saving intermediate results... (10 total combinations)
Results saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_results_20250818_121158.csv
💾 Recovery checkpoint saved: qdrop_lsq_mobilenet_v2_20250818_111330/recovery_checkpoint.json

🔄 [11/60] Running with alpha=0.2, num_clusters=64, pca_dim=50
[Alpha=0.20] Top-1 Accuracy: 0.27%
[Alpha=0.20] Top-5 Accuracy: 1.02%
✅ Result: Top-1: 0.27%, Top-5: 1.02%

🔄 [12/60] Running with alpha=0.2, num_clusters=64, pca_dim=100
[Alpha=0.20] Top-1 Accuracy: 0.29%
[Alpha=0.20] Top-5 Accuracy: 1.09%
✅ Result: Top-1: 0.29%, Top-5: 1.09%

🔄 [13/60] Running with alpha=0.4, num_clusters=8, pca_dim=25
[Alpha=0.40] Top-1 Accuracy: 0.23%
[Alpha=0.40] Top-5 Accuracy: 1.01%
✅ Result: Top-1: 0.23%, Top-5: 1.01%

🔄 [14/60] Running with alpha=0.4, num_clusters=8, pca_dim=50
[Alpha=0.40] Top-1 Accuracy: 0.26%
[Alpha=0.40] Top-5 Accuracy: 1.01%
✅ Result: Top-1: 0.26%, Top-5: 1.01%

🔄 [15/60] Running with alpha=0.4, num_clusters=8, pca_dim=100
[Alpha=0.40] Top-1 Accuracy: 0.25%
[Alpha=0.40] Top-5 Accuracy: 1.00%
✅ Result: Top-1: 0.25%, Top-5: 1.00%
💾 Saving intermediate results... (15 total combinations)
Results saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_results_20250818_121612.csv
💾 Recovery checkpoint saved: qdrop_lsq_mobilenet_v2_20250818_111330/recovery_checkpoint.json

🔄 [16/60] Running with alpha=0.4, num_clusters=16, pca_dim=25
[Alpha=0.40] Top-1 Accuracy: 0.22%
[Alpha=0.40] Top-5 Accuracy: 0.92%
✅ Result: Top-1: 0.22%, Top-5: 0.92%

🔄 [17/60] Running with alpha=0.4, num_clusters=16, pca_dim=50
[Alpha=0.40] Top-1 Accuracy: 0.26%
[Alpha=0.40] Top-5 Accuracy: 0.92%
✅ Result: Top-1: 0.26%, Top-5: 0.92%

🔄 [18/60] Running with alpha=0.4, num_clusters=16, pca_dim=100
[Alpha=0.40] Top-1 Accuracy: 0.26%
[Alpha=0.40] Top-5 Accuracy: 0.96%
✅ Result: Top-1: 0.26%, Top-5: 0.96%

🔄 [19/60] Running with alpha=0.4, num_clusters=32, pca_dim=25
[Alpha=0.40] Top-1 Accuracy: 0.20%
[Alpha=0.40] Top-5 Accuracy: 0.92%
✅ Result: Top-1: 0.20%, Top-5: 0.92%

🔄 [20/60] Running with alpha=0.4, num_clusters=32, pca_dim=50
[Alpha=0.40] Top-1 Accuracy: 0.21%
[Alpha=0.40] Top-5 Accuracy: 0.84%
✅ Result: Top-1: 0.21%, Top-5: 0.84%
💾 Saving intermediate results... (20 total combinations)
Results saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_results_20250818_122026.csv
💾 Recovery checkpoint saved: qdrop_lsq_mobilenet_v2_20250818_111330/recovery_checkpoint.json

🔄 [21/60] Running with alpha=0.4, num_clusters=32, pca_dim=100
[Alpha=0.40] Top-1 Accuracy: 0.18%
[Alpha=0.40] Top-5 Accuracy: 0.91%
✅ Result: Top-1: 0.18%, Top-5: 0.91%

🔄 [22/60] Running with alpha=0.4, num_clusters=64, pca_dim=25
[Alpha=0.40] Top-1 Accuracy: 0.15%
[Alpha=0.40] Top-5 Accuracy: 0.74%
✅ Result: Top-1: 0.15%, Top-5: 0.74%

🔄 [23/60] Running with alpha=0.4, num_clusters=64, pca_dim=50
[Alpha=0.40] Top-1 Accuracy: 0.15%
[Alpha=0.40] Top-5 Accuracy: 0.75%
✅ Result: Top-1: 0.15%, Top-5: 0.75%

🔄 [24/60] Running with alpha=0.4, num_clusters=64, pca_dim=100
[Alpha=0.40] Top-1 Accuracy: 0.16%
[Alpha=0.40] Top-5 Accuracy: 0.85%
✅ Result: Top-1: 0.16%, Top-5: 0.85%

🔄 [25/60] Running with alpha=0.6, num_clusters=8, pca_dim=25
[Alpha=0.60] Top-1 Accuracy: 0.15%
[Alpha=0.60] Top-5 Accuracy: 0.71%
✅ Result: Top-1: 0.15%, Top-5: 0.71%
💾 Saving intermediate results... (25 total combinations)
Results saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_results_20250818_122443.csv
💾 Recovery checkpoint saved: qdrop_lsq_mobilenet_v2_20250818_111330/recovery_checkpoint.json

🔄 [26/60] Running with alpha=0.6, num_clusters=8, pca_dim=50
[Alpha=0.60] Top-1 Accuracy: 0.16%
[Alpha=0.60] Top-5 Accuracy: 0.74%
✅ Result: Top-1: 0.16%, Top-5: 0.74%

🔄 [27/60] Running with alpha=0.6, num_clusters=8, pca_dim=100
[Alpha=0.60] Top-1 Accuracy: 0.17%
[Alpha=0.60] Top-5 Accuracy: 0.71%
✅ Result: Top-1: 0.17%, Top-5: 0.71%

🔄 [28/60] Running with alpha=0.6, num_clusters=16, pca_dim=25
[Alpha=0.60] Top-1 Accuracy: 0.11%
[Alpha=0.60] Top-5 Accuracy: 0.70%
✅ Result: Top-1: 0.11%, Top-5: 0.70%

🔄 [29/60] Running with alpha=0.6, num_clusters=16, pca_dim=50
[Alpha=0.60] Top-1 Accuracy: 0.15%
[Alpha=0.60] Top-5 Accuracy: 0.65%
✅ Result: Top-1: 0.15%, Top-5: 0.65%

🔄 [30/60] Running with alpha=0.6, num_clusters=16, pca_dim=100
[Alpha=0.60] Top-1 Accuracy: 0.15%
[Alpha=0.60] Top-5 Accuracy: 0.75%
✅ Result: Top-1: 0.15%, Top-5: 0.75%
💾 Saving intermediate results... (30 total combinations)
Results saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_results_20250818_122857.csv
💾 Recovery checkpoint saved: qdrop_lsq_mobilenet_v2_20250818_111330/recovery_checkpoint.json

🔄 [31/60] Running with alpha=0.6, num_clusters=32, pca_dim=25
[Alpha=0.60] Top-1 Accuracy: 0.12%
[Alpha=0.60] Top-5 Accuracy: 0.67%
✅ Result: Top-1: 0.12%, Top-5: 0.67%

🔄 [32/60] Running with alpha=0.6, num_clusters=32, pca_dim=50
[Alpha=0.60] Top-1 Accuracy: 0.15%
[Alpha=0.60] Top-5 Accuracy: 0.64%
✅ Result: Top-1: 0.15%, Top-5: 0.64%

🔄 [33/60] Running with alpha=0.6, num_clusters=32, pca_dim=100
[Alpha=0.60] Top-1 Accuracy: 0.12%
[Alpha=0.60] Top-5 Accuracy: 0.63%
✅ Result: Top-1: 0.12%, Top-5: 0.63%

🔄 [34/60] Running with alpha=0.6, num_clusters=64, pca_dim=25
[Alpha=0.60] Top-1 Accuracy: 0.11%
[Alpha=0.60] Top-5 Accuracy: 0.53%
✅ Result: Top-1: 0.11%, Top-5: 0.53%

🔄 [35/60] Running with alpha=0.6, num_clusters=64, pca_dim=50
[Alpha=0.60] Top-1 Accuracy: 0.10%
[Alpha=0.60] Top-5 Accuracy: 0.60%
✅ Result: Top-1: 0.10%, Top-5: 0.60%
💾 Saving intermediate results... (35 total combinations)
Results saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_results_20250818_123312.csv
💾 Recovery checkpoint saved: qdrop_lsq_mobilenet_v2_20250818_111330/recovery_checkpoint.json

🔄 [36/60] Running with alpha=0.6, num_clusters=64, pca_dim=100
[Alpha=0.60] Top-1 Accuracy: 0.11%
[Alpha=0.60] Top-5 Accuracy: 0.55%
✅ Result: Top-1: 0.11%, Top-5: 0.55%

🔄 [37/60] Running with alpha=0.8, num_clusters=8, pca_dim=25
[Alpha=0.80] Top-1 Accuracy: 0.12%
[Alpha=0.80] Top-5 Accuracy: 0.57%
✅ Result: Top-1: 0.12%, Top-5: 0.57%

🔄 [38/60] Running with alpha=0.8, num_clusters=8, pca_dim=50
[Alpha=0.80] Top-1 Accuracy: 0.11%
[Alpha=0.80] Top-5 Accuracy: 0.56%
✅ Result: Top-1: 0.11%, Top-5: 0.56%

🔄 [39/60] Running with alpha=0.8, num_clusters=8, pca_dim=100
[Alpha=0.80] Top-1 Accuracy: 0.12%
[Alpha=0.80] Top-5 Accuracy: 0.59%
✅ Result: Top-1: 0.12%, Top-5: 0.59%

🔄 [40/60] Running with alpha=0.8, num_clusters=16, pca_dim=25
[Alpha=0.80] Top-1 Accuracy: 0.10%
[Alpha=0.80] Top-5 Accuracy: 0.58%
✅ Result: Top-1: 0.10%, Top-5: 0.58%
💾 Saving intermediate results... (40 total combinations)
Results saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_results_20250818_123728.csv
💾 Recovery checkpoint saved: qdrop_lsq_mobilenet_v2_20250818_111330/recovery_checkpoint.json

🔄 [41/60] Running with alpha=0.8, num_clusters=16, pca_dim=50
[Alpha=0.80] Top-1 Accuracy: 0.11%
[Alpha=0.80] Top-5 Accuracy: 0.54%
✅ Result: Top-1: 0.11%, Top-5: 0.54%

🔄 [42/60] Running with alpha=0.8, num_clusters=16, pca_dim=100
[Alpha=0.80] Top-1 Accuracy: 0.12%
[Alpha=0.80] Top-5 Accuracy: 0.56%
✅ Result: Top-1: 0.12%, Top-5: 0.56%

🔄 [43/60] Running with alpha=0.8, num_clusters=32, pca_dim=25
[Alpha=0.80] Top-1 Accuracy: 0.13%
[Alpha=0.80] Top-5 Accuracy: 0.58%
✅ Result: Top-1: 0.13%, Top-5: 0.58%

🔄 [44/60] Running with alpha=0.8, num_clusters=32, pca_dim=50
[Alpha=0.80] Top-1 Accuracy: 0.12%
[Alpha=0.80] Top-5 Accuracy: 0.59%
✅ Result: Top-1: 0.12%, Top-5: 0.59%

🔄 [45/60] Running with alpha=0.8, num_clusters=32, pca_dim=100
[Alpha=0.80] Top-1 Accuracy: 0.11%
[Alpha=0.80] Top-5 Accuracy: 0.55%
✅ Result: Top-1: 0.11%, Top-5: 0.55%
💾 Saving intermediate results... (45 total combinations)
Results saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_results_20250818_124147.csv
💾 Recovery checkpoint saved: qdrop_lsq_mobilenet_v2_20250818_111330/recovery_checkpoint.json

🔄 [46/60] Running with alpha=0.8, num_clusters=64, pca_dim=25
[Alpha=0.80] Top-1 Accuracy: 0.11%
[Alpha=0.80] Top-5 Accuracy: 0.52%
✅ Result: Top-1: 0.11%, Top-5: 0.52%

🔄 [47/60] Running with alpha=0.8, num_clusters=64, pca_dim=50
[Alpha=0.80] Top-1 Accuracy: 0.10%
[Alpha=0.80] Top-5 Accuracy: 0.54%
✅ Result: Top-1: 0.10%, Top-5: 0.54%

🔄 [48/60] Running with alpha=0.8, num_clusters=64, pca_dim=100
[Alpha=0.80] Top-1 Accuracy: 0.10%
[Alpha=0.80] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.10%, Top-5: 0.50%

🔄 [49/60] Running with alpha=1.0, num_clusters=8, pca_dim=25
[Alpha=1.00] Top-1 Accuracy: 0.12%
[Alpha=1.00] Top-5 Accuracy: 0.54%
✅ Result: Top-1: 0.12%, Top-5: 0.54%

🔄 [50/60] Running with alpha=1.0, num_clusters=8, pca_dim=50
[Alpha=1.00] Top-1 Accuracy: 0.11%
[Alpha=1.00] Top-5 Accuracy: 0.53%
✅ Result: Top-1: 0.11%, Top-5: 0.53%
💾 Saving intermediate results... (50 total combinations)
Results saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_results_20250818_124603.csv
💾 Recovery checkpoint saved: qdrop_lsq_mobilenet_v2_20250818_111330/recovery_checkpoint.json

🔄 [51/60] Running with alpha=1.0, num_clusters=8, pca_dim=100
[Alpha=1.00] Top-1 Accuracy: 0.11%
[Alpha=1.00] Top-5 Accuracy: 0.56%
✅ Result: Top-1: 0.11%, Top-5: 0.56%

🔄 [52/60] Running with alpha=1.0, num_clusters=16, pca_dim=25
[Alpha=1.00] Top-1 Accuracy: 0.11%
[Alpha=1.00] Top-5 Accuracy: 0.58%
✅ Result: Top-1: 0.11%, Top-5: 0.58%

🔄 [53/60] Running with alpha=1.0, num_clusters=16, pca_dim=50
[Alpha=1.00] Top-1 Accuracy: 0.12%
[Alpha=1.00] Top-5 Accuracy: 0.53%
✅ Result: Top-1: 0.12%, Top-5: 0.53%

🔄 [54/60] Running with alpha=1.0, num_clusters=16, pca_dim=100
[Alpha=1.00] Top-1 Accuracy: 0.11%
[Alpha=1.00] Top-5 Accuracy: 0.59%
✅ Result: Top-1: 0.11%, Top-5: 0.59%

🔄 [55/60] Running with alpha=1.0, num_clusters=32, pca_dim=25
[Alpha=1.00] Top-1 Accuracy: 0.12%
[Alpha=1.00] Top-5 Accuracy: 0.59%
✅ Result: Top-1: 0.12%, Top-5: 0.59%
💾 Saving intermediate results... (55 total combinations)
Results saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_results_20250818_125020.csv
💾 Recovery checkpoint saved: qdrop_lsq_mobilenet_v2_20250818_111330/recovery_checkpoint.json

🔄 [56/60] Running with alpha=1.0, num_clusters=32, pca_dim=50
[Alpha=1.00] Top-1 Accuracy: 0.12%
[Alpha=1.00] Top-5 Accuracy: 0.52%
✅ Result: Top-1: 0.12%, Top-5: 0.52%

🔄 [57/60] Running with alpha=1.0, num_clusters=32, pca_dim=100
[Alpha=1.00] Top-1 Accuracy: 0.10%
[Alpha=1.00] Top-5 Accuracy: 0.52%
✅ Result: Top-1: 0.10%, Top-5: 0.52%

🔄 [58/60] Running with alpha=1.0, num_clusters=64, pca_dim=25
[Alpha=1.00] Top-1 Accuracy: 0.10%
[Alpha=1.00] Top-5 Accuracy: 0.49%
✅ Result: Top-1: 0.10%, Top-5: 0.49%

🔄 [59/60] Running with alpha=1.0, num_clusters=64, pca_dim=50
[Alpha=1.00] Top-1 Accuracy: 0.11%
[Alpha=1.00] Top-5 Accuracy: 0.53%
✅ Result: Top-1: 0.11%, Top-5: 0.53%

🔄 [60/60] Running with alpha=1.0, num_clusters=64, pca_dim=100
2025-08-18 12:54:36,471 | INFO | ✔ END: extract model logits (elapsed 3068.75s)
[Alpha=1.00] Top-1 Accuracy: 0.09%
[Alpha=1.00] Top-5 Accuracy: 0.49%
✅ Result: Top-1: 0.09%, Top-5: 0.49%
💾 Saving intermediate results... (60 total combinations)
Results saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_results_20250818_125436.csv
💾 Recovery checkpoint saved: qdrop_lsq_mobilenet_v2_20250818_111330/recovery_checkpoint.json

================================================================================
SUMMARY OF ALL RESULTS
================================================================================
Alpha    Clusters   PCA_dim    Top-1      Top-5     
--------------------------------------------------
0.20     8          25         0.27       1.12      
0.20     8          50         0.30       1.12      
0.20     8          100        0.30       1.11      
0.20     16         25         0.30       1.06      
0.20     16         50         0.29       1.08      
0.20     16         100        0.30       1.08      
0.20     32         25         0.31       1.08      
0.20     32         50         0.31       1.05      
0.20     32         100        0.26       1.08      
0.20     64         25         0.28       1.03      
0.20     64         50         0.27       1.02      
0.20     64         100        0.29       1.09      
0.40     8          25         0.23       1.01      
0.40     8          50         0.26       1.01      
0.40     8          100        0.25       1.00      
0.40     16         25         0.22       0.92      
0.40     16         50         0.26       0.92      
0.40     16         100        0.26       0.96      
0.40     32         25         0.20       0.92      
0.40     32         50         0.21       0.84      
0.40     32         100        0.18       0.91      
0.40     64         25         0.15       0.74      
0.40     64         50         0.15       0.75      
0.40     64         100        0.16       0.85      
0.60     8          25         0.15       0.71      
0.60     8          50         0.16       0.74      
0.60     8          100        0.17       0.71      
0.60     16         25         0.11       0.70      
0.60     16         50         0.15       0.65      
0.60     16         100        0.15       0.75      
0.60     32         25         0.12       0.67      
0.60     32         50         0.15       0.64      
0.60     32         100        0.12       0.63      
0.60     64         25         0.11       0.53      
0.60     64         50         0.10       0.60      
0.60     64         100        0.11       0.55      
0.80     8          25         0.12       0.57      
0.80     8          50         0.11       0.56      
0.80     8          100        0.12       0.59      
0.80     16         25         0.10       0.58      
0.80     16         50         0.11       0.54      
0.80     16         100        0.12       0.56      
0.80     32         25         0.13       0.58      
0.80     32         50         0.12       0.59      
0.80     32         100        0.11       0.55      
0.80     64         25         0.11       0.52      
0.80     64         50         0.10       0.54      
0.80     64         100        0.10       0.50      
1.00     8          25         0.12       0.54      
1.00     8          50         0.11       0.53      
1.00     8          100        0.11       0.56      
1.00     16         25         0.11       0.58      
1.00     16         50         0.12       0.53      
1.00     16         100        0.11       0.59      
1.00     32         25         0.12       0.59      
1.00     32         50         0.12       0.52      
1.00     32         100        0.10       0.52      
1.00     64         25         0.10       0.49      
1.00     64         50         0.11       0.53      
1.00     64         100        0.09       0.49      

BEST RESULT:
  Alpha: 0.2
  Clusters: 32
  PCA_dim: 50
  Top-1 Accuracy: 0.31%
  Top-5 Accuracy: 1.05%

ACCURACY COMPARISON:
  FP32 Model: 72.15%
  Baseline PTQ: 0.30%
  Best Clustering: 0.31%
  PTQ Degradation: 71.85%
  Clustering Recovery: 0.01%
  Final Gap to FP32: 71.84%
Results saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_results_20250818_125436.csv
Summary saved to: qdrop_lsq_mobilenet_v2_20250818_111330/ptq_summary_20250818_125436.csv
✅ Experiment completed successfully!
Results saved in: qdrop_lsq_mobilenet_v2_20250818_111330
------------------------------------------
🎉 Experiment finished!
Results directory: qdrop_lsq_mobilenet_v2_20250818_111330
