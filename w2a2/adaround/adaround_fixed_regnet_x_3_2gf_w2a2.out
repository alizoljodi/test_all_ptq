🚀 Starting PTQ Experiment: adaround + fixed + regnet_x_3_2gf
==========================================
Parameters:
  Model: regnet_x_3_2gf
  Advanced Mode: adaround
  Quant Model: fixed
  Weight Bits: 2
  Activation Bits: 2
  Alpha: 0.5
  Clusters: 16
  PCA dim: 50
  Batch Size: 64
  Calib Batches: 32
  Logits Batches: 10
  Alpha List: 0.2 0.4 0.6 0.8 1.0
  Clusters List: 8 16 32 64
  PCA Dim List: 25 50 100
==========================================
🔄 Running experiment...
Time: Mon Aug 18 02:20:05 PM CEST 2025
------------------------------------------
2025-08-18 14:20:07,512 | INFO | Environment: {'torch': '2.8.0+cu128', 'torchvision': '0.23.0+cu128', 'cuda_available': True, 'device_name': 'NVIDIA L40S', 'capability': '8.9', 'cudnn_version': 91002}
2025-08-18 14:20:07,512 | INFO | ▶ START: load fp32 model (torchvision weights API)
0.2%0.4%0.6%0.9%1.1%1.3%1.5%1.7%1.9%2.1%2.3%2.6%2.8%3.0%3.2%3.4%3.6%3.8%4.0%4.3%4.5%4.7%4.9%5.1%5.3%5.5%5.7%6.0%6.2%6.4%6.6%6.8%7.0%7.2%7.4%7.7%7.9%8.1%8.3%8.5%8.7%8.9%9.1%9.4%9.6%9.8%10.0%10.2%10.4%10.6%10.8%11.1%11.3%11.5%11.7%11.9%12.1%12.3%12.6%12.8%13.0%13.2%13.4%13.6%13.8%14.0%14.3%14.5%14.7%14.9%15.1%15.3%15.5%15.7%16.0%16.2%16.4%16.6%16.8%17.0%17.2%17.4%17.7%17.9%18.1%18.3%18.5%18.7%18.9%19.1%19.4%19.6%19.8%20.0%20.2%20.4%20.6%20.8%21.1%21.3%21.5%21.7%21.9%22.1%22.3%22.6%22.8%23.0%23.2%23.4%23.6%23.8%24.0%24.3%24.5%24.7%24.9%25.1%25.3%25.5%25.7%26.0%26.2%26.4%26.6%26.8%27.0%27.2%27.4%27.7%27.9%28.1%28.3%28.5%28.7%28.9%29.1%29.4%29.6%29.8%30.0%30.2%30.4%30.6%30.8%31.1%31.3%31.5%31.7%31.9%32.1%32.3%32.5%32.8%33.0%33.2%33.4%33.6%33.8%34.0%34.3%34.5%34.7%34.9%35.1%35.3%35.5%35.7%36.0%36.2%36.4%36.6%36.8%37.0%37.2%37.4%37.7%37.9%38.1%38.3%38.5%38.7%38.9%39.1%39.4%39.6%39.8%40.0%40.2%40.4%40.6%40.8%41.1%41.3%41.5%41.7%41.9%42.1%42.3%42.5%42.8%43.0%43.2%43.4%43.6%43.8%44.0%44.3%44.5%44.7%44.9%45.1%45.3%45.5%45.7%46.0%46.2%46.4%46.6%46.8%47.0%47.2%47.4%47.7%47.9%48.1%48.3%48.5%48.7%48.9%49.1%49.4%49.6%49.8%50.0%50.2%50.4%50.6%50.8%51.1%51.3%51.5%51.7%51.9%52.1%52.3%52.5%52.8%53.0%53.2%53.4%53.6%53.8%54.0%54.2%54.5%54.7%54.9%55.1%55.3%55.5%55.7%56.0%56.2%56.4%56.6%56.8%57.0%57.2%57.4%57.7%57.9%58.1%58.3%58.5%58.7%58.9%59.1%59.4%59.6%59.8%60.0%60.2%60.4%60.6%60.8%61.1%61.3%61.5%61.7%61.9%62.1%62.3%62.5%62.8%63.0%63.2%63.4%63.6%63.8%64.0%64.2%64.5%64.7%64.9%65.1%65.3%65.5%65.7%66.0%66.2%66.4%66.6%66.8%67.0%67.2%67.4%67.7%67.9%68.1%68.3%68.5%68.7%68.9%69.1%69.4%69.6%69.8%70.0%70.2%70.4%70.6%70.8%71.1%71.3%71.5%71.7%71.9%72.1%72.3%72.5%72.8%73.0%73.2%73.4%73.6%73.8%74.0%74.2%74.5%74.7%74.9%75.1%75.3%75.5%75.7%75.9%76.2%76.4%76.6%76.8%77.0%77.2%77.4%77.7%77.9%78.1%78.3%78.5%78.7%78.9%79.1%79.4%79.6%79.8%80.0%80.2%80.4%80.6%80.8%81.1%81.3%81.5%81.7%81.9%82.1%82.3%82.5%82.8%83.0%83.2%83.4%83.6%83.8%84.0%84.2%84.5%84.7%84.9%85.1%85.3%85.5%85.7%85.9%86.2%86.4%86.6%86.8%87.0%87.2%87.4%87.7%87.9%88.1%88.3%88.5%88.7%88.9%89.1%89.4%89.6%89.8%90.0%90.2%90.4%90.6%90.8%91.1%91.3%91.5%91.7%91.9%92.1%92.3%92.5%92.8%93.0%93.2%93.4%93.6%93.8%94.0%94.2%94.5%94.7%94.9%95.1%95.3%95.5%95.7%95.9%96.2%96.4%96.6%96.8%97.0%97.2%97.4%97.6%97.9%98.1%98.3%98.5%98.7%98.9%99.1%99.4%99.6%99.8%100.0%100.0%
2025-08-18 14:20:11,626 | INFO | Model: regnet_x_3_2gf | Weights: RegNet_X_3_2GF_Weights.IMAGENET1K_V2 | Params: 15.30M | Ref acc@1=None
2025-08-18 14:20:11,626 | INFO | ✔ END: load fp32 model (torchvision weights API) (elapsed 4.11s)
2025-08-18 14:20:11,626 | INFO | ▶ START: build & check loaders
2025-08-18 14:20:11,643 | INFO | Val structure looks OK (1000 synset folders).
2025-08-18 14:20:11,660 | INFO | Train structure looks OK (1000 synset folders).
2025-08-18 14:20:19,904 | INFO | ImageNet train: 1281167 images, 1000 classes
2025-08-18 14:20:20,631 | INFO | ImageNet val: 50000 images, 1000 classes
2025-08-18 14:20:20,631 | INFO | Val dataset size: 50000 | batch_size=64 | calib_batches=32
Downloading: "https://download.pytorch.org/models/regnet_x_3_2gf-7071aa85.pth" to /home/alz07xz/.cache/torch/hub/checkpoints/regnet_x_3_2gf-7071aa85.pth
2025-08-18 14:20:21,518 | INFO | [SANITY] Batch[0] stats: mean=-0.2055, std=1.1229, min=-2.118, max=2.640
2025-08-18 14:20:21,518 | INFO | ✔ END: build & check loaders (elapsed 9.89s)
2025-08-18 14:20:21,523 | INFO | ▶ START: prepare_by_platform(Academic)
2025-08-18 14:20:21,524 | INFO | [Academic extra_config] {'extra_qconfig_dict': {'w_observer': 'MinMaxObserver', 'a_observer': 'EMAMinMaxObserver', 'w_fakequantize': 'AdaRoundFakeQuantize', 'a_fakequantize': 'FixedFakeQuantize', 'w_qscheme': {'bit': 2, 'symmetry': True, 'per_channel': True, 'pot_scale': False}, 'a_qscheme': {'bit': 2, 'symmetry': False, 'per_channel': False, 'pot_scale': False}}}
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MinMaxObserver Params: Symmetric: True / Bitwidth: 2 / Per channel: True / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: FixedFakeQuantize Params: {}
    Oberver:      EMAMinMaxObserver Params: Symmetric: False / Bitwidth: 2 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer fc to 8 bit.
[MQBENCH] INFO: Set layer stem.0 to 8 bit.
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant stem_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block1_block1_0_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block1_block1_0_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block1_block1_0_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block1_block1_1_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block1_block1_1_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block1_block1_1_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_0_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_0_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_0_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_1_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_1_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_1_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_2_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_2_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_2_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_3_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_3_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_3_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_4_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_4_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_4_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_5_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_5_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block2_block2_5_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_0_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_0_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_0_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_1_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_1_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_1_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_2_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_2_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_2_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_3_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_3_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_3_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_4_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_4_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_4_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_5_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_5_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_5_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_6_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_6_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_6_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_7_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_7_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_7_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_8_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_8_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_8_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_9_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_9_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_9_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_10_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_10_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_10_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_11_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_11_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_11_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_12_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_12_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_12_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_13_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_13_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_13_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_14_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_14_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block3_block3_14_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block4_block4_0_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block4_block4_0_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block4_block4_0_activation_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block4_block4_1_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant trunk_output_block4_block4_1_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: Set flatten post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant flatten_post_act_fake_quantizer
2025-08-18 14:20:22,020 | INFO | Modules (total): 374 -> 690
2025-08-18 14:20:22,020 | INFO | 'Quantish' modules detected after prepare: 316
2025-08-18 14:20:22,020 | INFO | ▶ START: calibration (enable_calibration + forward)
[MQBENCH] INFO: Enable observer and Disable quantize.
2025-08-18 14:20:23,683 | INFO | [CALIB] step=1/32 seen=64 (38.6 img/s)
2025-08-18 14:20:24,351 | INFO | [CALIB] step=10/32 seen=640 (275.0 img/s)
2025-08-18 14:20:25,086 | INFO | [CALIB] step=20/32 seen=1280 (418.1 img/s)
2025-08-18 14:20:26,702 | INFO | [CALIB] step=30/32 seen=1920 (410.4 img/s)
2025-08-18 14:20:27,079 | INFO | [CALIB] total images seen: 2048
2025-08-18 14:20:27,080 | INFO | ✔ END: calibration (enable_calibration + forward) (elapsed 5.06s)
2025-08-18 14:20:27,080 | INFO | ▶ START: advanced PTQ reconstruction
2025-08-18 14:20:29,017 | INFO | [ADV] cfg={'pattern': 'block', 'scale_lr': 4e-05, 'warm_up': 0.2, 'weight': 0.01, 'max_count': 20000, 'b_range': [20, 2], 'keep_gpu': True, 'round_mode': 'learned_hard_sigmoid', 'prob': 1.0}
2025-08-18 14:20:29,019 | INFO | [ADV] stacked tensors: 32 | total calib images: 2048
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare block reconstruction for stem_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [x_post_act_fake_quantizer, stem_0, stem_1, stem_2, stem_2_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, x_post_act_fake_quantizer):
    stem_0 = getattr(self.stem, "0")(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None
    stem_1 = getattr(self.stem, "1")(stem_0);  stem_0 = None
    stem_2 = getattr(self.stem, "2")(stem_1);  stem_1 = None
    stem_2_post_act_fake_quantizer = self.stem_2_post_act_fake_quantizer(stem_2);  stem_2 = None
    return stem_2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for stem_2_post_act_fake_quantizer
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
2025-08-18 14:20:34,217 | WARNING | /home/alz07xz/project/kmeans_results/MQBench/mqbench/advanced_ptq.py:220: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)
  float(total_loss), float(rec_loss), float(round_loss), b, self.count))

[MQBENCH] INFO: Total loss:	471.359 (rec:471.359, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	476.043 (rec:476.043, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	481.583 (rec:481.583, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	471.494 (rec:471.494, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	458.029 (rec:458.029, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	463.977 (rec:463.977, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	476.232 (rec:476.232, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	466.333 (rec:461.789, round:4.545)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	451.502 (rec:448.494, round:3.008)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	451.123 (rec:448.492, round:2.631)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	484.173 (rec:481.795, round:2.378)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	478.528 (rec:476.317, round:2.211)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	483.816 (rec:481.803, round:2.013)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	483.643 (rec:481.804, round:1.839)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	467.668 (rec:465.956, round:1.712)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	491.030 (rec:489.409, round:1.621)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	465.008 (rec:463.531, round:1.477)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	483.936 (rec:482.566, round:1.370)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	459.394 (rec:458.170, round:1.224)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	477.470 (rec:476.307, round:1.163)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	464.274 (rec:463.148, round:1.126)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	478.054 (rec:476.977, round:1.078)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	451.023 (rec:449.970, round:1.054)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	464.561 (rec:463.531, round:1.030)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	477.956 (rec:476.989, round:0.967)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	473.598 (rec:472.688, round:0.909)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	472.568 (rec:471.679, round:0.889)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	454.740 (rec:453.891, round:0.849)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	490.217 (rec:489.401, round:0.816)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	477.296 (rec:476.525, round:0.772)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	482.519 (rec:481.795, round:0.723)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	489.301 (rec:488.616, round:0.685)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	467.761 (rec:467.113, round:0.648)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	453.708 (rec:453.088, round:0.620)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	464.643 (rec:464.047, round:0.596)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	466.515 (rec:465.949, round:0.566)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	453.615 (rec:453.081, round:0.535)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	466.441 (rec:465.951, round:0.490)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	457.627 (rec:457.272, round:0.355)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	463.394 (rec:463.133, round:0.260)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block1_block1_0_proj_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [stem_2_post_act_fake_quantizer, trunk_output_block1_block1_0_proj_0, trunk_output_block1_block1_0_proj_1, trunk_output_block1_block1_0_f_a_0, trunk_output_block1_block1_0_f_a_1, trunk_output_block1_block1_0_f_a_2, trunk_output_block1_block1_0_f_a_2_post_act_fake_quantizer, trunk_output_block1_block1_0_f_b_0, trunk_output_block1_block1_0_f_b_1, trunk_output_block1_block1_0_f_b_2, trunk_output_block1_block1_0_f_b_2_post_act_fake_quantizer, trunk_output_block1_block1_0_f_c_0, trunk_output_block1_block1_0_f_c_1, add, trunk_output_block1_block1_0_activation, trunk_output_block1_block1_0_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, stem_2_post_act_fake_quantizer):
    trunk_output_block1_block1_0_proj_0 = getattr(getattr(self.trunk_output.block1, "block1-0").proj, "0")(stem_2_post_act_fake_quantizer)
    trunk_output_block1_block1_0_proj_1 = getattr(getattr(self.trunk_output.block1, "block1-0").proj, "1")(trunk_output_block1_block1_0_proj_0);  trunk_output_block1_block1_0_proj_0 = None
    trunk_output_block1_block1_0_f_a_0 = getattr(getattr(self.trunk_output.block1, "block1-0").f.a, "0")(stem_2_post_act_fake_quantizer);  stem_2_post_act_fake_quantizer = None
    trunk_output_block1_block1_0_f_a_1 = getattr(getattr(self.trunk_output.block1, "block1-0").f.a, "1")(trunk_output_block1_block1_0_f_a_0);  trunk_output_block1_block1_0_f_a_0 = None
    trunk_output_block1_block1_0_f_a_2 = getattr(getattr(self.trunk_output.block1, "block1-0").f.a, "2")(trunk_output_block1_block1_0_f_a_1);  trunk_output_block1_block1_0_f_a_1 = None
    trunk_output_block1_block1_0_f_a_2_post_act_fake_quantizer = self.trunk_output_block1_block1_0_f_a_2_post_act_fake_quantizer(trunk_output_block1_block1_0_f_a_2);  trunk_output_block1_block1_0_f_a_2 = None
    trunk_output_block1_block1_0_f_b_0 = getattr(getattr(self.trunk_output.block1, "block1-0").f.b, "0")(trunk_output_block1_block1_0_f_a_2_post_act_fake_quantizer);  trunk_output_block1_block1_0_f_a_2_post_act_fake_quantizer = None
    trunk_output_block1_block1_0_f_b_1 = getattr(getattr(self.trunk_output.block1, "block1-0").f.b, "1")(trunk_output_block1_block1_0_f_b_0);  trunk_output_block1_block1_0_f_b_0 = None
    trunk_output_block1_block1_0_f_b_2 = getattr(getattr(self.trunk_output.block1, "block1-0").f.b, "2")(trunk_output_block1_block1_0_f_b_1);  trunk_output_block1_block1_0_f_b_1 = None
    trunk_output_block1_block1_0_f_b_2_post_act_fake_quantizer = self.trunk_output_block1_block1_0_f_b_2_post_act_fake_quantizer(trunk_output_block1_block1_0_f_b_2);  trunk_output_block1_block1_0_f_b_2 = None
    trunk_output_block1_block1_0_f_c_0 = getattr(getattr(self.trunk_output.block1, "block1-0").f.c, "0")(trunk_output_block1_block1_0_f_b_2_post_act_fake_quantizer);  trunk_output_block1_block1_0_f_b_2_post_act_fake_quantizer = None
    trunk_output_block1_block1_0_f_c_1 = getattr(getattr(self.trunk_output.block1, "block1-0").f.c, "1")(trunk_output_block1_block1_0_f_c_0);  trunk_output_block1_block1_0_f_c_0 = None
    add = trunk_output_block1_block1_0_proj_1 + trunk_output_block1_block1_0_f_c_1;  trunk_output_block1_block1_0_proj_1 = trunk_output_block1_block1_0_f_c_1 = None
    trunk_output_block1_block1_0_activation = getattr(self.trunk_output.block1, "block1-0").activation(add);  add = None
    trunk_output_block1_block1_0_activation_post_act_fake_quantizer = self.trunk_output_block1_block1_0_activation_post_act_fake_quantizer(trunk_output_block1_block1_0_activation);  trunk_output_block1_block1_0_activation = None
    return trunk_output_block1_block1_0_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block1_block1_0_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block1_block1_0_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block1_block1_0_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	2383.609 (rec:2383.609, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	2585.012 (rec:2585.012, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	2662.058 (rec:2662.058, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	2743.894 (rec:2743.894, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	2370.630 (rec:2370.630, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	2695.394 (rec:2695.394, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	2816.530 (rec:2816.530, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	2924.725 (rec:2538.395, round:386.329)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	2883.875 (rec:2591.843, round:292.033)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	3086.352 (rec:2817.445, round:268.907)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	3393.006 (rec:3139.737, round:253.269)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2864.825 (rec:2625.274, round:239.551)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	3067.304 (rec:2839.423, round:227.881)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2837.521 (rec:2619.889, round:217.631)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	3004.886 (rec:2796.754, round:208.132)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	2868.135 (rec:2668.549, round:199.586)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	2891.286 (rec:2700.423, round:190.863)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	3270.848 (rec:3087.581, round:183.266)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	2929.772 (rec:2753.810, round:175.962)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	3129.263 (rec:2959.776, round:169.487)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	2654.338 (rec:2490.839, round:163.499)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	3140.072 (rec:2982.405, round:157.667)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	2908.364 (rec:2756.491, round:151.873)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	2716.804 (rec:2570.316, round:146.488)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	2493.063 (rec:2351.829, round:141.234)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	2716.544 (rec:2580.321, round:136.223)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	3046.387 (rec:2915.139, round:131.248)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	2816.363 (rec:2690.091, round:126.272)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	2931.314 (rec:2809.891, round:121.423)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	2790.737 (rec:2674.207, round:116.531)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	3071.670 (rec:2960.168, round:111.502)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	2576.771 (rec:2470.179, round:106.591)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	2490.612 (rec:2389.025, round:101.587)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	2832.454 (rec:2735.768, round:96.686)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	2817.313 (rec:2725.528, round:91.785)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	2686.621 (rec:2600.012, round:86.609)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	2750.758 (rec:2669.644, round:81.114)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2916.840 (rec:2841.784, round:75.056)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	2924.805 (rec:2856.546, round:68.259)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	2747.317 (rec:2687.290, round:60.027)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block1_block1_1_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block1_block1_0_activation_post_act_fake_quantizer, trunk_output_block1_block1_1_f_a_0, trunk_output_block1_block1_1_f_a_1, trunk_output_block1_block1_1_f_a_2, trunk_output_block1_block1_1_f_a_2_post_act_fake_quantizer, trunk_output_block1_block1_1_f_b_0, trunk_output_block1_block1_1_f_b_1, trunk_output_block1_block1_1_f_b_2, trunk_output_block1_block1_1_f_b_2_post_act_fake_quantizer, trunk_output_block1_block1_1_f_c_0, trunk_output_block1_block1_1_f_c_1, add_1, trunk_output_block1_block1_1_activation, trunk_output_block1_block1_1_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block1_block1_0_activation_post_act_fake_quantizer):
    trunk_output_block1_block1_1_f_a_0 = getattr(getattr(self.trunk_output.block1, "block1-1").f.a, "0")(trunk_output_block1_block1_0_activation_post_act_fake_quantizer)
    trunk_output_block1_block1_1_f_a_1 = getattr(getattr(self.trunk_output.block1, "block1-1").f.a, "1")(trunk_output_block1_block1_1_f_a_0);  trunk_output_block1_block1_1_f_a_0 = None
    trunk_output_block1_block1_1_f_a_2 = getattr(getattr(self.trunk_output.block1, "block1-1").f.a, "2")(trunk_output_block1_block1_1_f_a_1);  trunk_output_block1_block1_1_f_a_1 = None
    trunk_output_block1_block1_1_f_a_2_post_act_fake_quantizer = self.trunk_output_block1_block1_1_f_a_2_post_act_fake_quantizer(trunk_output_block1_block1_1_f_a_2);  trunk_output_block1_block1_1_f_a_2 = None
    trunk_output_block1_block1_1_f_b_0 = getattr(getattr(self.trunk_output.block1, "block1-1").f.b, "0")(trunk_output_block1_block1_1_f_a_2_post_act_fake_quantizer);  trunk_output_block1_block1_1_f_a_2_post_act_fake_quantizer = None
    trunk_output_block1_block1_1_f_b_1 = getattr(getattr(self.trunk_output.block1, "block1-1").f.b, "1")(trunk_output_block1_block1_1_f_b_0);  trunk_output_block1_block1_1_f_b_0 = None
    trunk_output_block1_block1_1_f_b_2 = getattr(getattr(self.trunk_output.block1, "block1-1").f.b, "2")(trunk_output_block1_block1_1_f_b_1);  trunk_output_block1_block1_1_f_b_1 = None
    trunk_output_block1_block1_1_f_b_2_post_act_fake_quantizer = self.trunk_output_block1_block1_1_f_b_2_post_act_fake_quantizer(trunk_output_block1_block1_1_f_b_2);  trunk_output_block1_block1_1_f_b_2 = None
    trunk_output_block1_block1_1_f_c_0 = getattr(getattr(self.trunk_output.block1, "block1-1").f.c, "0")(trunk_output_block1_block1_1_f_b_2_post_act_fake_quantizer);  trunk_output_block1_block1_1_f_b_2_post_act_fake_quantizer = None
    trunk_output_block1_block1_1_f_c_1 = getattr(getattr(self.trunk_output.block1, "block1-1").f.c, "1")(trunk_output_block1_block1_1_f_c_0);  trunk_output_block1_block1_1_f_c_0 = None
    add_1 = trunk_output_block1_block1_0_activation_post_act_fake_quantizer + trunk_output_block1_block1_1_f_c_1;  trunk_output_block1_block1_0_activation_post_act_fake_quantizer = trunk_output_block1_block1_1_f_c_1 = None
    trunk_output_block1_block1_1_activation = getattr(self.trunk_output.block1, "block1-1").activation(add_1);  add_1 = None
    trunk_output_block1_block1_1_activation_post_act_fake_quantizer = self.trunk_output_block1_block1_1_activation_post_act_fake_quantizer(trunk_output_block1_block1_1_activation);  trunk_output_block1_block1_1_activation = None
    return trunk_output_block1_block1_1_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block1_block1_1_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block1_block1_1_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block1_block1_1_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	3469.834 (rec:3469.834, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	3532.637 (rec:3532.637, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	3546.483 (rec:3546.483, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	4151.243 (rec:4151.243, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	4005.145 (rec:4005.145, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	4152.907 (rec:4152.907, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	3781.528 (rec:3781.528, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	3917.599 (rec:3507.147, round:410.452)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	4362.068 (rec:4027.811, round:334.257)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	3899.126 (rec:3586.289, round:312.837)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	3936.141 (rec:3639.491, round:296.650)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	4327.668 (rec:4044.267, round:283.402)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	4168.686 (rec:3896.689, round:271.997)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	4540.396 (rec:4278.851, round:261.545)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	3906.684 (rec:3654.684, round:252.001)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	3905.298 (rec:3662.076, round:243.222)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	3737.625 (rec:3502.626, round:234.998)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	3975.518 (rec:3748.254, round:227.263)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	4019.604 (rec:3799.671, round:219.933)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	3568.660 (rec:3355.825, round:212.835)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	3574.108 (rec:3367.886, round:206.223)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	4096.954 (rec:3897.230, round:199.724)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	4244.512 (rec:4051.076, round:193.436)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	4241.384 (rec:4054.058, round:187.326)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	3839.838 (rec:3658.662, round:181.176)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	3815.552 (rec:3640.333, round:175.220)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	3572.841 (rec:3403.397, round:169.444)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	4291.481 (rec:4127.995, round:163.487)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	4146.910 (rec:3989.279, round:157.632)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	3982.689 (rec:3830.838, round:151.852)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	4074.400 (rec:3928.566, round:145.834)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	3869.923 (rec:3730.374, round:139.549)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	4544.616 (rec:4411.319, round:133.297)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	3875.817 (rec:3748.887, round:126.929)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	3964.741 (rec:3844.471, round:120.270)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	3972.203 (rec:3859.006, round:113.197)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	3760.803 (rec:3655.279, round:105.524)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	3547.177 (rec:3450.060, round:97.117)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	4117.249 (rec:4029.515, round:87.734)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	4471.845 (rec:4395.340, round:76.504)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block2_block2_0_proj_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block1_block1_1_activation_post_act_fake_quantizer, trunk_output_block2_block2_0_proj_0, trunk_output_block2_block2_0_proj_1, trunk_output_block2_block2_0_f_a_0, trunk_output_block2_block2_0_f_a_1, trunk_output_block2_block2_0_f_a_2, trunk_output_block2_block2_0_f_a_2_post_act_fake_quantizer, trunk_output_block2_block2_0_f_b_0, trunk_output_block2_block2_0_f_b_1, trunk_output_block2_block2_0_f_b_2, trunk_output_block2_block2_0_f_b_2_post_act_fake_quantizer, trunk_output_block2_block2_0_f_c_0, trunk_output_block2_block2_0_f_c_1, add_2, trunk_output_block2_block2_0_activation, trunk_output_block2_block2_0_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block1_block1_1_activation_post_act_fake_quantizer):
    trunk_output_block2_block2_0_proj_0 = getattr(getattr(self.trunk_output.block2, "block2-0").proj, "0")(trunk_output_block1_block1_1_activation_post_act_fake_quantizer)
    trunk_output_block2_block2_0_proj_1 = getattr(getattr(self.trunk_output.block2, "block2-0").proj, "1")(trunk_output_block2_block2_0_proj_0);  trunk_output_block2_block2_0_proj_0 = None
    trunk_output_block2_block2_0_f_a_0 = getattr(getattr(self.trunk_output.block2, "block2-0").f.a, "0")(trunk_output_block1_block1_1_activation_post_act_fake_quantizer);  trunk_output_block1_block1_1_activation_post_act_fake_quantizer = None
    trunk_output_block2_block2_0_f_a_1 = getattr(getattr(self.trunk_output.block2, "block2-0").f.a, "1")(trunk_output_block2_block2_0_f_a_0);  trunk_output_block2_block2_0_f_a_0 = None
    trunk_output_block2_block2_0_f_a_2 = getattr(getattr(self.trunk_output.block2, "block2-0").f.a, "2")(trunk_output_block2_block2_0_f_a_1);  trunk_output_block2_block2_0_f_a_1 = None
    trunk_output_block2_block2_0_f_a_2_post_act_fake_quantizer = self.trunk_output_block2_block2_0_f_a_2_post_act_fake_quantizer(trunk_output_block2_block2_0_f_a_2);  trunk_output_block2_block2_0_f_a_2 = None
    trunk_output_block2_block2_0_f_b_0 = getattr(getattr(self.trunk_output.block2, "block2-0").f.b, "0")(trunk_output_block2_block2_0_f_a_2_post_act_fake_quantizer);  trunk_output_block2_block2_0_f_a_2_post_act_fake_quantizer = None
    trunk_output_block2_block2_0_f_b_1 = getattr(getattr(self.trunk_output.block2, "block2-0").f.b, "1")(trunk_output_block2_block2_0_f_b_0);  trunk_output_block2_block2_0_f_b_0 = None
    trunk_output_block2_block2_0_f_b_2 = getattr(getattr(self.trunk_output.block2, "block2-0").f.b, "2")(trunk_output_block2_block2_0_f_b_1);  trunk_output_block2_block2_0_f_b_1 = None
    trunk_output_block2_block2_0_f_b_2_post_act_fake_quantizer = self.trunk_output_block2_block2_0_f_b_2_post_act_fake_quantizer(trunk_output_block2_block2_0_f_b_2);  trunk_output_block2_block2_0_f_b_2 = None
    trunk_output_block2_block2_0_f_c_0 = getattr(getattr(self.trunk_output.block2, "block2-0").f.c, "0")(trunk_output_block2_block2_0_f_b_2_post_act_fake_quantizer);  trunk_output_block2_block2_0_f_b_2_post_act_fake_quantizer = None
    trunk_output_block2_block2_0_f_c_1 = getattr(getattr(self.trunk_output.block2, "block2-0").f.c, "1")(trunk_output_block2_block2_0_f_c_0);  trunk_output_block2_block2_0_f_c_0 = None
    add_2 = trunk_output_block2_block2_0_proj_1 + trunk_output_block2_block2_0_f_c_1;  trunk_output_block2_block2_0_proj_1 = trunk_output_block2_block2_0_f_c_1 = None
    trunk_output_block2_block2_0_activation = getattr(self.trunk_output.block2, "block2-0").activation(add_2);  add_2 = None
    trunk_output_block2_block2_0_activation_post_act_fake_quantizer = self.trunk_output_block2_block2_0_activation_post_act_fake_quantizer(trunk_output_block2_block2_0_activation);  trunk_output_block2_block2_0_activation = None
    return trunk_output_block2_block2_0_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_0_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_0_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_0_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	3588.212 (rec:3588.212, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	3686.078 (rec:3686.078, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	3878.714 (rec:3878.714, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	3759.236 (rec:3759.236, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	3892.563 (rec:3892.563, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	3888.432 (rec:3888.432, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	3830.685 (rec:3830.685, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	5008.557 (rec:3883.729, round:1124.828)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	4745.115 (rec:3929.177, round:815.938)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	4645.803 (rec:3905.252, round:740.550)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	4525.828 (rec:3839.080, round:686.748)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	4443.627 (rec:3800.201, round:643.426)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	4503.562 (rec:3896.528, round:607.034)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	4491.339 (rec:3916.124, round:575.215)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	4343.758 (rec:3798.170, round:545.589)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	4495.385 (rec:3978.160, round:517.225)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	4390.310 (rec:3899.702, round:490.608)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	4399.189 (rec:3933.350, round:465.839)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	4415.192 (rec:3973.194, round:441.998)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	4297.728 (rec:3877.559, round:420.169)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	4288.252 (rec:3888.963, round:399.289)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	4244.890 (rec:3864.899, round:379.990)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	4291.380 (rec:3930.204, round:361.176)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	4197.492 (rec:3853.741, round:343.751)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	4356.626 (rec:4029.604, round:327.023)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	4264.119 (rec:3954.058, round:310.061)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	4100.519 (rec:3806.442, round:294.077)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	4198.589 (rec:3920.165, round:278.424)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	4239.814 (rec:3976.490, round:263.324)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	4308.112 (rec:4059.132, round:248.981)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	4064.334 (rec:3829.589, round:234.745)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	3849.416 (rec:3628.589, round:220.826)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	4203.342 (rec:3995.948, round:207.394)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	4032.405 (rec:3838.245, round:194.160)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	4061.528 (rec:3880.854, round:180.674)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	4094.332 (rec:3926.932, round:167.400)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	4120.030 (rec:3966.056, round:153.974)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	4066.767 (rec:3926.544, round:140.223)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	3972.424 (rec:3846.717, round:125.707)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	3960.026 (rec:3851.236, round:108.790)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block2_block2_1_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block2_block2_0_activation_post_act_fake_quantizer, trunk_output_block2_block2_1_f_a_0, trunk_output_block2_block2_1_f_a_1, trunk_output_block2_block2_1_f_a_2, trunk_output_block2_block2_1_f_a_2_post_act_fake_quantizer, trunk_output_block2_block2_1_f_b_0, trunk_output_block2_block2_1_f_b_1, trunk_output_block2_block2_1_f_b_2, trunk_output_block2_block2_1_f_b_2_post_act_fake_quantizer, trunk_output_block2_block2_1_f_c_0, trunk_output_block2_block2_1_f_c_1, add_3, trunk_output_block2_block2_1_activation, trunk_output_block2_block2_1_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block2_block2_0_activation_post_act_fake_quantizer):
    trunk_output_block2_block2_1_f_a_0 = getattr(getattr(self.trunk_output.block2, "block2-1").f.a, "0")(trunk_output_block2_block2_0_activation_post_act_fake_quantizer)
    trunk_output_block2_block2_1_f_a_1 = getattr(getattr(self.trunk_output.block2, "block2-1").f.a, "1")(trunk_output_block2_block2_1_f_a_0);  trunk_output_block2_block2_1_f_a_0 = None
    trunk_output_block2_block2_1_f_a_2 = getattr(getattr(self.trunk_output.block2, "block2-1").f.a, "2")(trunk_output_block2_block2_1_f_a_1);  trunk_output_block2_block2_1_f_a_1 = None
    trunk_output_block2_block2_1_f_a_2_post_act_fake_quantizer = self.trunk_output_block2_block2_1_f_a_2_post_act_fake_quantizer(trunk_output_block2_block2_1_f_a_2);  trunk_output_block2_block2_1_f_a_2 = None
    trunk_output_block2_block2_1_f_b_0 = getattr(getattr(self.trunk_output.block2, "block2-1").f.b, "0")(trunk_output_block2_block2_1_f_a_2_post_act_fake_quantizer);  trunk_output_block2_block2_1_f_a_2_post_act_fake_quantizer = None
    trunk_output_block2_block2_1_f_b_1 = getattr(getattr(self.trunk_output.block2, "block2-1").f.b, "1")(trunk_output_block2_block2_1_f_b_0);  trunk_output_block2_block2_1_f_b_0 = None
    trunk_output_block2_block2_1_f_b_2 = getattr(getattr(self.trunk_output.block2, "block2-1").f.b, "2")(trunk_output_block2_block2_1_f_b_1);  trunk_output_block2_block2_1_f_b_1 = None
    trunk_output_block2_block2_1_f_b_2_post_act_fake_quantizer = self.trunk_output_block2_block2_1_f_b_2_post_act_fake_quantizer(trunk_output_block2_block2_1_f_b_2);  trunk_output_block2_block2_1_f_b_2 = None
    trunk_output_block2_block2_1_f_c_0 = getattr(getattr(self.trunk_output.block2, "block2-1").f.c, "0")(trunk_output_block2_block2_1_f_b_2_post_act_fake_quantizer);  trunk_output_block2_block2_1_f_b_2_post_act_fake_quantizer = None
    trunk_output_block2_block2_1_f_c_1 = getattr(getattr(self.trunk_output.block2, "block2-1").f.c, "1")(trunk_output_block2_block2_1_f_c_0);  trunk_output_block2_block2_1_f_c_0 = None
    add_3 = trunk_output_block2_block2_0_activation_post_act_fake_quantizer + trunk_output_block2_block2_1_f_c_1;  trunk_output_block2_block2_0_activation_post_act_fake_quantizer = trunk_output_block2_block2_1_f_c_1 = None
    trunk_output_block2_block2_1_activation = getattr(self.trunk_output.block2, "block2-1").activation(add_3);  add_3 = None
    trunk_output_block2_block2_1_activation_post_act_fake_quantizer = self.trunk_output_block2_block2_1_activation_post_act_fake_quantizer(trunk_output_block2_block2_1_activation);  trunk_output_block2_block2_1_activation = None
    return trunk_output_block2_block2_1_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_1_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_1_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_1_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	7229.420 (rec:7229.420, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	7177.833 (rec:7177.833, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	6976.963 (rec:6976.963, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	7224.525 (rec:7224.525, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	7256.040 (rec:7256.040, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	7185.671 (rec:7185.671, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	7405.246 (rec:7405.246, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	8472.069 (rec:7261.971, round:1210.099)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	8206.891 (rec:7410.161, round:796.729)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	8066.900 (rec:7341.051, round:725.850)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	8270.925 (rec:7595.255, round:675.669)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	7954.937 (rec:7319.413, round:635.524)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	7997.989 (rec:7398.021, round:599.968)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	7832.741 (rec:7264.885, round:567.856)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	7724.583 (rec:7186.342, round:538.242)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	7744.651 (rec:7234.635, round:510.016)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	7600.352 (rec:7116.440, round:483.912)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	7514.836 (rec:7055.271, round:459.565)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	7272.827 (rec:6834.888, round:437.939)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	7546.053 (rec:7128.132, round:417.921)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	7897.326 (rec:7498.862, round:398.464)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	7711.998 (rec:7332.055, round:379.944)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	7549.647 (rec:7187.869, round:361.779)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	7501.828 (rec:7156.686, round:345.142)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	7368.662 (rec:7039.882, round:328.780)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	7491.567 (rec:7179.016, round:312.552)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	7390.774 (rec:7093.432, round:297.342)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	7517.097 (rec:7234.226, round:282.871)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	7573.700 (rec:7305.139, round:268.561)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	7480.871 (rec:7226.013, round:254.858)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	7456.527 (rec:7215.139, round:241.389)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	7381.957 (rec:7153.955, round:228.002)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	7765.626 (rec:7550.426, round:215.200)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	7466.562 (rec:7264.167, round:202.394)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	7260.739 (rec:7071.100, round:189.639)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	7625.431 (rec:7448.693, round:176.738)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	7842.536 (rec:7678.913, round:163.622)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	7600.095 (rec:7450.163, round:149.932)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	7580.274 (rec:7444.869, round:135.405)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	7283.007 (rec:7164.924, round:118.083)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block2_block2_2_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block2_block2_1_activation_post_act_fake_quantizer, trunk_output_block2_block2_2_f_a_0, trunk_output_block2_block2_2_f_a_1, trunk_output_block2_block2_2_f_a_2, trunk_output_block2_block2_2_f_a_2_post_act_fake_quantizer, trunk_output_block2_block2_2_f_b_0, trunk_output_block2_block2_2_f_b_1, trunk_output_block2_block2_2_f_b_2, trunk_output_block2_block2_2_f_b_2_post_act_fake_quantizer, trunk_output_block2_block2_2_f_c_0, trunk_output_block2_block2_2_f_c_1, add_4, trunk_output_block2_block2_2_activation, trunk_output_block2_block2_2_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block2_block2_1_activation_post_act_fake_quantizer):
    trunk_output_block2_block2_2_f_a_0 = getattr(getattr(self.trunk_output.block2, "block2-2").f.a, "0")(trunk_output_block2_block2_1_activation_post_act_fake_quantizer)
    trunk_output_block2_block2_2_f_a_1 = getattr(getattr(self.trunk_output.block2, "block2-2").f.a, "1")(trunk_output_block2_block2_2_f_a_0);  trunk_output_block2_block2_2_f_a_0 = None
    trunk_output_block2_block2_2_f_a_2 = getattr(getattr(self.trunk_output.block2, "block2-2").f.a, "2")(trunk_output_block2_block2_2_f_a_1);  trunk_output_block2_block2_2_f_a_1 = None
    trunk_output_block2_block2_2_f_a_2_post_act_fake_quantizer = self.trunk_output_block2_block2_2_f_a_2_post_act_fake_quantizer(trunk_output_block2_block2_2_f_a_2);  trunk_output_block2_block2_2_f_a_2 = None
    trunk_output_block2_block2_2_f_b_0 = getattr(getattr(self.trunk_output.block2, "block2-2").f.b, "0")(trunk_output_block2_block2_2_f_a_2_post_act_fake_quantizer);  trunk_output_block2_block2_2_f_a_2_post_act_fake_quantizer = None
    trunk_output_block2_block2_2_f_b_1 = getattr(getattr(self.trunk_output.block2, "block2-2").f.b, "1")(trunk_output_block2_block2_2_f_b_0);  trunk_output_block2_block2_2_f_b_0 = None
    trunk_output_block2_block2_2_f_b_2 = getattr(getattr(self.trunk_output.block2, "block2-2").f.b, "2")(trunk_output_block2_block2_2_f_b_1);  trunk_output_block2_block2_2_f_b_1 = None
    trunk_output_block2_block2_2_f_b_2_post_act_fake_quantizer = self.trunk_output_block2_block2_2_f_b_2_post_act_fake_quantizer(trunk_output_block2_block2_2_f_b_2);  trunk_output_block2_block2_2_f_b_2 = None
    trunk_output_block2_block2_2_f_c_0 = getattr(getattr(self.trunk_output.block2, "block2-2").f.c, "0")(trunk_output_block2_block2_2_f_b_2_post_act_fake_quantizer);  trunk_output_block2_block2_2_f_b_2_post_act_fake_quantizer = None
    trunk_output_block2_block2_2_f_c_1 = getattr(getattr(self.trunk_output.block2, "block2-2").f.c, "1")(trunk_output_block2_block2_2_f_c_0);  trunk_output_block2_block2_2_f_c_0 = None
    add_4 = trunk_output_block2_block2_1_activation_post_act_fake_quantizer + trunk_output_block2_block2_2_f_c_1;  trunk_output_block2_block2_1_activation_post_act_fake_quantizer = trunk_output_block2_block2_2_f_c_1 = None
    trunk_output_block2_block2_2_activation = getattr(self.trunk_output.block2, "block2-2").activation(add_4);  add_4 = None
    trunk_output_block2_block2_2_activation_post_act_fake_quantizer = self.trunk_output_block2_block2_2_activation_post_act_fake_quantizer(trunk_output_block2_block2_2_activation);  trunk_output_block2_block2_2_activation = None
    return trunk_output_block2_block2_2_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_2_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_2_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_2_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	10558.561 (rec:10558.561, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	10569.043 (rec:10569.043, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	10732.225 (rec:10732.225, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	10875.651 (rec:10875.651, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	10666.844 (rec:10666.844, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	10681.862 (rec:10681.862, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	10859.495 (rec:10859.495, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	12486.440 (rec:11313.316, round:1173.124)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	11616.180 (rec:10794.133, round:822.047)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	11390.489 (rec:10644.590, round:745.900)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	11205.000 (rec:10515.097, round:689.903)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	11335.842 (rec:10691.809, round:644.034)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	11906.685 (rec:11301.376, round:605.309)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	11372.009 (rec:10801.518, round:570.491)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	11183.005 (rec:10643.604, round:539.400)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	11846.304 (rec:11335.212, round:511.092)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	10760.111 (rec:10275.195, round:484.916)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	11808.495 (rec:11348.018, round:460.477)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	11608.659 (rec:11170.125, round:438.534)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	11575.262 (rec:11157.158, round:418.104)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	10975.505 (rec:10576.825, round:398.680)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	11667.004 (rec:11285.686, round:381.318)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	11294.704 (rec:10930.153, round:364.550)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	11679.547 (rec:11331.312, round:348.234)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	11035.322 (rec:10702.495, round:332.827)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	11078.246 (rec:10760.041, round:318.205)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	11003.047 (rec:10698.921, round:304.126)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	11222.544 (rec:10931.984, round:290.559)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	11127.637 (rec:10850.575, round:277.062)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	11281.973 (rec:11018.219, round:263.754)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	11073.902 (rec:10822.829, round:251.073)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	10925.108 (rec:10687.025, round:238.083)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	11452.500 (rec:11227.022, round:225.477)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	10893.898 (rec:10681.240, round:212.659)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	11231.265 (rec:11031.548, round:199.717)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	11306.904 (rec:11119.883, round:187.021)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	11543.954 (rec:11370.304, round:173.650)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	10503.038 (rec:10343.485, round:159.553)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	10654.468 (rec:10510.169, round:144.299)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	11473.153 (rec:11347.071, round:126.082)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block2_block2_3_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block2_block2_2_activation_post_act_fake_quantizer, trunk_output_block2_block2_3_f_a_0, trunk_output_block2_block2_3_f_a_1, trunk_output_block2_block2_3_f_a_2, trunk_output_block2_block2_3_f_a_2_post_act_fake_quantizer, trunk_output_block2_block2_3_f_b_0, trunk_output_block2_block2_3_f_b_1, trunk_output_block2_block2_3_f_b_2, trunk_output_block2_block2_3_f_b_2_post_act_fake_quantizer, trunk_output_block2_block2_3_f_c_0, trunk_output_block2_block2_3_f_c_1, add_5, trunk_output_block2_block2_3_activation, trunk_output_block2_block2_3_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block2_block2_2_activation_post_act_fake_quantizer):
    trunk_output_block2_block2_3_f_a_0 = getattr(getattr(self.trunk_output.block2, "block2-3").f.a, "0")(trunk_output_block2_block2_2_activation_post_act_fake_quantizer)
    trunk_output_block2_block2_3_f_a_1 = getattr(getattr(self.trunk_output.block2, "block2-3").f.a, "1")(trunk_output_block2_block2_3_f_a_0);  trunk_output_block2_block2_3_f_a_0 = None
    trunk_output_block2_block2_3_f_a_2 = getattr(getattr(self.trunk_output.block2, "block2-3").f.a, "2")(trunk_output_block2_block2_3_f_a_1);  trunk_output_block2_block2_3_f_a_1 = None
    trunk_output_block2_block2_3_f_a_2_post_act_fake_quantizer = self.trunk_output_block2_block2_3_f_a_2_post_act_fake_quantizer(trunk_output_block2_block2_3_f_a_2);  trunk_output_block2_block2_3_f_a_2 = None
    trunk_output_block2_block2_3_f_b_0 = getattr(getattr(self.trunk_output.block2, "block2-3").f.b, "0")(trunk_output_block2_block2_3_f_a_2_post_act_fake_quantizer);  trunk_output_block2_block2_3_f_a_2_post_act_fake_quantizer = None
    trunk_output_block2_block2_3_f_b_1 = getattr(getattr(self.trunk_output.block2, "block2-3").f.b, "1")(trunk_output_block2_block2_3_f_b_0);  trunk_output_block2_block2_3_f_b_0 = None
    trunk_output_block2_block2_3_f_b_2 = getattr(getattr(self.trunk_output.block2, "block2-3").f.b, "2")(trunk_output_block2_block2_3_f_b_1);  trunk_output_block2_block2_3_f_b_1 = None
    trunk_output_block2_block2_3_f_b_2_post_act_fake_quantizer = self.trunk_output_block2_block2_3_f_b_2_post_act_fake_quantizer(trunk_output_block2_block2_3_f_b_2);  trunk_output_block2_block2_3_f_b_2 = None
    trunk_output_block2_block2_3_f_c_0 = getattr(getattr(self.trunk_output.block2, "block2-3").f.c, "0")(trunk_output_block2_block2_3_f_b_2_post_act_fake_quantizer);  trunk_output_block2_block2_3_f_b_2_post_act_fake_quantizer = None
    trunk_output_block2_block2_3_f_c_1 = getattr(getattr(self.trunk_output.block2, "block2-3").f.c, "1")(trunk_output_block2_block2_3_f_c_0);  trunk_output_block2_block2_3_f_c_0 = None
    add_5 = trunk_output_block2_block2_2_activation_post_act_fake_quantizer + trunk_output_block2_block2_3_f_c_1;  trunk_output_block2_block2_2_activation_post_act_fake_quantizer = trunk_output_block2_block2_3_f_c_1 = None
    trunk_output_block2_block2_3_activation = getattr(self.trunk_output.block2, "block2-3").activation(add_5);  add_5 = None
    trunk_output_block2_block2_3_activation_post_act_fake_quantizer = self.trunk_output_block2_block2_3_activation_post_act_fake_quantizer(trunk_output_block2_block2_3_activation);  trunk_output_block2_block2_3_activation = None
    return trunk_output_block2_block2_3_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_3_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_3_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_3_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	16132.836 (rec:16132.836, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	16625.557 (rec:16625.557, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	17495.852 (rec:17495.852, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	17460.141 (rec:17460.141, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	18219.406 (rec:18219.406, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	18215.590 (rec:18215.590, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	18066.141 (rec:18066.141, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	18952.543 (rec:17751.969, round:1200.575)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	19099.912 (rec:18148.715, round:951.198)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	18804.410 (rec:17914.094, round:890.316)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	18780.832 (rec:17936.217, round:844.616)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	18634.498 (rec:17828.611, round:805.887)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	18623.725 (rec:17851.260, round:772.466)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	18670.408 (rec:17927.568, round:742.839)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	18605.852 (rec:17890.807, round:715.046)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	18620.309 (rec:17931.207, round:689.101)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	18634.365 (rec:17969.902, round:664.463)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	18668.812 (rec:18027.277, round:641.536)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	18840.078 (rec:18220.938, round:619.140)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	18714.674 (rec:18116.730, round:597.943)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	18738.217 (rec:18160.516, round:577.701)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	18891.826 (rec:18333.242, round:558.585)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	18583.547 (rec:18044.016, round:539.532)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	18898.346 (rec:18376.770, round:521.576)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	18994.811 (rec:18490.512, round:504.300)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	18693.072 (rec:18205.869, round:487.202)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	18427.064 (rec:17956.209, round:470.855)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	18598.438 (rec:18144.033, round:454.404)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	18919.152 (rec:18481.391, round:437.762)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	18796.740 (rec:18375.426, round:421.314)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	18483.256 (rec:18078.254, round:405.002)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	18534.318 (rec:18146.051, round:388.267)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	18880.146 (rec:18509.029, round:371.117)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	18965.938 (rec:18612.020, round:353.919)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	18916.377 (rec:18580.211, round:336.165)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	18773.977 (rec:18456.641, round:317.337)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	18442.943 (rec:18145.725, round:297.219)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	19012.533 (rec:18737.070, round:275.462)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	19130.506 (rec:18879.428, round:251.079)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	18392.225 (rec:18170.576, round:221.649)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block2_block2_4_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block2_block2_3_activation_post_act_fake_quantizer, trunk_output_block2_block2_4_f_a_0, trunk_output_block2_block2_4_f_a_1, trunk_output_block2_block2_4_f_a_2, trunk_output_block2_block2_4_f_a_2_post_act_fake_quantizer, trunk_output_block2_block2_4_f_b_0, trunk_output_block2_block2_4_f_b_1, trunk_output_block2_block2_4_f_b_2, trunk_output_block2_block2_4_f_b_2_post_act_fake_quantizer, trunk_output_block2_block2_4_f_c_0, trunk_output_block2_block2_4_f_c_1, add_6, trunk_output_block2_block2_4_activation, trunk_output_block2_block2_4_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block2_block2_3_activation_post_act_fake_quantizer):
    trunk_output_block2_block2_4_f_a_0 = getattr(getattr(self.trunk_output.block2, "block2-4").f.a, "0")(trunk_output_block2_block2_3_activation_post_act_fake_quantizer)
    trunk_output_block2_block2_4_f_a_1 = getattr(getattr(self.trunk_output.block2, "block2-4").f.a, "1")(trunk_output_block2_block2_4_f_a_0);  trunk_output_block2_block2_4_f_a_0 = None
    trunk_output_block2_block2_4_f_a_2 = getattr(getattr(self.trunk_output.block2, "block2-4").f.a, "2")(trunk_output_block2_block2_4_f_a_1);  trunk_output_block2_block2_4_f_a_1 = None
    trunk_output_block2_block2_4_f_a_2_post_act_fake_quantizer = self.trunk_output_block2_block2_4_f_a_2_post_act_fake_quantizer(trunk_output_block2_block2_4_f_a_2);  trunk_output_block2_block2_4_f_a_2 = None
    trunk_output_block2_block2_4_f_b_0 = getattr(getattr(self.trunk_output.block2, "block2-4").f.b, "0")(trunk_output_block2_block2_4_f_a_2_post_act_fake_quantizer);  trunk_output_block2_block2_4_f_a_2_post_act_fake_quantizer = None
    trunk_output_block2_block2_4_f_b_1 = getattr(getattr(self.trunk_output.block2, "block2-4").f.b, "1")(trunk_output_block2_block2_4_f_b_0);  trunk_output_block2_block2_4_f_b_0 = None
    trunk_output_block2_block2_4_f_b_2 = getattr(getattr(self.trunk_output.block2, "block2-4").f.b, "2")(trunk_output_block2_block2_4_f_b_1);  trunk_output_block2_block2_4_f_b_1 = None
    trunk_output_block2_block2_4_f_b_2_post_act_fake_quantizer = self.trunk_output_block2_block2_4_f_b_2_post_act_fake_quantizer(trunk_output_block2_block2_4_f_b_2);  trunk_output_block2_block2_4_f_b_2 = None
    trunk_output_block2_block2_4_f_c_0 = getattr(getattr(self.trunk_output.block2, "block2-4").f.c, "0")(trunk_output_block2_block2_4_f_b_2_post_act_fake_quantizer);  trunk_output_block2_block2_4_f_b_2_post_act_fake_quantizer = None
    trunk_output_block2_block2_4_f_c_1 = getattr(getattr(self.trunk_output.block2, "block2-4").f.c, "1")(trunk_output_block2_block2_4_f_c_0);  trunk_output_block2_block2_4_f_c_0 = None
    add_6 = trunk_output_block2_block2_3_activation_post_act_fake_quantizer + trunk_output_block2_block2_4_f_c_1;  trunk_output_block2_block2_3_activation_post_act_fake_quantizer = trunk_output_block2_block2_4_f_c_1 = None
    trunk_output_block2_block2_4_activation = getattr(self.trunk_output.block2, "block2-4").activation(add_6);  add_6 = None
    trunk_output_block2_block2_4_activation_post_act_fake_quantizer = self.trunk_output_block2_block2_4_activation_post_act_fake_quantizer(trunk_output_block2_block2_4_activation);  trunk_output_block2_block2_4_activation = None
    return trunk_output_block2_block2_4_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_4_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_4_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_4_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	25633.326 (rec:25633.326, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	26105.229 (rec:26105.229, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	26354.654 (rec:26354.654, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	26386.010 (rec:26386.010, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	26993.395 (rec:26993.395, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	26055.709 (rec:26055.709, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	26569.760 (rec:26569.760, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	28162.459 (rec:26833.543, round:1328.915)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	28645.719 (rec:27590.332, round:1055.387)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	27381.016 (rec:26391.393, round:989.624)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	27413.334 (rec:26472.795, round:940.540)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	27307.873 (rec:26409.283, round:898.590)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	28454.281 (rec:27593.775, round:860.506)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	28878.965 (rec:28052.334, round:826.631)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	27018.707 (rec:26224.475, round:794.233)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	28390.234 (rec:27628.320, round:761.913)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	26744.479 (rec:26013.133, round:731.345)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	26176.521 (rec:25475.688, round:700.833)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	25912.148 (rec:25240.213, round:671.936)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	27776.584 (rec:27131.943, round:644.641)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	28257.062 (rec:27638.744, round:618.318)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	28653.854 (rec:28061.055, round:592.799)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	28548.910 (rec:27980.611, round:568.300)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	27265.166 (rec:26722.207, round:542.960)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	29160.791 (rec:28641.883, round:518.909)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	27802.912 (rec:27307.607, round:495.305)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	27957.844 (rec:27485.127, round:472.717)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	30731.131 (rec:30280.418, round:450.713)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	26341.588 (rec:25913.117, round:428.470)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	28074.982 (rec:27668.363, round:406.619)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	27064.410 (rec:26678.824, round:385.586)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	27565.699 (rec:27201.836, round:363.863)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	26032.689 (rec:25689.658, round:343.032)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	27659.031 (rec:27337.734, round:321.298)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	26653.742 (rec:26353.570, round:300.172)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	27441.949 (rec:27163.500, round:278.450)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	26894.441 (rec:26638.578, round:255.863)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	28261.797 (rec:28028.887, round:232.910)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	28355.254 (rec:28146.709, round:208.546)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	29702.615 (rec:29521.303, round:181.312)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block2_block2_5_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block2_block2_4_activation_post_act_fake_quantizer, trunk_output_block2_block2_5_f_a_0, trunk_output_block2_block2_5_f_a_1, trunk_output_block2_block2_5_f_a_2, trunk_output_block2_block2_5_f_a_2_post_act_fake_quantizer, trunk_output_block2_block2_5_f_b_0, trunk_output_block2_block2_5_f_b_1, trunk_output_block2_block2_5_f_b_2, trunk_output_block2_block2_5_f_b_2_post_act_fake_quantizer, trunk_output_block2_block2_5_f_c_0, trunk_output_block2_block2_5_f_c_1, add_7, trunk_output_block2_block2_5_activation, trunk_output_block2_block2_5_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block2_block2_4_activation_post_act_fake_quantizer):
    trunk_output_block2_block2_5_f_a_0 = getattr(getattr(self.trunk_output.block2, "block2-5").f.a, "0")(trunk_output_block2_block2_4_activation_post_act_fake_quantizer)
    trunk_output_block2_block2_5_f_a_1 = getattr(getattr(self.trunk_output.block2, "block2-5").f.a, "1")(trunk_output_block2_block2_5_f_a_0);  trunk_output_block2_block2_5_f_a_0 = None
    trunk_output_block2_block2_5_f_a_2 = getattr(getattr(self.trunk_output.block2, "block2-5").f.a, "2")(trunk_output_block2_block2_5_f_a_1);  trunk_output_block2_block2_5_f_a_1 = None
    trunk_output_block2_block2_5_f_a_2_post_act_fake_quantizer = self.trunk_output_block2_block2_5_f_a_2_post_act_fake_quantizer(trunk_output_block2_block2_5_f_a_2);  trunk_output_block2_block2_5_f_a_2 = None
    trunk_output_block2_block2_5_f_b_0 = getattr(getattr(self.trunk_output.block2, "block2-5").f.b, "0")(trunk_output_block2_block2_5_f_a_2_post_act_fake_quantizer);  trunk_output_block2_block2_5_f_a_2_post_act_fake_quantizer = None
    trunk_output_block2_block2_5_f_b_1 = getattr(getattr(self.trunk_output.block2, "block2-5").f.b, "1")(trunk_output_block2_block2_5_f_b_0);  trunk_output_block2_block2_5_f_b_0 = None
    trunk_output_block2_block2_5_f_b_2 = getattr(getattr(self.trunk_output.block2, "block2-5").f.b, "2")(trunk_output_block2_block2_5_f_b_1);  trunk_output_block2_block2_5_f_b_1 = None
    trunk_output_block2_block2_5_f_b_2_post_act_fake_quantizer = self.trunk_output_block2_block2_5_f_b_2_post_act_fake_quantizer(trunk_output_block2_block2_5_f_b_2);  trunk_output_block2_block2_5_f_b_2 = None
    trunk_output_block2_block2_5_f_c_0 = getattr(getattr(self.trunk_output.block2, "block2-5").f.c, "0")(trunk_output_block2_block2_5_f_b_2_post_act_fake_quantizer);  trunk_output_block2_block2_5_f_b_2_post_act_fake_quantizer = None
    trunk_output_block2_block2_5_f_c_1 = getattr(getattr(self.trunk_output.block2, "block2-5").f.c, "1")(trunk_output_block2_block2_5_f_c_0);  trunk_output_block2_block2_5_f_c_0 = None
    add_7 = trunk_output_block2_block2_4_activation_post_act_fake_quantizer + trunk_output_block2_block2_5_f_c_1;  trunk_output_block2_block2_4_activation_post_act_fake_quantizer = trunk_output_block2_block2_5_f_c_1 = None
    trunk_output_block2_block2_5_activation = getattr(self.trunk_output.block2, "block2-5").activation(add_7);  add_7 = None
    trunk_output_block2_block2_5_activation_post_act_fake_quantizer = self.trunk_output_block2_block2_5_activation_post_act_fake_quantizer(trunk_output_block2_block2_5_activation);  trunk_output_block2_block2_5_activation = None
    return trunk_output_block2_block2_5_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_5_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_5_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block2_block2_5_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	26105.809 (rec:26105.809, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	26522.652 (rec:26522.652, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	25481.908 (rec:25481.908, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	25314.396 (rec:25314.396, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	25888.893 (rec:25888.893, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	28231.428 (rec:28231.428, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	25439.012 (rec:25439.012, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	24451.939 (rec:23079.971, round:1371.968)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	22624.643 (rec:21531.104, round:1093.539)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	28534.084 (rec:27504.004, round:1030.080)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	25144.367 (rec:24159.838, round:984.529)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	25754.023 (rec:24808.041, round:945.981)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	21804.264 (rec:20893.297, round:910.966)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	24360.014 (rec:23483.494, round:876.519)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	26222.131 (rec:25377.582, round:844.548)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	25250.369 (rec:24436.459, round:813.911)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	29419.078 (rec:28634.643, round:784.436)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	27800.326 (rec:27045.078, round:755.248)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	26648.480 (rec:25922.260, round:726.221)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	22095.148 (rec:21397.035, round:698.114)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	25197.686 (rec:24528.117, round:669.569)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	25405.541 (rec:24763.568, round:641.973)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	25957.961 (rec:25343.225, round:614.735)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	25434.480 (rec:24846.887, round:587.594)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	25866.516 (rec:25305.492, round:561.023)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	26293.852 (rec:25759.320, round:534.530)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	27757.494 (rec:27249.467, round:508.027)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	23605.891 (rec:23124.016, round:481.874)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	24604.350 (rec:24148.688, round:455.661)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	23861.662 (rec:23431.320, round:430.341)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	23435.781 (rec:23030.523, round:405.259)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	24299.467 (rec:23919.229, round:380.238)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	24304.793 (rec:23950.150, round:354.643)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	23538.352 (rec:23209.092, round:329.260)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	23498.018 (rec:23193.943, round:304.075)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	23309.137 (rec:23030.449, round:278.687)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	23251.510 (rec:22999.141, round:252.370)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	24679.844 (rec:24453.625, round:226.219)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	23194.338 (rec:22995.184, round:199.155)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	23873.660 (rec:23703.086, round:170.575)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_0_proj_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block2_block2_5_activation_post_act_fake_quantizer, trunk_output_block3_block3_0_proj_0, trunk_output_block3_block3_0_proj_1, trunk_output_block3_block3_0_f_a_0, trunk_output_block3_block3_0_f_a_1, trunk_output_block3_block3_0_f_a_2, trunk_output_block3_block3_0_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_0_f_b_0, trunk_output_block3_block3_0_f_b_1, trunk_output_block3_block3_0_f_b_2, trunk_output_block3_block3_0_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_0_f_c_0, trunk_output_block3_block3_0_f_c_1, add_8, trunk_output_block3_block3_0_activation, trunk_output_block3_block3_0_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block2_block2_5_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_0_proj_0 = getattr(getattr(self.trunk_output.block3, "block3-0").proj, "0")(trunk_output_block2_block2_5_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_0_proj_1 = getattr(getattr(self.trunk_output.block3, "block3-0").proj, "1")(trunk_output_block3_block3_0_proj_0);  trunk_output_block3_block3_0_proj_0 = None
    trunk_output_block3_block3_0_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-0").f.a, "0")(trunk_output_block2_block2_5_activation_post_act_fake_quantizer);  trunk_output_block2_block2_5_activation_post_act_fake_quantizer = None
    trunk_output_block3_block3_0_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-0").f.a, "1")(trunk_output_block3_block3_0_f_a_0);  trunk_output_block3_block3_0_f_a_0 = None
    trunk_output_block3_block3_0_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-0").f.a, "2")(trunk_output_block3_block3_0_f_a_1);  trunk_output_block3_block3_0_f_a_1 = None
    trunk_output_block3_block3_0_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_0_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_0_f_a_2);  trunk_output_block3_block3_0_f_a_2 = None
    trunk_output_block3_block3_0_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-0").f.b, "0")(trunk_output_block3_block3_0_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_0_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_0_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-0").f.b, "1")(trunk_output_block3_block3_0_f_b_0);  trunk_output_block3_block3_0_f_b_0 = None
    trunk_output_block3_block3_0_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-0").f.b, "2")(trunk_output_block3_block3_0_f_b_1);  trunk_output_block3_block3_0_f_b_1 = None
    trunk_output_block3_block3_0_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_0_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_0_f_b_2);  trunk_output_block3_block3_0_f_b_2 = None
    trunk_output_block3_block3_0_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-0").f.c, "0")(trunk_output_block3_block3_0_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_0_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_0_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-0").f.c, "1")(trunk_output_block3_block3_0_f_c_0);  trunk_output_block3_block3_0_f_c_0 = None
    add_8 = trunk_output_block3_block3_0_proj_1 + trunk_output_block3_block3_0_f_c_1;  trunk_output_block3_block3_0_proj_1 = trunk_output_block3_block3_0_f_c_1 = None
    trunk_output_block3_block3_0_activation = getattr(self.trunk_output.block3, "block3-0").activation(add_8);  add_8 = None
    trunk_output_block3_block3_0_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_0_activation_post_act_fake_quantizer(trunk_output_block3_block3_0_activation);  trunk_output_block3_block3_0_activation = None
    return trunk_output_block3_block3_0_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_0_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_0_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_0_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	13228.871 (rec:13228.871, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	14286.000 (rec:14286.000, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	14080.737 (rec:14080.737, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	14522.448 (rec:14522.448, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	14234.987 (rec:14234.987, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	14507.987 (rec:14507.987, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	14524.046 (rec:14524.046, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	19298.281 (rec:14856.025, round:4442.255)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	17957.137 (rec:14833.987, round:3123.149)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	17486.957 (rec:14648.929, round:2838.028)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	17085.689 (rec:14446.279, round:2639.410)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	16981.686 (rec:14505.831, round:2475.855)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	17036.797 (rec:14703.673, round:2333.123)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	15984.545 (rec:13782.972, round:2201.574)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	16639.867 (rec:14560.436, round:2079.432)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	16163.248 (rec:14197.668, round:1965.580)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	16525.098 (rec:14667.579, round:1857.519)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	16077.580 (rec:14323.638, round:1753.942)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	15779.256 (rec:14123.991, round:1655.265)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	16023.717 (rec:14463.277, round:1560.439)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	16062.823 (rec:14594.062, round:1468.761)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	14622.172 (rec:13240.847, round:1381.325)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	16777.205 (rec:15479.480, round:1297.724)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	15489.526 (rec:14272.151, round:1217.375)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	17204.184 (rec:16064.191, round:1139.992)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	14103.880 (rec:13038.186, round:1065.694)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	15731.168 (rec:14736.915, round:994.253)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	15618.120 (rec:14692.509, round:925.612)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	15884.103 (rec:15026.744, round:857.358)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	15405.751 (rec:14613.235, round:792.516)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	15185.484 (rec:14455.959, round:729.526)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	14313.603 (rec:13645.926, round:667.677)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	15765.438 (rec:15157.237, round:608.200)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	14000.872 (rec:13451.193, round:549.679)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	13883.399 (rec:13390.227, round:493.173)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	14965.158 (rec:14526.758, round:438.400)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	14585.081 (rec:14199.352, round:385.729)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	14809.293 (rec:14473.540, round:335.753)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	15165.314 (rec:14877.158, round:288.156)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	14927.906 (rec:14687.304, round:240.602)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_1_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_0_activation_post_act_fake_quantizer, trunk_output_block3_block3_1_f_a_0, trunk_output_block3_block3_1_f_a_1, trunk_output_block3_block3_1_f_a_2, trunk_output_block3_block3_1_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_1_f_b_0, trunk_output_block3_block3_1_f_b_1, trunk_output_block3_block3_1_f_b_2, trunk_output_block3_block3_1_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_1_f_c_0, trunk_output_block3_block3_1_f_c_1, add_9, trunk_output_block3_block3_1_activation, trunk_output_block3_block3_1_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_0_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_1_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-1").f.a, "0")(trunk_output_block3_block3_0_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_1_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-1").f.a, "1")(trunk_output_block3_block3_1_f_a_0);  trunk_output_block3_block3_1_f_a_0 = None
    trunk_output_block3_block3_1_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-1").f.a, "2")(trunk_output_block3_block3_1_f_a_1);  trunk_output_block3_block3_1_f_a_1 = None
    trunk_output_block3_block3_1_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_1_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_1_f_a_2);  trunk_output_block3_block3_1_f_a_2 = None
    trunk_output_block3_block3_1_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-1").f.b, "0")(trunk_output_block3_block3_1_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_1_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_1_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-1").f.b, "1")(trunk_output_block3_block3_1_f_b_0);  trunk_output_block3_block3_1_f_b_0 = None
    trunk_output_block3_block3_1_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-1").f.b, "2")(trunk_output_block3_block3_1_f_b_1);  trunk_output_block3_block3_1_f_b_1 = None
    trunk_output_block3_block3_1_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_1_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_1_f_b_2);  trunk_output_block3_block3_1_f_b_2 = None
    trunk_output_block3_block3_1_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-1").f.c, "0")(trunk_output_block3_block3_1_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_1_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_1_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-1").f.c, "1")(trunk_output_block3_block3_1_f_c_0);  trunk_output_block3_block3_1_f_c_0 = None
    add_9 = trunk_output_block3_block3_0_activation_post_act_fake_quantizer + trunk_output_block3_block3_1_f_c_1;  trunk_output_block3_block3_0_activation_post_act_fake_quantizer = trunk_output_block3_block3_1_f_c_1 = None
    trunk_output_block3_block3_1_activation = getattr(self.trunk_output.block3, "block3-1").activation(add_9);  add_9 = None
    trunk_output_block3_block3_1_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_1_activation_post_act_fake_quantizer(trunk_output_block3_block3_1_activation);  trunk_output_block3_block3_1_activation = None
    return trunk_output_block3_block3_1_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_1_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_1_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_1_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	12331.291 (rec:12331.291, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	12079.581 (rec:12079.581, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	12080.931 (rec:12080.931, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	12127.604 (rec:12127.604, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	12133.456 (rec:12133.456, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	11986.922 (rec:11986.922, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	11973.969 (rec:11973.969, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	16607.199 (rec:12162.106, round:4445.092)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	14318.228 (rec:11784.100, round:2534.128)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	13977.649 (rec:11725.872, round:2251.777)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	14253.188 (rec:12191.436, round:2061.751)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	13264.854 (rec:11356.574, round:1908.280)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	13598.104 (rec:11822.217, round:1775.887)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	13447.707 (rec:11789.066, round:1658.641)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	13009.900 (rec:11460.727, round:1549.173)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	13494.231 (rec:12045.952, round:1448.280)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	13272.028 (rec:11916.574, round:1355.454)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	13235.658 (rec:11964.667, round:1270.992)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	13097.899 (rec:11903.535, round:1194.364)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	12472.092 (rec:11347.864, round:1124.228)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	13468.852 (rec:12409.434, round:1059.418)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	12487.254 (rec:11487.859, round:999.394)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	12554.377 (rec:11611.931, round:942.446)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	13119.338 (rec:12231.048, round:888.290)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	13152.902 (rec:12316.370, round:836.533)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	12203.404 (rec:11415.293, round:788.111)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	12898.713 (rec:12157.311, round:741.402)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	12741.684 (rec:12044.263, round:697.421)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	13056.729 (rec:12401.885, round:654.843)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	12874.453 (rec:12260.340, round:614.114)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	11830.048 (rec:11255.168, round:574.880)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	13182.908 (rec:12646.002, round:536.906)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	11921.301 (rec:11420.459, round:500.842)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	11599.052 (rec:11133.438, round:465.615)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	12435.276 (rec:12004.262, round:431.014)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	12165.841 (rec:11769.259, round:396.582)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	11668.904 (rec:11306.512, round:362.392)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	12642.726 (rec:12314.625, round:328.100)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	12677.175 (rec:12384.765, round:292.410)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	12165.383 (rec:11913.393, round:251.991)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_2_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_1_activation_post_act_fake_quantizer, trunk_output_block3_block3_2_f_a_0, trunk_output_block3_block3_2_f_a_1, trunk_output_block3_block3_2_f_a_2, trunk_output_block3_block3_2_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_2_f_b_0, trunk_output_block3_block3_2_f_b_1, trunk_output_block3_block3_2_f_b_2, trunk_output_block3_block3_2_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_2_f_c_0, trunk_output_block3_block3_2_f_c_1, add_10, trunk_output_block3_block3_2_activation, trunk_output_block3_block3_2_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_1_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_2_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-2").f.a, "0")(trunk_output_block3_block3_1_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_2_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-2").f.a, "1")(trunk_output_block3_block3_2_f_a_0);  trunk_output_block3_block3_2_f_a_0 = None
    trunk_output_block3_block3_2_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-2").f.a, "2")(trunk_output_block3_block3_2_f_a_1);  trunk_output_block3_block3_2_f_a_1 = None
    trunk_output_block3_block3_2_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_2_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_2_f_a_2);  trunk_output_block3_block3_2_f_a_2 = None
    trunk_output_block3_block3_2_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-2").f.b, "0")(trunk_output_block3_block3_2_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_2_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_2_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-2").f.b, "1")(trunk_output_block3_block3_2_f_b_0);  trunk_output_block3_block3_2_f_b_0 = None
    trunk_output_block3_block3_2_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-2").f.b, "2")(trunk_output_block3_block3_2_f_b_1);  trunk_output_block3_block3_2_f_b_1 = None
    trunk_output_block3_block3_2_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_2_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_2_f_b_2);  trunk_output_block3_block3_2_f_b_2 = None
    trunk_output_block3_block3_2_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-2").f.c, "0")(trunk_output_block3_block3_2_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_2_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_2_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-2").f.c, "1")(trunk_output_block3_block3_2_f_c_0);  trunk_output_block3_block3_2_f_c_0 = None
    add_10 = trunk_output_block3_block3_1_activation_post_act_fake_quantizer + trunk_output_block3_block3_2_f_c_1;  trunk_output_block3_block3_1_activation_post_act_fake_quantizer = trunk_output_block3_block3_2_f_c_1 = None
    trunk_output_block3_block3_2_activation = getattr(self.trunk_output.block3, "block3-2").activation(add_10);  add_10 = None
    trunk_output_block3_block3_2_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_2_activation_post_act_fake_quantizer(trunk_output_block3_block3_2_activation);  trunk_output_block3_block3_2_activation = None
    return trunk_output_block3_block3_2_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_2_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_2_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_2_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	9131.401 (rec:9131.401, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	9410.871 (rec:9410.871, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	9231.597 (rec:9231.597, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	9317.680 (rec:9317.680, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	9353.101 (rec:9353.101, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	9265.144 (rec:9265.144, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	9464.625 (rec:9464.625, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	13690.883 (rec:9195.562, round:4495.320)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	11929.699 (rec:9335.446, round:2594.252)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	11515.115 (rec:9215.969, round:2299.146)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	11366.279 (rec:9262.655, round:2103.624)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	11166.868 (rec:9223.235, round:1943.633)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	11469.424 (rec:9663.646, round:1805.778)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	11147.769 (rec:9462.922, round:1684.847)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	10874.358 (rec:9299.953, round:1574.406)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	10750.605 (rec:9281.051, round:1469.555)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	10864.326 (rec:9489.885, round:1374.441)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	10550.242 (rec:9263.298, round:1286.945)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	10047.606 (rec:8841.793, round:1205.814)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	9970.714 (rec:8839.399, round:1131.314)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	10027.608 (rec:8966.339, round:1061.270)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	10304.033 (rec:9307.340, round:996.693)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	9391.045 (rec:8456.199, round:934.846)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	10281.140 (rec:9403.979, round:877.161)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	10606.022 (rec:9783.936, round:822.087)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	10217.830 (rec:9448.486, round:769.343)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	9501.798 (rec:8782.280, round:719.518)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	9858.775 (rec:9187.921, round:670.855)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	9226.036 (rec:8600.302, round:625.734)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	9565.476 (rec:8982.115, round:583.360)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	9830.932 (rec:9288.421, round:542.511)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	9627.118 (rec:9122.442, round:504.676)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	10035.534 (rec:9567.839, round:467.695)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	9840.047 (rec:9407.441, round:432.605)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	10106.094 (rec:9706.796, round:399.298)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	9470.602 (rec:9104.791, round:365.811)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	9325.778 (rec:8992.705, round:333.073)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	9569.293 (rec:9268.459, round:300.834)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	10481.917 (rec:10213.622, round:268.295)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	9885.325 (rec:9653.518, round:231.808)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_3_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_2_activation_post_act_fake_quantizer, trunk_output_block3_block3_3_f_a_0, trunk_output_block3_block3_3_f_a_1, trunk_output_block3_block3_3_f_a_2, trunk_output_block3_block3_3_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_3_f_b_0, trunk_output_block3_block3_3_f_b_1, trunk_output_block3_block3_3_f_b_2, trunk_output_block3_block3_3_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_3_f_c_0, trunk_output_block3_block3_3_f_c_1, add_11, trunk_output_block3_block3_3_activation, trunk_output_block3_block3_3_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_2_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_3_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-3").f.a, "0")(trunk_output_block3_block3_2_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_3_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-3").f.a, "1")(trunk_output_block3_block3_3_f_a_0);  trunk_output_block3_block3_3_f_a_0 = None
    trunk_output_block3_block3_3_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-3").f.a, "2")(trunk_output_block3_block3_3_f_a_1);  trunk_output_block3_block3_3_f_a_1 = None
    trunk_output_block3_block3_3_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_3_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_3_f_a_2);  trunk_output_block3_block3_3_f_a_2 = None
    trunk_output_block3_block3_3_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-3").f.b, "0")(trunk_output_block3_block3_3_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_3_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_3_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-3").f.b, "1")(trunk_output_block3_block3_3_f_b_0);  trunk_output_block3_block3_3_f_b_0 = None
    trunk_output_block3_block3_3_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-3").f.b, "2")(trunk_output_block3_block3_3_f_b_1);  trunk_output_block3_block3_3_f_b_1 = None
    trunk_output_block3_block3_3_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_3_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_3_f_b_2);  trunk_output_block3_block3_3_f_b_2 = None
    trunk_output_block3_block3_3_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-3").f.c, "0")(trunk_output_block3_block3_3_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_3_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_3_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-3").f.c, "1")(trunk_output_block3_block3_3_f_c_0);  trunk_output_block3_block3_3_f_c_0 = None
    add_11 = trunk_output_block3_block3_2_activation_post_act_fake_quantizer + trunk_output_block3_block3_3_f_c_1;  trunk_output_block3_block3_2_activation_post_act_fake_quantizer = trunk_output_block3_block3_3_f_c_1 = None
    trunk_output_block3_block3_3_activation = getattr(self.trunk_output.block3, "block3-3").activation(add_11);  add_11 = None
    trunk_output_block3_block3_3_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_3_activation_post_act_fake_quantizer(trunk_output_block3_block3_3_activation);  trunk_output_block3_block3_3_activation = None
    return trunk_output_block3_block3_3_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_3_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_3_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_3_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	10015.945 (rec:10015.945, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	9995.838 (rec:9995.838, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	10105.776 (rec:10105.776, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	10132.414 (rec:10132.414, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	10162.312 (rec:10162.312, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	10244.293 (rec:10244.293, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	10210.116 (rec:10210.116, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	14767.310 (rec:10225.238, round:4542.071)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	12862.926 (rec:10084.886, round:2778.040)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	12494.238 (rec:10021.310, round:2472.929)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	12512.657 (rec:10243.942, round:2268.715)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	11933.838 (rec:9828.822, round:2105.016)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	12216.854 (rec:10252.685, round:1964.168)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	12235.987 (rec:10394.966, round:1841.021)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	12583.893 (rec:10854.811, round:1729.082)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	11768.679 (rec:10140.980, round:1627.698)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	11404.660 (rec:9873.176, round:1531.484)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	11518.710 (rec:10077.493, round:1441.217)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	11600.552 (rec:10243.198, round:1357.354)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	12025.695 (rec:10748.063, round:1277.632)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	11908.131 (rec:10705.768, round:1202.364)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	11704.217 (rec:10575.162, round:1129.055)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	11225.905 (rec:10165.030, round:1060.875)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	10651.283 (rec:9654.584, round:996.700)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	11302.190 (rec:10367.638, round:934.553)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	10932.648 (rec:10059.352, round:873.297)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	10211.610 (rec:9394.903, round:816.707)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	11117.199 (rec:10353.300, round:763.899)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	10340.337 (rec:9627.893, round:712.445)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	10800.398 (rec:10137.574, round:662.824)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	10409.992 (rec:9794.284, round:615.708)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	10969.898 (rec:10400.720, round:569.179)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	10617.234 (rec:10092.585, round:524.650)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	10120.977 (rec:9639.434, round:481.543)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	10261.757 (rec:9822.173, round:439.584)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	10433.358 (rec:10034.622, round:398.736)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	10287.095 (rec:9928.055, round:359.040)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	10059.557 (rec:9739.583, round:319.973)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	10391.498 (rec:10110.140, round:281.359)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	10182.949 (rec:9943.487, round:239.462)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_4_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_3_activation_post_act_fake_quantizer, trunk_output_block3_block3_4_f_a_0, trunk_output_block3_block3_4_f_a_1, trunk_output_block3_block3_4_f_a_2, trunk_output_block3_block3_4_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_4_f_b_0, trunk_output_block3_block3_4_f_b_1, trunk_output_block3_block3_4_f_b_2, trunk_output_block3_block3_4_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_4_f_c_0, trunk_output_block3_block3_4_f_c_1, add_12, trunk_output_block3_block3_4_activation, trunk_output_block3_block3_4_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_3_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_4_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-4").f.a, "0")(trunk_output_block3_block3_3_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_4_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-4").f.a, "1")(trunk_output_block3_block3_4_f_a_0);  trunk_output_block3_block3_4_f_a_0 = None
    trunk_output_block3_block3_4_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-4").f.a, "2")(trunk_output_block3_block3_4_f_a_1);  trunk_output_block3_block3_4_f_a_1 = None
    trunk_output_block3_block3_4_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_4_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_4_f_a_2);  trunk_output_block3_block3_4_f_a_2 = None
    trunk_output_block3_block3_4_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-4").f.b, "0")(trunk_output_block3_block3_4_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_4_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_4_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-4").f.b, "1")(trunk_output_block3_block3_4_f_b_0);  trunk_output_block3_block3_4_f_b_0 = None
    trunk_output_block3_block3_4_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-4").f.b, "2")(trunk_output_block3_block3_4_f_b_1);  trunk_output_block3_block3_4_f_b_1 = None
    trunk_output_block3_block3_4_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_4_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_4_f_b_2);  trunk_output_block3_block3_4_f_b_2 = None
    trunk_output_block3_block3_4_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-4").f.c, "0")(trunk_output_block3_block3_4_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_4_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_4_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-4").f.c, "1")(trunk_output_block3_block3_4_f_c_0);  trunk_output_block3_block3_4_f_c_0 = None
    add_12 = trunk_output_block3_block3_3_activation_post_act_fake_quantizer + trunk_output_block3_block3_4_f_c_1;  trunk_output_block3_block3_3_activation_post_act_fake_quantizer = trunk_output_block3_block3_4_f_c_1 = None
    trunk_output_block3_block3_4_activation = getattr(self.trunk_output.block3, "block3-4").activation(add_12);  add_12 = None
    trunk_output_block3_block3_4_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_4_activation_post_act_fake_quantizer(trunk_output_block3_block3_4_activation);  trunk_output_block3_block3_4_activation = None
    return trunk_output_block3_block3_4_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_4_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_4_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_4_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	7814.336 (rec:7814.336, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	7801.928 (rec:7801.928, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	7870.677 (rec:7870.677, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	7990.638 (rec:7990.638, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	7717.280 (rec:7717.280, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	7712.932 (rec:7712.932, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	7609.196 (rec:7609.196, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	12163.488 (rec:7666.398, round:4497.090)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	10354.994 (rec:7560.817, round:2794.177)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	10217.807 (rec:7741.990, round:2475.817)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	9815.585 (rec:7553.242, round:2262.343)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	9809.217 (rec:7719.321, round:2089.896)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	9597.607 (rec:7657.307, round:1940.300)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	9404.338 (rec:7593.877, round:1810.461)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	9503.779 (rec:7812.464, round:1691.315)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	9194.609 (rec:7614.711, round:1579.899)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	9160.342 (rec:7681.791, round:1478.551)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	9108.628 (rec:7720.696, round:1387.932)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	8722.377 (rec:7419.438, round:1302.940)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	8788.837 (rec:7567.110, round:1221.726)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	8809.483 (rec:7663.673, round:1145.810)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	8676.321 (rec:7602.176, round:1074.145)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	8281.952 (rec:7274.813, round:1007.138)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	8631.115 (rec:7688.251, round:942.864)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	8404.143 (rec:7521.040, round:883.103)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	8324.448 (rec:7498.761, round:825.687)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	8479.896 (rec:7706.451, round:773.444)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	8546.520 (rec:7823.558, round:722.962)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	8239.510 (rec:7563.673, round:675.837)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	8136.022 (rec:7505.576, round:630.446)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	8573.043 (rec:7985.908, round:587.135)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	8203.289 (rec:7656.925, round:546.364)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	7863.626 (rec:7356.837, round:506.789)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	8259.537 (rec:7791.288, round:468.249)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	8123.668 (rec:7693.089, round:430.579)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	8010.573 (rec:7617.143, round:393.430)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	7937.575 (rec:7580.669, round:356.905)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	7867.744 (rec:7547.172, round:320.572)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	7761.888 (rec:7478.541, round:283.347)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	7708.504 (rec:7466.550, round:241.954)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_5_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_4_activation_post_act_fake_quantizer, trunk_output_block3_block3_5_f_a_0, trunk_output_block3_block3_5_f_a_1, trunk_output_block3_block3_5_f_a_2, trunk_output_block3_block3_5_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_5_f_b_0, trunk_output_block3_block3_5_f_b_1, trunk_output_block3_block3_5_f_b_2, trunk_output_block3_block3_5_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_5_f_c_0, trunk_output_block3_block3_5_f_c_1, add_13, trunk_output_block3_block3_5_activation, trunk_output_block3_block3_5_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_4_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_5_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-5").f.a, "0")(trunk_output_block3_block3_4_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_5_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-5").f.a, "1")(trunk_output_block3_block3_5_f_a_0);  trunk_output_block3_block3_5_f_a_0 = None
    trunk_output_block3_block3_5_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-5").f.a, "2")(trunk_output_block3_block3_5_f_a_1);  trunk_output_block3_block3_5_f_a_1 = None
    trunk_output_block3_block3_5_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_5_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_5_f_a_2);  trunk_output_block3_block3_5_f_a_2 = None
    trunk_output_block3_block3_5_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-5").f.b, "0")(trunk_output_block3_block3_5_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_5_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_5_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-5").f.b, "1")(trunk_output_block3_block3_5_f_b_0);  trunk_output_block3_block3_5_f_b_0 = None
    trunk_output_block3_block3_5_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-5").f.b, "2")(trunk_output_block3_block3_5_f_b_1);  trunk_output_block3_block3_5_f_b_1 = None
    trunk_output_block3_block3_5_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_5_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_5_f_b_2);  trunk_output_block3_block3_5_f_b_2 = None
    trunk_output_block3_block3_5_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-5").f.c, "0")(trunk_output_block3_block3_5_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_5_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_5_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-5").f.c, "1")(trunk_output_block3_block3_5_f_c_0);  trunk_output_block3_block3_5_f_c_0 = None
    add_13 = trunk_output_block3_block3_4_activation_post_act_fake_quantizer + trunk_output_block3_block3_5_f_c_1;  trunk_output_block3_block3_4_activation_post_act_fake_quantizer = trunk_output_block3_block3_5_f_c_1 = None
    trunk_output_block3_block3_5_activation = getattr(self.trunk_output.block3, "block3-5").activation(add_13);  add_13 = None
    trunk_output_block3_block3_5_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_5_activation_post_act_fake_quantizer(trunk_output_block3_block3_5_activation);  trunk_output_block3_block3_5_activation = None
    return trunk_output_block3_block3_5_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_5_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_5_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_5_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	8781.074 (rec:8781.074, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	9061.033 (rec:9061.033, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	9066.390 (rec:9066.390, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	9215.847 (rec:9215.847, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	9133.889 (rec:9133.889, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	9192.410 (rec:9192.410, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	9282.175 (rec:9282.175, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	13835.474 (rec:9259.596, round:4575.878)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	12235.453 (rec:9283.809, round:2951.644)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	11980.279 (rec:9353.484, round:2626.795)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	11761.823 (rec:9350.482, round:2411.341)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	11354.230 (rec:9118.562, round:2235.669)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	11527.412 (rec:9445.245, round:2082.167)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	11167.314 (rec:9224.031, round:1943.283)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	10927.449 (rec:9110.431, round:1817.019)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	11040.236 (rec:9338.727, round:1701.509)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	11212.557 (rec:9618.938, round:1593.619)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	10858.618 (rec:9364.498, round:1494.120)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	10717.871 (rec:9315.269, round:1402.603)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	10766.275 (rec:9449.396, round:1316.879)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	10783.712 (rec:9547.284, round:1236.428)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	10744.223 (rec:9582.317, round:1161.905)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	10691.390 (rec:9598.755, round:1092.635)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	10639.971 (rec:9614.863, round:1025.108)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	10138.027 (rec:9174.287, round:963.741)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	10522.545 (rec:9618.181, round:904.364)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	10147.968 (rec:9298.352, round:849.616)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	10006.563 (rec:9210.254, round:796.310)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	10112.248 (rec:9367.172, round:745.077)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	9867.155 (rec:9170.460, round:696.696)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	9604.656 (rec:8954.727, round:649.930)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	10006.511 (rec:9401.177, round:605.334)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	9996.784 (rec:9434.854, round:561.930)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	9920.383 (rec:9400.331, round:520.052)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	9569.827 (rec:9091.201, round:478.626)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	10084.837 (rec:9648.039, round:436.798)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	9258.720 (rec:8862.101, round:396.619)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	9792.216 (rec:9435.741, round:356.475)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	9437.870 (rec:9122.509, round:315.361)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	10369.442 (rec:10100.505, round:268.937)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_6_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_5_activation_post_act_fake_quantizer, trunk_output_block3_block3_6_f_a_0, trunk_output_block3_block3_6_f_a_1, trunk_output_block3_block3_6_f_a_2, trunk_output_block3_block3_6_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_6_f_b_0, trunk_output_block3_block3_6_f_b_1, trunk_output_block3_block3_6_f_b_2, trunk_output_block3_block3_6_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_6_f_c_0, trunk_output_block3_block3_6_f_c_1, add_14, trunk_output_block3_block3_6_activation, trunk_output_block3_block3_6_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_5_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_6_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-6").f.a, "0")(trunk_output_block3_block3_5_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_6_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-6").f.a, "1")(trunk_output_block3_block3_6_f_a_0);  trunk_output_block3_block3_6_f_a_0 = None
    trunk_output_block3_block3_6_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-6").f.a, "2")(trunk_output_block3_block3_6_f_a_1);  trunk_output_block3_block3_6_f_a_1 = None
    trunk_output_block3_block3_6_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_6_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_6_f_a_2);  trunk_output_block3_block3_6_f_a_2 = None
    trunk_output_block3_block3_6_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-6").f.b, "0")(trunk_output_block3_block3_6_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_6_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_6_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-6").f.b, "1")(trunk_output_block3_block3_6_f_b_0);  trunk_output_block3_block3_6_f_b_0 = None
    trunk_output_block3_block3_6_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-6").f.b, "2")(trunk_output_block3_block3_6_f_b_1);  trunk_output_block3_block3_6_f_b_1 = None
    trunk_output_block3_block3_6_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_6_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_6_f_b_2);  trunk_output_block3_block3_6_f_b_2 = None
    trunk_output_block3_block3_6_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-6").f.c, "0")(trunk_output_block3_block3_6_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_6_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_6_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-6").f.c, "1")(trunk_output_block3_block3_6_f_c_0);  trunk_output_block3_block3_6_f_c_0 = None
    add_14 = trunk_output_block3_block3_5_activation_post_act_fake_quantizer + trunk_output_block3_block3_6_f_c_1;  trunk_output_block3_block3_5_activation_post_act_fake_quantizer = trunk_output_block3_block3_6_f_c_1 = None
    trunk_output_block3_block3_6_activation = getattr(self.trunk_output.block3, "block3-6").activation(add_14);  add_14 = None
    trunk_output_block3_block3_6_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_6_activation_post_act_fake_quantizer(trunk_output_block3_block3_6_activation);  trunk_output_block3_block3_6_activation = None
    return trunk_output_block3_block3_6_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_6_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_6_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_6_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	9375.539 (rec:9375.539, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	9616.195 (rec:9616.195, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	9715.704 (rec:9715.704, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	9887.473 (rec:9887.473, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	9884.190 (rec:9884.190, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	9843.567 (rec:9843.567, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	9828.822 (rec:9828.822, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	14495.102 (rec:9859.387, round:4635.715)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	12811.980 (rec:9773.846, round:3038.135)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	12481.645 (rec:9770.342, round:2711.303)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	12348.038 (rec:9853.496, round:2494.542)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	12285.820 (rec:9967.449, round:2318.371)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	11841.050 (rec:9676.854, round:2164.196)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	11889.691 (rec:9865.379, round:2024.313)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	11641.363 (rec:9746.910, round:1894.453)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	11558.993 (rec:9785.264, round:1773.730)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	11328.483 (rec:9667.506, round:1660.978)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	11570.297 (rec:10014.914, round:1555.383)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	11391.525 (rec:9935.733, round:1455.792)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	11094.312 (rec:9731.647, round:1362.665)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	10963.806 (rec:9687.173, round:1276.633)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	11049.647 (rec:9855.159, round:1194.488)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	10814.144 (rec:9697.262, round:1116.882)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	11090.264 (rec:10046.322, round:1043.941)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	10690.698 (rec:9716.830, round:973.868)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	10598.003 (rec:9689.420, round:908.583)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	10795.497 (rec:9948.514, round:846.983)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	10330.360 (rec:9542.936, round:787.425)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	10585.556 (rec:9853.927, round:731.629)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	10502.664 (rec:9825.737, round:676.927)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	10234.238 (rec:9610.043, round:624.196)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	10540.471 (rec:9966.723, round:573.748)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	10153.367 (rec:9627.612, round:525.755)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	10143.048 (rec:9664.701, round:478.347)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	10065.015 (rec:9632.339, round:432.676)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	10143.780 (rec:9755.637, round:388.144)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	10292.136 (rec:9946.170, round:345.966)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	10040.113 (rec:9735.142, round:304.972)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	10208.937 (rec:9943.322, round:265.614)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	9994.112 (rec:9770.723, round:223.390)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_7_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_6_activation_post_act_fake_quantizer, trunk_output_block3_block3_7_f_a_0, trunk_output_block3_block3_7_f_a_1, trunk_output_block3_block3_7_f_a_2, trunk_output_block3_block3_7_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_7_f_b_0, trunk_output_block3_block3_7_f_b_1, trunk_output_block3_block3_7_f_b_2, trunk_output_block3_block3_7_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_7_f_c_0, trunk_output_block3_block3_7_f_c_1, add_15, trunk_output_block3_block3_7_activation, trunk_output_block3_block3_7_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_6_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_7_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-7").f.a, "0")(trunk_output_block3_block3_6_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_7_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-7").f.a, "1")(trunk_output_block3_block3_7_f_a_0);  trunk_output_block3_block3_7_f_a_0 = None
    trunk_output_block3_block3_7_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-7").f.a, "2")(trunk_output_block3_block3_7_f_a_1);  trunk_output_block3_block3_7_f_a_1 = None
    trunk_output_block3_block3_7_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_7_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_7_f_a_2);  trunk_output_block3_block3_7_f_a_2 = None
    trunk_output_block3_block3_7_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-7").f.b, "0")(trunk_output_block3_block3_7_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_7_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_7_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-7").f.b, "1")(trunk_output_block3_block3_7_f_b_0);  trunk_output_block3_block3_7_f_b_0 = None
    trunk_output_block3_block3_7_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-7").f.b, "2")(trunk_output_block3_block3_7_f_b_1);  trunk_output_block3_block3_7_f_b_1 = None
    trunk_output_block3_block3_7_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_7_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_7_f_b_2);  trunk_output_block3_block3_7_f_b_2 = None
    trunk_output_block3_block3_7_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-7").f.c, "0")(trunk_output_block3_block3_7_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_7_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_7_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-7").f.c, "1")(trunk_output_block3_block3_7_f_c_0);  trunk_output_block3_block3_7_f_c_0 = None
    add_15 = trunk_output_block3_block3_6_activation_post_act_fake_quantizer + trunk_output_block3_block3_7_f_c_1;  trunk_output_block3_block3_6_activation_post_act_fake_quantizer = trunk_output_block3_block3_7_f_c_1 = None
    trunk_output_block3_block3_7_activation = getattr(self.trunk_output.block3, "block3-7").activation(add_15);  add_15 = None
    trunk_output_block3_block3_7_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_7_activation_post_act_fake_quantizer(trunk_output_block3_block3_7_activation);  trunk_output_block3_block3_7_activation = None
    return trunk_output_block3_block3_7_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_7_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_7_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_7_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	15719.864 (rec:15719.864, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	16729.717 (rec:16729.717, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	16642.637 (rec:16642.637, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	16696.229 (rec:16696.229, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	16877.422 (rec:16877.422, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	16752.059 (rec:16752.059, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	16697.953 (rec:16697.953, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	21408.268 (rec:16867.729, round:4540.539)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	20123.375 (rec:16964.676, round:3158.699)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	19757.238 (rec:16927.438, round:2829.801)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	19348.514 (rec:16743.203, round:2605.311)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	19400.010 (rec:16977.732, round:2422.278)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	19048.709 (rec:16781.510, round:2267.199)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	19078.738 (rec:16952.033, round:2126.706)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	18703.416 (rec:16705.652, round:1997.764)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	18850.975 (rec:16971.312, round:1879.663)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	18564.529 (rec:16789.990, round:1774.539)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	18334.746 (rec:16658.186, round:1676.560)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	18352.363 (rec:16766.266, round:1586.097)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	18368.625 (rec:16868.590, round:1500.035)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	17962.660 (rec:16542.471, round:1420.189)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	18186.580 (rec:16840.705, round:1345.875)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	18619.789 (rec:17344.238, round:1275.550)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	18368.635 (rec:17159.090, round:1209.544)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	17353.961 (rec:16207.844, round:1146.118)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	17405.648 (rec:16321.290, round:1084.358)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	18197.848 (rec:17172.963, round:1024.885)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	17726.594 (rec:16758.000, round:968.594)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	17241.141 (rec:16326.429, round:914.712)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	18026.465 (rec:17164.301, round:862.164)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	18760.357 (rec:17948.457, round:811.900)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	17729.021 (rec:16966.316, round:762.705)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	17729.455 (rec:17015.887, round:713.568)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	17028.182 (rec:16362.555, round:665.628)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	16784.826 (rec:16166.993, round:617.833)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	16991.762 (rec:16422.146, round:569.615)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	16919.887 (rec:16398.285, round:521.601)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	17060.766 (rec:16588.254, round:472.511)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	18596.920 (rec:18176.043, round:420.876)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	17335.965 (rec:16974.039, round:361.925)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_8_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_7_activation_post_act_fake_quantizer, trunk_output_block3_block3_8_f_a_0, trunk_output_block3_block3_8_f_a_1, trunk_output_block3_block3_8_f_a_2, trunk_output_block3_block3_8_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_8_f_b_0, trunk_output_block3_block3_8_f_b_1, trunk_output_block3_block3_8_f_b_2, trunk_output_block3_block3_8_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_8_f_c_0, trunk_output_block3_block3_8_f_c_1, add_16, trunk_output_block3_block3_8_activation, trunk_output_block3_block3_8_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_7_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_8_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-8").f.a, "0")(trunk_output_block3_block3_7_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_8_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-8").f.a, "1")(trunk_output_block3_block3_8_f_a_0);  trunk_output_block3_block3_8_f_a_0 = None
    trunk_output_block3_block3_8_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-8").f.a, "2")(trunk_output_block3_block3_8_f_a_1);  trunk_output_block3_block3_8_f_a_1 = None
    trunk_output_block3_block3_8_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_8_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_8_f_a_2);  trunk_output_block3_block3_8_f_a_2 = None
    trunk_output_block3_block3_8_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-8").f.b, "0")(trunk_output_block3_block3_8_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_8_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_8_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-8").f.b, "1")(trunk_output_block3_block3_8_f_b_0);  trunk_output_block3_block3_8_f_b_0 = None
    trunk_output_block3_block3_8_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-8").f.b, "2")(trunk_output_block3_block3_8_f_b_1);  trunk_output_block3_block3_8_f_b_1 = None
    trunk_output_block3_block3_8_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_8_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_8_f_b_2);  trunk_output_block3_block3_8_f_b_2 = None
    trunk_output_block3_block3_8_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-8").f.c, "0")(trunk_output_block3_block3_8_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_8_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_8_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-8").f.c, "1")(trunk_output_block3_block3_8_f_c_0);  trunk_output_block3_block3_8_f_c_0 = None
    add_16 = trunk_output_block3_block3_7_activation_post_act_fake_quantizer + trunk_output_block3_block3_8_f_c_1;  trunk_output_block3_block3_7_activation_post_act_fake_quantizer = trunk_output_block3_block3_8_f_c_1 = None
    trunk_output_block3_block3_8_activation = getattr(self.trunk_output.block3, "block3-8").activation(add_16);  add_16 = None
    trunk_output_block3_block3_8_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_8_activation_post_act_fake_quantizer(trunk_output_block3_block3_8_activation);  trunk_output_block3_block3_8_activation = None
    return trunk_output_block3_block3_8_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_8_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_8_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_8_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	14039.834 (rec:14039.834, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	14455.856 (rec:14455.856, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	14537.276 (rec:14537.276, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	14657.504 (rec:14657.504, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	14498.227 (rec:14498.227, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	14712.477 (rec:14712.477, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	14465.075 (rec:14465.075, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	19452.992 (rec:14683.833, round:4769.160)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	17961.312 (rec:14614.566, round:3346.745)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	17544.277 (rec:14506.547, round:3037.730)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	17231.562 (rec:14400.318, round:2831.245)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	17069.273 (rec:14409.646, round:2659.627)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	17079.766 (rec:14572.646, round:2507.121)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	16927.561 (rec:14560.227, round:2367.334)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	16747.084 (rec:14510.950, round:2236.134)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	17055.865 (rec:14945.101, round:2110.765)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	16804.777 (rec:14813.339, round:1991.438)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	16479.541 (rec:14601.793, round:1877.747)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	16579.215 (rec:14807.505, round:1771.710)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	16204.352 (rec:14533.689, round:1670.662)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	16077.387 (rec:14503.888, round:1573.500)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	16788.311 (rec:15307.397, round:1480.913)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	15279.631 (rec:13884.235, round:1395.395)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	16428.469 (rec:15116.089, round:1312.379)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	16397.379 (rec:15165.413, round:1231.966)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	15965.942 (rec:14810.314, round:1155.628)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	15826.771 (rec:14745.188, round:1081.583)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	15531.970 (rec:14521.570, round:1010.399)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	15385.580 (rec:14443.890, round:941.691)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	15857.972 (rec:14982.900, round:875.071)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	14788.520 (rec:13977.962, round:810.558)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	15157.643 (rec:14408.897, round:748.745)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	14831.639 (rec:14142.896, round:688.743)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	15388.536 (rec:14759.055, round:629.481)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	15415.970 (rec:14844.304, round:571.666)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	15200.528 (rec:14686.100, round:514.429)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	15131.594 (rec:14672.727, round:458.867)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	15302.612 (rec:14899.044, round:403.568)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	14841.777 (rec:14493.493, round:348.284)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	15023.231 (rec:14731.913, round:291.318)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_9_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_8_activation_post_act_fake_quantizer, trunk_output_block3_block3_9_f_a_0, trunk_output_block3_block3_9_f_a_1, trunk_output_block3_block3_9_f_a_2, trunk_output_block3_block3_9_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_9_f_b_0, trunk_output_block3_block3_9_f_b_1, trunk_output_block3_block3_9_f_b_2, trunk_output_block3_block3_9_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_9_f_c_0, trunk_output_block3_block3_9_f_c_1, add_17, trunk_output_block3_block3_9_activation, trunk_output_block3_block3_9_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_8_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_9_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-9").f.a, "0")(trunk_output_block3_block3_8_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_9_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-9").f.a, "1")(trunk_output_block3_block3_9_f_a_0);  trunk_output_block3_block3_9_f_a_0 = None
    trunk_output_block3_block3_9_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-9").f.a, "2")(trunk_output_block3_block3_9_f_a_1);  trunk_output_block3_block3_9_f_a_1 = None
    trunk_output_block3_block3_9_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_9_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_9_f_a_2);  trunk_output_block3_block3_9_f_a_2 = None
    trunk_output_block3_block3_9_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-9").f.b, "0")(trunk_output_block3_block3_9_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_9_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_9_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-9").f.b, "1")(trunk_output_block3_block3_9_f_b_0);  trunk_output_block3_block3_9_f_b_0 = None
    trunk_output_block3_block3_9_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-9").f.b, "2")(trunk_output_block3_block3_9_f_b_1);  trunk_output_block3_block3_9_f_b_1 = None
    trunk_output_block3_block3_9_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_9_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_9_f_b_2);  trunk_output_block3_block3_9_f_b_2 = None
    trunk_output_block3_block3_9_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-9").f.c, "0")(trunk_output_block3_block3_9_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_9_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_9_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-9").f.c, "1")(trunk_output_block3_block3_9_f_c_0);  trunk_output_block3_block3_9_f_c_0 = None
    add_17 = trunk_output_block3_block3_8_activation_post_act_fake_quantizer + trunk_output_block3_block3_9_f_c_1;  trunk_output_block3_block3_8_activation_post_act_fake_quantizer = trunk_output_block3_block3_9_f_c_1 = None
    trunk_output_block3_block3_9_activation = getattr(self.trunk_output.block3, "block3-9").activation(add_17);  add_17 = None
    trunk_output_block3_block3_9_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_9_activation_post_act_fake_quantizer(trunk_output_block3_block3_9_activation);  trunk_output_block3_block3_9_activation = None
    return trunk_output_block3_block3_9_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_9_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_9_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_9_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	15204.614 (rec:15204.614, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	15421.959 (rec:15421.959, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	15259.640 (rec:15259.640, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	15421.217 (rec:15421.217, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	15390.102 (rec:15390.102, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	15199.648 (rec:15199.648, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	15543.800 (rec:15543.800, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	20200.924 (rec:15351.336, round:4849.588)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	18974.957 (rec:15554.821, round:3420.136)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	18646.066 (rec:15522.988, round:3123.078)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	18072.152 (rec:15147.268, round:2924.884)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	18006.490 (rec:15243.079, round:2763.411)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	17713.191 (rec:15094.327, round:2618.864)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	17739.266 (rec:15251.455, round:2487.811)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	17743.930 (rec:15379.467, round:2364.464)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	17854.650 (rec:15606.660, round:2247.990)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	17524.562 (rec:15390.301, round:2134.262)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	17728.578 (rec:15704.283, round:2024.294)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	17298.355 (rec:15380.021, round:1918.334)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	17181.660 (rec:15363.277, round:1818.383)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	17021.414 (rec:15301.768, round:1719.647)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	17555.713 (rec:15933.150, round:1622.563)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	16882.225 (rec:15353.013, round:1529.213)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	16896.438 (rec:15455.685, round:1440.752)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	16617.652 (rec:15262.487, round:1355.165)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	16666.057 (rec:15393.847, round:1272.210)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	16506.053 (rec:15313.550, round:1192.503)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	16545.000 (rec:15430.197, round:1114.803)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	16538.566 (rec:15499.314, round:1039.253)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	16582.078 (rec:15616.779, round:965.299)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	16591.703 (rec:15698.606, round:893.096)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	16379.262 (rec:15556.510, round:822.751)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	16342.290 (rec:15587.550, round:754.740)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	16526.947 (rec:15839.744, round:687.204)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	16045.157 (rec:15422.606, round:622.551)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	16182.624 (rec:15623.448, round:559.176)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	16400.139 (rec:15903.096, round:497.043)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	15835.677 (rec:15399.535, round:436.142)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	15978.433 (rec:15601.571, round:376.862)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	16101.767 (rec:15785.844, round:315.923)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_10_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_9_activation_post_act_fake_quantizer, trunk_output_block3_block3_10_f_a_0, trunk_output_block3_block3_10_f_a_1, trunk_output_block3_block3_10_f_a_2, trunk_output_block3_block3_10_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_10_f_b_0, trunk_output_block3_block3_10_f_b_1, trunk_output_block3_block3_10_f_b_2, trunk_output_block3_block3_10_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_10_f_c_0, trunk_output_block3_block3_10_f_c_1, add_18, trunk_output_block3_block3_10_activation, trunk_output_block3_block3_10_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_9_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_10_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-10").f.a, "0")(trunk_output_block3_block3_9_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_10_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-10").f.a, "1")(trunk_output_block3_block3_10_f_a_0);  trunk_output_block3_block3_10_f_a_0 = None
    trunk_output_block3_block3_10_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-10").f.a, "2")(trunk_output_block3_block3_10_f_a_1);  trunk_output_block3_block3_10_f_a_1 = None
    trunk_output_block3_block3_10_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_10_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_10_f_a_2);  trunk_output_block3_block3_10_f_a_2 = None
    trunk_output_block3_block3_10_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-10").f.b, "0")(trunk_output_block3_block3_10_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_10_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_10_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-10").f.b, "1")(trunk_output_block3_block3_10_f_b_0);  trunk_output_block3_block3_10_f_b_0 = None
    trunk_output_block3_block3_10_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-10").f.b, "2")(trunk_output_block3_block3_10_f_b_1);  trunk_output_block3_block3_10_f_b_1 = None
    trunk_output_block3_block3_10_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_10_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_10_f_b_2);  trunk_output_block3_block3_10_f_b_2 = None
    trunk_output_block3_block3_10_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-10").f.c, "0")(trunk_output_block3_block3_10_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_10_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_10_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-10").f.c, "1")(trunk_output_block3_block3_10_f_c_0);  trunk_output_block3_block3_10_f_c_0 = None
    add_18 = trunk_output_block3_block3_9_activation_post_act_fake_quantizer + trunk_output_block3_block3_10_f_c_1;  trunk_output_block3_block3_9_activation_post_act_fake_quantizer = trunk_output_block3_block3_10_f_c_1 = None
    trunk_output_block3_block3_10_activation = getattr(self.trunk_output.block3, "block3-10").activation(add_18);  add_18 = None
    trunk_output_block3_block3_10_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_10_activation_post_act_fake_quantizer(trunk_output_block3_block3_10_activation);  trunk_output_block3_block3_10_activation = None
    return trunk_output_block3_block3_10_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_10_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_10_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_10_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	16615.887 (rec:16615.887, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	17420.188 (rec:17420.188, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	17394.650 (rec:17394.650, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	17428.062 (rec:17428.062, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	17455.844 (rec:17455.844, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	17176.369 (rec:17176.369, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	17424.893 (rec:17424.893, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	22083.176 (rec:17380.844, round:4702.333)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	20696.459 (rec:17517.137, round:3179.323)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	20364.150 (rec:17475.594, round:2888.557)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	20184.672 (rec:17492.576, round:2692.095)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	20122.562 (rec:17590.904, round:2531.657)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	20148.412 (rec:17757.471, round:2390.941)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	19738.568 (rec:17474.457, round:2264.112)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	19814.359 (rec:17667.758, round:2146.602)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	19435.973 (rec:17397.652, round:2038.321)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	19487.252 (rec:17548.959, round:1938.293)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	19310.447 (rec:17465.160, round:1845.287)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	18890.777 (rec:17135.205, round:1755.572)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	19273.416 (rec:17605.354, round:1668.062)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	19810.791 (rec:18225.686, round:1585.105)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	18777.311 (rec:17271.748, round:1505.563)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	18665.549 (rec:17236.309, round:1429.241)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	19217.223 (rec:17860.887, round:1356.336)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	19290.969 (rec:18005.268, round:1285.702)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	18984.068 (rec:17767.344, round:1216.724)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	19259.828 (rec:18108.967, round:1150.862)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	18943.832 (rec:17857.248, round:1086.585)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	19954.348 (rec:18929.070, round:1025.277)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	18223.115 (rec:17257.959, round:965.157)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	18610.773 (rec:17705.279, round:905.493)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	18762.424 (rec:17915.391, round:847.034)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	18801.182 (rec:18011.385, round:789.797)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	18214.475 (rec:17480.959, round:733.516)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	17940.070 (rec:17262.816, round:677.254)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	18109.824 (rec:17490.414, round:619.410)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	18708.889 (rec:18147.869, round:561.020)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	18128.131 (rec:17627.307, round:500.824)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	18303.369 (rec:17864.824, round:438.545)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	17909.287 (rec:17537.836, round:371.452)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_11_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_10_activation_post_act_fake_quantizer, trunk_output_block3_block3_11_f_a_0, trunk_output_block3_block3_11_f_a_1, trunk_output_block3_block3_11_f_a_2, trunk_output_block3_block3_11_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_11_f_b_0, trunk_output_block3_block3_11_f_b_1, trunk_output_block3_block3_11_f_b_2, trunk_output_block3_block3_11_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_11_f_c_0, trunk_output_block3_block3_11_f_c_1, add_19, trunk_output_block3_block3_11_activation, trunk_output_block3_block3_11_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_10_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_11_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-11").f.a, "0")(trunk_output_block3_block3_10_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_11_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-11").f.a, "1")(trunk_output_block3_block3_11_f_a_0);  trunk_output_block3_block3_11_f_a_0 = None
    trunk_output_block3_block3_11_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-11").f.a, "2")(trunk_output_block3_block3_11_f_a_1);  trunk_output_block3_block3_11_f_a_1 = None
    trunk_output_block3_block3_11_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_11_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_11_f_a_2);  trunk_output_block3_block3_11_f_a_2 = None
    trunk_output_block3_block3_11_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-11").f.b, "0")(trunk_output_block3_block3_11_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_11_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_11_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-11").f.b, "1")(trunk_output_block3_block3_11_f_b_0);  trunk_output_block3_block3_11_f_b_0 = None
    trunk_output_block3_block3_11_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-11").f.b, "2")(trunk_output_block3_block3_11_f_b_1);  trunk_output_block3_block3_11_f_b_1 = None
    trunk_output_block3_block3_11_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_11_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_11_f_b_2);  trunk_output_block3_block3_11_f_b_2 = None
    trunk_output_block3_block3_11_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-11").f.c, "0")(trunk_output_block3_block3_11_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_11_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_11_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-11").f.c, "1")(trunk_output_block3_block3_11_f_c_0);  trunk_output_block3_block3_11_f_c_0 = None
    add_19 = trunk_output_block3_block3_10_activation_post_act_fake_quantizer + trunk_output_block3_block3_11_f_c_1;  trunk_output_block3_block3_10_activation_post_act_fake_quantizer = trunk_output_block3_block3_11_f_c_1 = None
    trunk_output_block3_block3_11_activation = getattr(self.trunk_output.block3, "block3-11").activation(add_19);  add_19 = None
    trunk_output_block3_block3_11_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_11_activation_post_act_fake_quantizer(trunk_output_block3_block3_11_activation);  trunk_output_block3_block3_11_activation = None
    return trunk_output_block3_block3_11_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_11_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_11_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_11_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	17524.643 (rec:17524.643, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	18003.236 (rec:18003.236, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	18491.861 (rec:18491.861, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	18352.650 (rec:18352.650, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	18451.812 (rec:18451.812, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	18253.574 (rec:18253.574, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	18452.729 (rec:18452.729, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	23599.742 (rec:18798.406, round:4801.335)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	21996.785 (rec:18613.570, round:3383.216)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	22041.525 (rec:18951.199, round:3090.326)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	21454.131 (rec:18565.607, round:2888.523)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	21336.559 (rec:18614.145, round:2722.414)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	21261.621 (rec:18680.828, round:2580.792)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	20876.623 (rec:18424.332, round:2452.292)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	20206.482 (rec:17872.531, round:2333.952)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	21414.213 (rec:19191.955, round:2222.258)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	20989.678 (rec:18873.311, round:2116.367)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	20544.594 (rec:18529.121, round:2015.472)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	21248.615 (rec:19328.873, round:1919.742)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	20142.754 (rec:18316.225, round:1826.529)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	20426.592 (rec:18691.299, round:1735.292)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	19804.432 (rec:18157.232, round:1647.200)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	19916.572 (rec:18353.508, round:1563.064)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	20264.395 (rec:18784.299, round:1480.095)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	19362.453 (rec:17962.252, round:1400.201)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	21050.711 (rec:19727.678, round:1323.032)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	19629.902 (rec:18384.369, round:1245.534)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	19998.178 (rec:18828.561, round:1169.617)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	19780.527 (rec:18683.832, round:1096.695)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	20309.941 (rec:19282.928, round:1027.014)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	18821.922 (rec:17864.508, round:957.414)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	20202.514 (rec:19313.645, round:888.869)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	19535.660 (rec:18713.883, round:821.777)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	18390.729 (rec:17634.719, round:756.009)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	17658.918 (rec:16969.154, round:689.764)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	19277.166 (rec:18652.902, round:624.264)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	18212.512 (rec:17652.248, round:560.264)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	18853.824 (rec:18357.236, round:496.588)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	20132.768 (rec:19701.191, round:431.576)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	18996.770 (rec:18634.424, round:362.346)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_12_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_11_activation_post_act_fake_quantizer, trunk_output_block3_block3_12_f_a_0, trunk_output_block3_block3_12_f_a_1, trunk_output_block3_block3_12_f_a_2, trunk_output_block3_block3_12_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_12_f_b_0, trunk_output_block3_block3_12_f_b_1, trunk_output_block3_block3_12_f_b_2, trunk_output_block3_block3_12_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_12_f_c_0, trunk_output_block3_block3_12_f_c_1, add_20, trunk_output_block3_block3_12_activation, trunk_output_block3_block3_12_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_11_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_12_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-12").f.a, "0")(trunk_output_block3_block3_11_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_12_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-12").f.a, "1")(trunk_output_block3_block3_12_f_a_0);  trunk_output_block3_block3_12_f_a_0 = None
    trunk_output_block3_block3_12_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-12").f.a, "2")(trunk_output_block3_block3_12_f_a_1);  trunk_output_block3_block3_12_f_a_1 = None
    trunk_output_block3_block3_12_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_12_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_12_f_a_2);  trunk_output_block3_block3_12_f_a_2 = None
    trunk_output_block3_block3_12_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-12").f.b, "0")(trunk_output_block3_block3_12_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_12_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_12_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-12").f.b, "1")(trunk_output_block3_block3_12_f_b_0);  trunk_output_block3_block3_12_f_b_0 = None
    trunk_output_block3_block3_12_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-12").f.b, "2")(trunk_output_block3_block3_12_f_b_1);  trunk_output_block3_block3_12_f_b_1 = None
    trunk_output_block3_block3_12_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_12_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_12_f_b_2);  trunk_output_block3_block3_12_f_b_2 = None
    trunk_output_block3_block3_12_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-12").f.c, "0")(trunk_output_block3_block3_12_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_12_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_12_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-12").f.c, "1")(trunk_output_block3_block3_12_f_c_0);  trunk_output_block3_block3_12_f_c_0 = None
    add_20 = trunk_output_block3_block3_11_activation_post_act_fake_quantizer + trunk_output_block3_block3_12_f_c_1;  trunk_output_block3_block3_11_activation_post_act_fake_quantizer = trunk_output_block3_block3_12_f_c_1 = None
    trunk_output_block3_block3_12_activation = getattr(self.trunk_output.block3, "block3-12").activation(add_20);  add_20 = None
    trunk_output_block3_block3_12_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_12_activation_post_act_fake_quantizer(trunk_output_block3_block3_12_activation);  trunk_output_block3_block3_12_activation = None
    return trunk_output_block3_block3_12_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_12_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_12_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_12_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	18582.910 (rec:18582.910, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	19172.221 (rec:19172.221, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	18823.092 (rec:18823.092, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	18952.826 (rec:18952.826, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	18683.928 (rec:18683.928, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	18905.924 (rec:18905.924, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	18979.293 (rec:18979.293, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	24029.711 (rec:19180.088, round:4849.624)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	22595.586 (rec:19138.746, round:3456.841)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	22674.604 (rec:19508.223, round:3166.382)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	21807.369 (rec:18833.770, round:2973.600)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	21520.674 (rec:18704.914, round:2815.760)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	21692.422 (rec:19012.303, round:2680.119)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	22253.871 (rec:19697.562, round:2556.309)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	21344.539 (rec:18905.201, round:2439.338)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	21253.525 (rec:18925.693, round:2327.833)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	20597.648 (rec:18377.174, round:2220.475)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	20418.781 (rec:18300.549, round:2118.233)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	21557.863 (rec:19538.619, round:2019.243)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	21064.473 (rec:19140.742, round:1923.731)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	20718.641 (rec:18887.982, round:1830.659)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	20455.596 (rec:18715.357, round:1740.239)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	20407.988 (rec:18755.908, round:1652.079)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	21514.387 (rec:19948.559, round:1565.829)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	20059.768 (rec:18577.600, round:1482.167)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	20683.730 (rec:19285.744, round:1397.986)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	19933.428 (rec:18617.570, round:1315.858)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	20143.814 (rec:18907.525, round:1236.288)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	20301.451 (rec:19142.166, round:1159.284)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	20017.584 (rec:18933.969, round:1083.615)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	20609.881 (rec:19600.652, round:1009.228)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	20570.877 (rec:19635.283, round:935.593)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	19674.627 (rec:18813.029, round:861.597)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	20790.154 (rec:20001.287, round:788.867)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	19046.676 (rec:18329.467, round:717.208)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	19755.285 (rec:19109.482, round:645.803)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	19064.264 (rec:18489.148, round:575.115)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	19931.707 (rec:19426.668, round:505.039)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	19127.752 (rec:18692.869, round:434.882)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	20709.898 (rec:20347.840, round:362.058)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_13_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_12_activation_post_act_fake_quantizer, trunk_output_block3_block3_13_f_a_0, trunk_output_block3_block3_13_f_a_1, trunk_output_block3_block3_13_f_a_2, trunk_output_block3_block3_13_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_13_f_b_0, trunk_output_block3_block3_13_f_b_1, trunk_output_block3_block3_13_f_b_2, trunk_output_block3_block3_13_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_13_f_c_0, trunk_output_block3_block3_13_f_c_1, add_21, trunk_output_block3_block3_13_activation, trunk_output_block3_block3_13_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_12_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_13_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-13").f.a, "0")(trunk_output_block3_block3_12_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_13_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-13").f.a, "1")(trunk_output_block3_block3_13_f_a_0);  trunk_output_block3_block3_13_f_a_0 = None
    trunk_output_block3_block3_13_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-13").f.a, "2")(trunk_output_block3_block3_13_f_a_1);  trunk_output_block3_block3_13_f_a_1 = None
    trunk_output_block3_block3_13_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_13_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_13_f_a_2);  trunk_output_block3_block3_13_f_a_2 = None
    trunk_output_block3_block3_13_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-13").f.b, "0")(trunk_output_block3_block3_13_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_13_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_13_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-13").f.b, "1")(trunk_output_block3_block3_13_f_b_0);  trunk_output_block3_block3_13_f_b_0 = None
    trunk_output_block3_block3_13_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-13").f.b, "2")(trunk_output_block3_block3_13_f_b_1);  trunk_output_block3_block3_13_f_b_1 = None
    trunk_output_block3_block3_13_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_13_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_13_f_b_2);  trunk_output_block3_block3_13_f_b_2 = None
    trunk_output_block3_block3_13_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-13").f.c, "0")(trunk_output_block3_block3_13_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_13_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_13_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-13").f.c, "1")(trunk_output_block3_block3_13_f_c_0);  trunk_output_block3_block3_13_f_c_0 = None
    add_21 = trunk_output_block3_block3_12_activation_post_act_fake_quantizer + trunk_output_block3_block3_13_f_c_1;  trunk_output_block3_block3_12_activation_post_act_fake_quantizer = trunk_output_block3_block3_13_f_c_1 = None
    trunk_output_block3_block3_13_activation = getattr(self.trunk_output.block3, "block3-13").activation(add_21);  add_21 = None
    trunk_output_block3_block3_13_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_13_activation_post_act_fake_quantizer(trunk_output_block3_block3_13_activation);  trunk_output_block3_block3_13_activation = None
    return trunk_output_block3_block3_13_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_13_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_13_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_13_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	19656.465 (rec:19656.465, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	20035.309 (rec:20035.309, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	20149.941 (rec:20149.941, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	20299.559 (rec:20299.559, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	20427.695 (rec:20427.695, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	20267.141 (rec:20267.141, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	20106.078 (rec:20106.078, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	25380.203 (rec:20506.320, round:4873.883)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	23729.428 (rec:20248.768, round:3480.660)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	23449.283 (rec:20247.990, round:3201.292)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	23756.225 (rec:20744.459, round:3011.766)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	23939.195 (rec:21080.953, round:2858.243)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	23740.643 (rec:21019.326, round:2721.317)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	23235.062 (rec:20638.961, round:2596.101)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	23985.188 (rec:21505.043, round:2480.144)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	22475.309 (rec:20104.885, round:2370.424)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	22828.428 (rec:20561.654, round:2266.773)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	22390.754 (rec:20222.109, round:2168.645)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	22194.818 (rec:20120.074, round:2074.744)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	21667.020 (rec:19683.953, round:1983.067)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	22794.867 (rec:20902.438, round:1892.429)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	21685.520 (rec:19880.457, round:1805.062)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	23451.238 (rec:21734.002, round:1717.236)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	21884.396 (rec:20253.574, round:1630.823)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	21776.854 (rec:20230.854, round:1546.000)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	24148.377 (rec:22685.744, round:1462.633)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	22284.754 (rec:20906.775, round:1377.979)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	22247.047 (rec:20950.869, round:1296.177)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	21921.445 (rec:20706.232, round:1215.212)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	22665.811 (rec:21532.133, round:1133.677)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	21037.316 (rec:19984.713, round:1052.604)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	20976.027 (rec:20003.012, round:973.016)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	21949.514 (rec:21056.379, round:893.135)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	21645.131 (rec:20829.416, round:815.715)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	21371.004 (rec:20633.102, round:737.903)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	21334.170 (rec:20673.500, round:660.670)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	22284.865 (rec:21699.641, round:585.225)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	21329.424 (rec:20817.635, round:511.790)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	22256.172 (rec:21816.412, round:439.759)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	20003.561 (rec:19636.775, round:366.786)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block3_block3_14_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_13_activation_post_act_fake_quantizer, trunk_output_block3_block3_14_f_a_0, trunk_output_block3_block3_14_f_a_1, trunk_output_block3_block3_14_f_a_2, trunk_output_block3_block3_14_f_a_2_post_act_fake_quantizer, trunk_output_block3_block3_14_f_b_0, trunk_output_block3_block3_14_f_b_1, trunk_output_block3_block3_14_f_b_2, trunk_output_block3_block3_14_f_b_2_post_act_fake_quantizer, trunk_output_block3_block3_14_f_c_0, trunk_output_block3_block3_14_f_c_1, add_22, trunk_output_block3_block3_14_activation, trunk_output_block3_block3_14_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_13_activation_post_act_fake_quantizer):
    trunk_output_block3_block3_14_f_a_0 = getattr(getattr(self.trunk_output.block3, "block3-14").f.a, "0")(trunk_output_block3_block3_13_activation_post_act_fake_quantizer)
    trunk_output_block3_block3_14_f_a_1 = getattr(getattr(self.trunk_output.block3, "block3-14").f.a, "1")(trunk_output_block3_block3_14_f_a_0);  trunk_output_block3_block3_14_f_a_0 = None
    trunk_output_block3_block3_14_f_a_2 = getattr(getattr(self.trunk_output.block3, "block3-14").f.a, "2")(trunk_output_block3_block3_14_f_a_1);  trunk_output_block3_block3_14_f_a_1 = None
    trunk_output_block3_block3_14_f_a_2_post_act_fake_quantizer = self.trunk_output_block3_block3_14_f_a_2_post_act_fake_quantizer(trunk_output_block3_block3_14_f_a_2);  trunk_output_block3_block3_14_f_a_2 = None
    trunk_output_block3_block3_14_f_b_0 = getattr(getattr(self.trunk_output.block3, "block3-14").f.b, "0")(trunk_output_block3_block3_14_f_a_2_post_act_fake_quantizer);  trunk_output_block3_block3_14_f_a_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_14_f_b_1 = getattr(getattr(self.trunk_output.block3, "block3-14").f.b, "1")(trunk_output_block3_block3_14_f_b_0);  trunk_output_block3_block3_14_f_b_0 = None
    trunk_output_block3_block3_14_f_b_2 = getattr(getattr(self.trunk_output.block3, "block3-14").f.b, "2")(trunk_output_block3_block3_14_f_b_1);  trunk_output_block3_block3_14_f_b_1 = None
    trunk_output_block3_block3_14_f_b_2_post_act_fake_quantizer = self.trunk_output_block3_block3_14_f_b_2_post_act_fake_quantizer(trunk_output_block3_block3_14_f_b_2);  trunk_output_block3_block3_14_f_b_2 = None
    trunk_output_block3_block3_14_f_c_0 = getattr(getattr(self.trunk_output.block3, "block3-14").f.c, "0")(trunk_output_block3_block3_14_f_b_2_post_act_fake_quantizer);  trunk_output_block3_block3_14_f_b_2_post_act_fake_quantizer = None
    trunk_output_block3_block3_14_f_c_1 = getattr(getattr(self.trunk_output.block3, "block3-14").f.c, "1")(trunk_output_block3_block3_14_f_c_0);  trunk_output_block3_block3_14_f_c_0 = None
    add_22 = trunk_output_block3_block3_13_activation_post_act_fake_quantizer + trunk_output_block3_block3_14_f_c_1;  trunk_output_block3_block3_13_activation_post_act_fake_quantizer = trunk_output_block3_block3_14_f_c_1 = None
    trunk_output_block3_block3_14_activation = getattr(self.trunk_output.block3, "block3-14").activation(add_22);  add_22 = None
    trunk_output_block3_block3_14_activation_post_act_fake_quantizer = self.trunk_output_block3_block3_14_activation_post_act_fake_quantizer(trunk_output_block3_block3_14_activation);  trunk_output_block3_block3_14_activation = None
    return trunk_output_block3_block3_14_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_14_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_14_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block3_block3_14_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	17831.061 (rec:17831.061, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	18345.311 (rec:18345.311, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	18365.750 (rec:18365.750, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	18697.699 (rec:18697.699, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	18800.328 (rec:18800.328, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	18899.518 (rec:18899.518, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	18904.846 (rec:18904.846, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	23764.711 (rec:18880.873, round:4883.839)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	21936.039 (rec:18317.721, round:3618.319)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	21597.969 (rec:18255.846, round:3342.123)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	21818.781 (rec:18668.969, round:3149.812)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	21369.930 (rec:18377.605, round:2992.323)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	21365.658 (rec:18514.150, round:2851.508)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	21214.846 (rec:18494.961, round:2719.885)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	20711.969 (rec:18117.473, round:2594.496)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	20676.574 (rec:18202.311, round:2474.263)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	20769.977 (rec:18413.602, round:2356.375)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	20639.766 (rec:18398.406, round:2241.359)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	20084.271 (rec:17953.311, round:2130.961)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	20330.258 (rec:18306.434, round:2023.825)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	20191.795 (rec:18271.273, round:1920.521)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	20112.248 (rec:18293.445, round:1818.802)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	19865.848 (rec:18145.883, round:1719.964)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	20147.117 (rec:18523.025, round:1624.092)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	19889.775 (rec:18358.900, round:1530.875)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	19707.395 (rec:18264.592, round:1442.802)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	19410.008 (rec:18052.779, round:1357.229)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	19625.105 (rec:18351.881, round:1273.224)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	19190.240 (rec:17999.270, round:1190.971)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	19192.441 (rec:18082.227, round:1110.215)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	18965.467 (rec:17934.723, round:1030.744)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	19715.742 (rec:18763.023, round:952.719)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	18866.043 (rec:17990.561, round:875.483)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	19120.189 (rec:18319.896, round:800.294)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	18625.318 (rec:17898.535, round:726.783)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	18971.836 (rec:18319.068, round:652.767)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	18996.791 (rec:18416.637, round:580.154)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	18388.746 (rec:17880.498, round:508.247)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	19160.693 (rec:18722.840, round:437.854)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	18796.068 (rec:18430.346, round:365.723)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block4_block4_0_proj_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block3_block3_14_activation_post_act_fake_quantizer, trunk_output_block4_block4_0_proj_0, trunk_output_block4_block4_0_proj_1, trunk_output_block4_block4_0_f_a_0, trunk_output_block4_block4_0_f_a_1, trunk_output_block4_block4_0_f_a_2, trunk_output_block4_block4_0_f_a_2_post_act_fake_quantizer, trunk_output_block4_block4_0_f_b_0, trunk_output_block4_block4_0_f_b_1, trunk_output_block4_block4_0_f_b_2, trunk_output_block4_block4_0_f_b_2_post_act_fake_quantizer, trunk_output_block4_block4_0_f_c_0, trunk_output_block4_block4_0_f_c_1, add_23, trunk_output_block4_block4_0_activation, trunk_output_block4_block4_0_activation_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block3_block3_14_activation_post_act_fake_quantizer):
    trunk_output_block4_block4_0_proj_0 = getattr(getattr(self.trunk_output.block4, "block4-0").proj, "0")(trunk_output_block3_block3_14_activation_post_act_fake_quantizer)
    trunk_output_block4_block4_0_proj_1 = getattr(getattr(self.trunk_output.block4, "block4-0").proj, "1")(trunk_output_block4_block4_0_proj_0);  trunk_output_block4_block4_0_proj_0 = None
    trunk_output_block4_block4_0_f_a_0 = getattr(getattr(self.trunk_output.block4, "block4-0").f.a, "0")(trunk_output_block3_block3_14_activation_post_act_fake_quantizer);  trunk_output_block3_block3_14_activation_post_act_fake_quantizer = None
    trunk_output_block4_block4_0_f_a_1 = getattr(getattr(self.trunk_output.block4, "block4-0").f.a, "1")(trunk_output_block4_block4_0_f_a_0);  trunk_output_block4_block4_0_f_a_0 = None
    trunk_output_block4_block4_0_f_a_2 = getattr(getattr(self.trunk_output.block4, "block4-0").f.a, "2")(trunk_output_block4_block4_0_f_a_1);  trunk_output_block4_block4_0_f_a_1 = None
    trunk_output_block4_block4_0_f_a_2_post_act_fake_quantizer = self.trunk_output_block4_block4_0_f_a_2_post_act_fake_quantizer(trunk_output_block4_block4_0_f_a_2);  trunk_output_block4_block4_0_f_a_2 = None
    trunk_output_block4_block4_0_f_b_0 = getattr(getattr(self.trunk_output.block4, "block4-0").f.b, "0")(trunk_output_block4_block4_0_f_a_2_post_act_fake_quantizer);  trunk_output_block4_block4_0_f_a_2_post_act_fake_quantizer = None
    trunk_output_block4_block4_0_f_b_1 = getattr(getattr(self.trunk_output.block4, "block4-0").f.b, "1")(trunk_output_block4_block4_0_f_b_0);  trunk_output_block4_block4_0_f_b_0 = None
    trunk_output_block4_block4_0_f_b_2 = getattr(getattr(self.trunk_output.block4, "block4-0").f.b, "2")(trunk_output_block4_block4_0_f_b_1);  trunk_output_block4_block4_0_f_b_1 = None
    trunk_output_block4_block4_0_f_b_2_post_act_fake_quantizer = self.trunk_output_block4_block4_0_f_b_2_post_act_fake_quantizer(trunk_output_block4_block4_0_f_b_2);  trunk_output_block4_block4_0_f_b_2 = None
    trunk_output_block4_block4_0_f_c_0 = getattr(getattr(self.trunk_output.block4, "block4-0").f.c, "0")(trunk_output_block4_block4_0_f_b_2_post_act_fake_quantizer);  trunk_output_block4_block4_0_f_b_2_post_act_fake_quantizer = None
    trunk_output_block4_block4_0_f_c_1 = getattr(getattr(self.trunk_output.block4, "block4-0").f.c, "1")(trunk_output_block4_block4_0_f_c_0);  trunk_output_block4_block4_0_f_c_0 = None
    add_23 = trunk_output_block4_block4_0_proj_1 + trunk_output_block4_block4_0_f_c_1;  trunk_output_block4_block4_0_proj_1 = trunk_output_block4_block4_0_f_c_1 = None
    trunk_output_block4_block4_0_activation = getattr(self.trunk_output.block4, "block4-0").activation(add_23);  add_23 = None
    trunk_output_block4_block4_0_activation_post_act_fake_quantizer = self.trunk_output_block4_block4_0_activation_post_act_fake_quantizer(trunk_output_block4_block4_0_activation);  trunk_output_block4_block4_0_activation = None
    return trunk_output_block4_block4_0_activation_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block4_block4_0_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block4_block4_0_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block4_block4_0_activation_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	1492.144 (rec:1492.144, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	1429.930 (rec:1429.930, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	1450.783 (rec:1450.783, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	1394.144 (rec:1394.144, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	1441.147 (rec:1441.147, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	1253.043 (rec:1253.043, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	1519.154 (rec:1519.154, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	22677.068 (rec:1448.455, round:21228.613)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	14117.857 (rec:1564.627, round:12553.230)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	13172.508 (rec:1624.221, round:11548.287)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	12474.269 (rec:1546.479, round:10927.789)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	11945.567 (rec:1508.255, round:10437.312)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	11560.077 (rec:1552.821, round:10007.256)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	11061.751 (rec:1453.419, round:9608.332)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	10779.918 (rec:1554.730, round:9225.188)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	10192.812 (rec:1336.753, round:8856.059)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	9921.033 (rec:1427.256, round:8493.777)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	9656.806 (rec:1520.752, round:8136.054)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	9256.937 (rec:1477.862, round:7779.074)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	8853.005 (rec:1429.765, round:7423.240)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	8558.665 (rec:1491.010, round:7067.655)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	8167.212 (rec:1459.924, round:6707.288)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	7776.244 (rec:1427.826, round:6348.418)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	7493.317 (rec:1503.203, round:5990.114)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	7106.697 (rec:1475.480, round:5631.218)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	6825.692 (rec:1551.383, round:5274.309)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	6504.417 (rec:1589.134, round:4915.284)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	6088.568 (rec:1536.762, round:4551.806)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	5649.612 (rec:1464.686, round:4184.926)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	5192.642 (rec:1374.206, round:3818.436)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	5011.485 (rec:1555.477, round:3456.008)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	4643.945 (rec:1545.872, round:3098.073)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	4206.609 (rec:1463.080, round:2743.530)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	4019.574 (rec:1625.730, round:2393.844)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	3447.932 (rec:1392.953, round:2054.979)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	3311.116 (rec:1583.741, round:1727.375)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	2934.154 (rec:1519.630, round:1414.525)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2520.072 (rec:1394.868, round:1125.205)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	2327.810 (rec:1465.183, round:862.627)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	2212.799 (rec:1594.972, round:617.827)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for trunk_output_block4_block4_1_f_a_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [trunk_output_block4_block4_0_activation_post_act_fake_quantizer, trunk_output_block4_block4_1_f_a_0, trunk_output_block4_block4_1_f_a_1, trunk_output_block4_block4_1_f_a_2, trunk_output_block4_block4_1_f_a_2_post_act_fake_quantizer, trunk_output_block4_block4_1_f_b_0, trunk_output_block4_block4_1_f_b_1, trunk_output_block4_block4_1_f_b_2, trunk_output_block4_block4_1_f_b_2_post_act_fake_quantizer, trunk_output_block4_block4_1_f_c_0, trunk_output_block4_block4_1_f_c_1, add_24, trunk_output_block4_block4_1_activation, avgpool, flatten, flatten_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, trunk_output_block4_block4_0_activation_post_act_fake_quantizer):
    trunk_output_block4_block4_1_f_a_0 = getattr(getattr(self.trunk_output.block4, "block4-1").f.a, "0")(trunk_output_block4_block4_0_activation_post_act_fake_quantizer)
    trunk_output_block4_block4_1_f_a_1 = getattr(getattr(self.trunk_output.block4, "block4-1").f.a, "1")(trunk_output_block4_block4_1_f_a_0);  trunk_output_block4_block4_1_f_a_0 = None
    trunk_output_block4_block4_1_f_a_2 = getattr(getattr(self.trunk_output.block4, "block4-1").f.a, "2")(trunk_output_block4_block4_1_f_a_1);  trunk_output_block4_block4_1_f_a_1 = None
    trunk_output_block4_block4_1_f_a_2_post_act_fake_quantizer = self.trunk_output_block4_block4_1_f_a_2_post_act_fake_quantizer(trunk_output_block4_block4_1_f_a_2);  trunk_output_block4_block4_1_f_a_2 = None
    trunk_output_block4_block4_1_f_b_0 = getattr(getattr(self.trunk_output.block4, "block4-1").f.b, "0")(trunk_output_block4_block4_1_f_a_2_post_act_fake_quantizer);  trunk_output_block4_block4_1_f_a_2_post_act_fake_quantizer = None
    trunk_output_block4_block4_1_f_b_1 = getattr(getattr(self.trunk_output.block4, "block4-1").f.b, "1")(trunk_output_block4_block4_1_f_b_0);  trunk_output_block4_block4_1_f_b_0 = None
    trunk_output_block4_block4_1_f_b_2 = getattr(getattr(self.trunk_output.block4, "block4-1").f.b, "2")(trunk_output_block4_block4_1_f_b_1);  trunk_output_block4_block4_1_f_b_1 = None
    trunk_output_block4_block4_1_f_b_2_post_act_fake_quantizer = self.trunk_output_block4_block4_1_f_b_2_post_act_fake_quantizer(trunk_output_block4_block4_1_f_b_2);  trunk_output_block4_block4_1_f_b_2 = None
    trunk_output_block4_block4_1_f_c_0 = getattr(getattr(self.trunk_output.block4, "block4-1").f.c, "0")(trunk_output_block4_block4_1_f_b_2_post_act_fake_quantizer);  trunk_output_block4_block4_1_f_b_2_post_act_fake_quantizer = None
    trunk_output_block4_block4_1_f_c_1 = getattr(getattr(self.trunk_output.block4, "block4-1").f.c, "1")(trunk_output_block4_block4_1_f_c_0);  trunk_output_block4_block4_1_f_c_0 = None
    add_24 = trunk_output_block4_block4_0_activation_post_act_fake_quantizer + trunk_output_block4_block4_1_f_c_1;  trunk_output_block4_block4_0_activation_post_act_fake_quantizer = trunk_output_block4_block4_1_f_c_1 = None
    trunk_output_block4_block4_1_activation = getattr(self.trunk_output.block4, "block4-1").activation(add_24);  add_24 = None
    avgpool = self.avgpool(trunk_output_block4_block4_1_activation);  trunk_output_block4_block4_1_activation = None
    flatten = avgpool.flatten(start_dim = 1);  avgpool = None
    flatten_post_act_fake_quantizer = self.flatten_post_act_fake_quantizer(flatten);  flatten = None
    return flatten_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for trunk_output_block4_block4_1_f_a_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for trunk_output_block4_block4_1_f_b_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for flatten_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	116.437 (rec:116.437, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	78.812 (rec:78.812, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	82.163 (rec:82.163, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	81.319 (rec:81.319, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	86.277 (rec:86.277, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	82.824 (rec:82.824, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	92.623 (rec:92.623, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	22777.445 (rec:89.656, round:22687.789)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	7518.114 (rec:92.484, round:7425.630)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	6869.789 (rec:96.062, round:6773.727)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	6483.462 (rec:87.833, round:6395.629)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	6192.838 (rec:99.496, round:6093.342)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	5919.958 (rec:96.472, round:5823.485)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	5652.780 (rec:87.108, round:5565.673)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	5400.031 (rec:85.053, round:5314.978)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	5141.384 (rec:76.143, round:5065.241)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	4894.509 (rec:76.340, round:4818.169)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	4680.381 (rec:109.839, round:4570.542)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	4432.812 (rec:109.221, round:4323.591)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	4161.580 (rec:82.225, round:4079.354)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	3926.696 (rec:95.238, round:3831.458)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	3677.648 (rec:96.054, round:3581.594)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	3414.096 (rec:84.758, round:3329.338)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	3153.299 (rec:78.807, round:3074.492)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	2930.306 (rec:107.931, round:2822.375)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	2673.765 (rec:106.689, round:2567.077)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	2396.006 (rec:84.101, round:2311.906)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	2140.438 (rec:78.206, round:2062.231)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	1923.186 (rec:108.082, round:1815.103)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	1688.899 (rec:112.898, round:1576.002)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	1432.975 (rec:94.063, round:1338.911)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	1197.389 (rec:84.165, round:1113.224)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	981.834 (rec:80.132, round:901.702)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	807.042 (rec:93.977, round:713.065)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	637.843 (rec:90.413, round:547.430)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	482.732 (rec:72.070, round:410.661)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	401.916 (rec:94.435, round:307.480)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	329.555 (rec:94.050, round:235.505)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	263.762 (rec:78.982, round:184.781)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	233.832 (rec:95.222, round:138.611)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for fc
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [flatten_post_act_fake_quantizer, fc]
[MQBENCH] INFO: 


def forward(self, flatten_post_act_fake_quantizer):
    fc = self.fc(flatten_post_act_fake_quantizer);  flatten_post_act_fake_quantizer = None
    return fc
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	1126.411 (rec:1126.411, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	908.588 (rec:908.588, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	757.118 (rec:757.118, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	579.188 (rec:579.188, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	615.698 (rec:615.698, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	590.514 (rec:590.514, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	526.851 (rec:526.851, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	8131.960 (rec:572.942, round:7559.018)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	4967.352 (rec:502.359, round:4464.993)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	4643.963 (rec:535.572, round:4108.391)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	4393.120 (rec:515.282, round:3877.838)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	4214.359 (rec:536.934, round:3677.426)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	3982.302 (rec:497.504, round:3484.799)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	3803.274 (rec:502.921, round:3300.353)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	3542.641 (rec:427.317, round:3115.324)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	3431.708 (rec:496.345, round:2935.363)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	3305.328 (rec:540.906, round:2764.422)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	3095.107 (rec:494.572, round:2600.535)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	2932.795 (rec:490.777, round:2442.018)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	2837.918 (rec:545.573, round:2292.345)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	2693.587 (rec:545.053, round:2148.534)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	2521.039 (rec:509.753, round:2011.286)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	2391.203 (rec:510.451, round:1880.752)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	2261.324 (rec:507.260, round:1754.065)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	2131.102 (rec:497.969, round:1633.133)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	2045.617 (rec:529.481, round:1516.137)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	1943.288 (rec:540.901, round:1402.386)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	1827.861 (rec:538.778, round:1289.084)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	1686.482 (rec:505.690, round:1180.791)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	1618.392 (rec:543.231, round:1075.162)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	1393.122 (rec:422.977, round:970.146)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	1404.219 (rec:541.070, round:863.148)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	1263.838 (rec:507.591, round:756.247)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	1177.010 (rec:527.492, round:649.518)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	1049.098 (rec:504.897, round:544.202)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	1019.266 (rec:580.461, round:438.805)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	838.949 (rec:505.269, round:333.680)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	766.759 (rec:536.618, round:230.141)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	587.372 (rec:455.191, round:132.181)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	564.865 (rec:503.480, round:61.385)	b=2.00	count=20000
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node x_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node stem.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node stem.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node stem.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node stem_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-0.proj.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-0.proj.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-0.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-0.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-0.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block1_block1_0_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-0.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-0.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-0.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block1_block1_0_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-0.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-0.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-0.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block1_block1_0_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-1.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-1.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-1.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block1_block1_1_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-1.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-1.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-1.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block1_block1_1_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-1.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-1.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block1.block1-1.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block1_block1_1_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-0.proj.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-0.proj.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-0.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-0.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-0.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_0_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-0.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-0.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-0.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_0_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-0.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-0.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-0.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_0_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-1.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-1.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-1.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_1_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-1.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-1.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-1.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_1_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-1.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-1.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-1.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_1_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-2.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-2.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-2.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_2_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-2.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-2.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-2.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_2_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-2.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-2.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-2.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_2_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-3.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-3.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-3.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_3_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-3.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-3.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-3.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_3_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-3.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-3.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-3.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_3_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-4.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-4.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-4.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_4_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-4.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-4.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-4.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_4_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-4.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-4.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-4.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_4_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-5.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-5.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-5.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_5_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-5.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-5.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-5.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_5_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-5.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-5.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block2.block2-5.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block2_block2_5_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-0.proj.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-0.proj.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-0.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-0.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-0.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_0_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-0.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-0.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-0.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_0_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-0.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-0.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-0.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_0_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-1.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-1.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-1.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_1_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-1.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-1.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-1.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_1_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-1.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-1.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-1.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_1_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-2.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-2.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-2.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_2_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-2.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-2.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-2.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_2_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-2.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-2.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-2.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_2_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-3.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-3.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-3.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_3_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-3.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-3.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-3.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_3_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-3.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-3.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-3.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_3_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-4.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-4.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-4.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_4_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-4.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-4.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-4.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_4_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-4.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-4.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-4.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_4_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-5.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-5.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-5.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_5_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-5.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-5.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-5.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_5_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-5.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-5.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-5.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_5_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-6.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-6.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-6.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_6_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-6.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-6.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-6.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_6_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-6.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-6.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-6.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_6_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-7.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-7.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-7.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_7_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-7.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-7.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-7.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_7_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-7.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-7.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-7.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_7_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-8.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-8.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-8.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_8_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-8.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-8.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-8.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_8_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-8.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-8.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-8.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_8_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-9.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-9.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-9.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_9_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-9.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-9.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-9.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_9_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-9.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-9.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-9.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_9_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-10.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-10.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-10.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_10_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-10.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-10.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-10.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_10_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-10.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-10.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-10.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_10_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-11.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-11.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-11.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_11_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-11.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-11.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-11.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_11_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-11.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-11.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-11.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_11_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-12.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-12.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-12.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_12_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-12.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-12.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-12.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_12_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-12.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-12.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-12.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_12_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-13.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-13.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-13.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_13_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-13.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-13.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-13.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_13_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-13.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-13.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-13.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_13_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-14.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-14.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-14.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_14_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-14.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-14.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-14.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_14_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-14.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-14.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block3.block3-14.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block3_block3_14_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-0.proj.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-0.proj.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-0.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-0.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-0.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block4_block4_0_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-0.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-0.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-0.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block4_block4_0_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-0.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-0.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-0.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block4_block4_0_activation_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-1.f.a.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-1.f.a.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-1.f.a.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block4_block4_1_f_a_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-1.f.b.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-1.f.b.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-1.f.b.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output_block4_block4_1_f_b_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-1.f.c.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-1.f.c.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node trunk_output.block4.block4-1.activation in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node avgpool in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node flatten_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node fc in quant
2025-08-18 15:39:48,277 | INFO | ✔ END: advanced PTQ reconstruction (elapsed 4761.20s)
2025-08-18 15:39:48,279 | INFO | ▶ START: enable_quantization (simulate INT8)
[MQBENCH] INFO: Disable observer and Enable quantize.
2025-08-18 15:39:48,284 | INFO | ✔ END: enable_quantization (simulate INT8) (elapsed 0.01s)
2025-08-18 15:39:48,285 | INFO | ✔ END: prepare_by_platform(Academic) (elapsed 4766.76s)
2025-08-18 15:39:48,286 | INFO | ▶ START: evaluate INT8-sim
2025-08-18 15:39:51,275 | INFO | [EVAL_INT8] progress: 50 batches, running top1=0.47%
2025-08-18 15:39:53,520 | INFO | [EVAL_INT8] progress: 100 batches, running top1=0.23%
2025-08-18 15:39:55,761 | INFO | [EVAL_INT8] progress: 150 batches, running top1=0.16%
2025-08-18 15:39:58,010 | INFO | [EVAL_INT8] progress: 200 batches, running top1=0.12%
2025-08-18 15:40:00,345 | INFO | [EVAL_INT8] progress: 250 batches, running top1=0.09%
2025-08-18 15:40:02,606 | INFO | [EVAL_INT8] progress: 300 batches, running top1=0.08%
2025-08-18 15:40:04,853 | INFO | [EVAL_INT8] progress: 350 batches, running top1=0.07%
2025-08-18 15:40:07,103 | INFO | [EVAL_INT8] progress: 400 batches, running top1=0.06%
2025-08-18 15:40:09,368 | INFO | [EVAL_INT8] progress: 450 batches, running top1=0.05%
2025-08-18 15:40:11,605 | INFO | [EVAL_INT8] progress: 500 batches, running top1=0.05%
2025-08-18 15:40:13,830 | INFO | [EVAL_INT8] progress: 550 batches, running top1=0.04%
2025-08-18 15:40:16,051 | INFO | [EVAL_INT8] progress: 600 batches, running top1=0.04%
2025-08-18 15:40:18,283 | INFO | [EVAL_INT8] progress: 650 batches, running top1=0.04%
2025-08-18 15:40:20,513 | INFO | [EVAL_INT8] progress: 700 batches, running top1=0.03%
2025-08-18 15:40:22,740 | INFO | [EVAL_INT8] progress: 750 batches, running top1=0.03%
2025-08-18 15:40:24,199 | INFO | [EVAL_INT8] done: 782 batches in 35.91s, top1=0.11%
2025-08-18 15:40:24,200 | INFO | [PTQ][regnet_x_3_2gf][Academic] [ADV] Top-1 = 0.11%
2025-08-18 15:40:24,200 | INFO | ✔ END: evaluate INT8-sim (elapsed 35.91s)
2025-08-18 15:40:24,200 | INFO | ▶ START: evaluate FP32 baseline
2025-08-18 15:40:26,855 | INFO | [EVAL_FP32] progress: 50 batches, running top1=85.84%
2025-08-18 15:40:28,869 | INFO | [EVAL_FP32] progress: 100 batches, running top1=86.62%
2025-08-18 15:40:30,760 | INFO | [EVAL_FP32] progress: 150 batches, running top1=86.46%
2025-08-18 15:40:32,686 | INFO | [EVAL_FP32] progress: 200 batches, running top1=85.96%
2025-08-18 15:40:34,650 | INFO | [EVAL_FP32] progress: 250 batches, running top1=85.72%
2025-08-18 15:40:36,626 | INFO | [EVAL_FP32] progress: 300 batches, running top1=85.83%
2025-08-18 15:40:38,441 | INFO | [EVAL_FP32] progress: 350 batches, running top1=84.94%
2025-08-18 15:40:40,462 | INFO | [EVAL_FP32] progress: 400 batches, running top1=83.70%
2025-08-18 15:40:42,390 | INFO | [EVAL_FP32] progress: 450 batches, running top1=83.33%
2025-08-18 15:40:44,347 | INFO | [EVAL_FP32] progress: 500 batches, running top1=82.68%
2025-08-18 15:40:46,299 | INFO | [EVAL_FP32] progress: 550 batches, running top1=82.36%
2025-08-18 15:40:48,189 | INFO | [EVAL_FP32] progress: 600 batches, running top1=82.05%
2025-08-18 15:40:50,122 | INFO | [EVAL_FP32] progress: 650 batches, running top1=81.71%
2025-08-18 15:40:51,991 | INFO | [EVAL_FP32] progress: 700 batches, running top1=81.34%
2025-08-18 15:40:53,814 | INFO | [EVAL_FP32] progress: 750 batches, running top1=81.30%
2025-08-18 15:40:55,118 | INFO | [EVAL_FP32] done: 782 batches in 30.92s, top1=81.18%
2025-08-18 15:40:55,118 | INFO | [FP32] Top-1 = 81.18% (expected ~None)
2025-08-18 15:40:55,118 | INFO | ✔ END: evaluate FP32 baseline (elapsed 30.92s)
2025-08-18 15:40:55,119 | INFO | ▶ START: extract model logits
2025-08-18 15:40:55,122 | INFO | Extracting logits from both models...

============================================================
BASELINE ACCURACIES (Before Clustering)
============================================================
  FP32 Model: 81.18%
  Baseline PTQ: 0.11%
  PTQ Degradation: 81.07%
============================================================
Extracting logits from quantized and full-precision models...
2025-08-18 15:40:57,911 | INFO | Processed 5 batches
2025-08-18 15:40:59,559 | INFO | Processed 10 batches
2025-08-18 15:41:00,863 | INFO | Extracted logits: Q=torch.Size([640, 1000]), FP=torch.Size([640, 1000])
Logits extraction complete.
Quantized logits shape: torch.Size([640, 1000])
Full-precision logits shape: torch.Size([640, 1000])
🔍 Parameter ranges to test:
  Alpha values: [0.2, 0.4, 0.6, 0.8, 1.0]
  Cluster numbers: [8, 16, 32, 64]
  PCA dimensions: [25, 50, 100]
  Total combinations: 60
🚀 Running all 60 combinations...

🔄 [1/60] Running with alpha=0.2, num_clusters=8, pca_dim=25
[Alpha=0.20] Top-1 Accuracy: 0.11%
[Alpha=0.20] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%

🔄 [2/60] Running with alpha=0.2, num_clusters=8, pca_dim=50
[Alpha=0.20] Top-1 Accuracy: 0.11%
[Alpha=0.20] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%

🔄 [3/60] Running with alpha=0.2, num_clusters=8, pca_dim=100
[Alpha=0.20] Top-1 Accuracy: 0.11%
[Alpha=0.20] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%

🔄 [4/60] Running with alpha=0.2, num_clusters=16, pca_dim=25
[Alpha=0.20] Top-1 Accuracy: 0.11%
[Alpha=0.20] Top-5 Accuracy: 0.51%
✅ Result: Top-1: 0.11%, Top-5: 0.51%

🔄 [5/60] Running with alpha=0.2, num_clusters=16, pca_dim=50
[Alpha=0.20] Top-1 Accuracy: 0.11%
[Alpha=0.20] Top-5 Accuracy: 0.51%
✅ Result: Top-1: 0.11%, Top-5: 0.51%
💾 Saving intermediate results... (5 total combinations)
Results saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_results_20250818_154553.csv
💾 Recovery checkpoint saved: adaround_fixed_regnet_x_3_2gf_20250818_142005/recovery_checkpoint.json

🔄 [6/60] Running with alpha=0.2, num_clusters=16, pca_dim=100
[Alpha=0.20] Top-1 Accuracy: 0.11%
[Alpha=0.20] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%

🔄 [7/60] Running with alpha=0.2, num_clusters=32, pca_dim=25
[Alpha=0.20] Top-1 Accuracy: 0.11%
[Alpha=0.20] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%

🔄 [8/60] Running with alpha=0.2, num_clusters=32, pca_dim=50
[Alpha=0.20] Top-1 Accuracy: 0.11%
[Alpha=0.20] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%

🔄 [9/60] Running with alpha=0.2, num_clusters=32, pca_dim=100
[Alpha=0.20] Top-1 Accuracy: 0.11%
[Alpha=0.20] Top-5 Accuracy: 0.51%
✅ Result: Top-1: 0.11%, Top-5: 0.51%

🔄 [10/60] Running with alpha=0.2, num_clusters=64, pca_dim=25
[Alpha=0.20] Top-1 Accuracy: 0.11%
[Alpha=0.20] Top-5 Accuracy: 0.49%
✅ Result: Top-1: 0.11%, Top-5: 0.49%
💾 Saving intermediate results... (10 total combinations)
Results saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_results_20250818_155045.csv
💾 Recovery checkpoint saved: adaround_fixed_regnet_x_3_2gf_20250818_142005/recovery_checkpoint.json

🔄 [11/60] Running with alpha=0.2, num_clusters=64, pca_dim=50
[Alpha=0.20] Top-1 Accuracy: 0.11%
[Alpha=0.20] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%

🔄 [12/60] Running with alpha=0.2, num_clusters=64, pca_dim=100
[Alpha=0.20] Top-1 Accuracy: 0.11%
[Alpha=0.20] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%

🔄 [13/60] Running with alpha=0.4, num_clusters=8, pca_dim=25
[Alpha=0.40] Top-1 Accuracy: 0.11%
[Alpha=0.40] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%

🔄 [14/60] Running with alpha=0.4, num_clusters=8, pca_dim=50
[Alpha=0.40] Top-1 Accuracy: 0.11%
[Alpha=0.40] Top-5 Accuracy: 0.51%
✅ Result: Top-1: 0.11%, Top-5: 0.51%

🔄 [15/60] Running with alpha=0.4, num_clusters=8, pca_dim=100
[Alpha=0.40] Top-1 Accuracy: 0.11%
[Alpha=0.40] Top-5 Accuracy: 0.51%
✅ Result: Top-1: 0.11%, Top-5: 0.51%
💾 Saving intermediate results... (15 total combinations)
Results saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_results_20250818_155539.csv
💾 Recovery checkpoint saved: adaround_fixed_regnet_x_3_2gf_20250818_142005/recovery_checkpoint.json

🔄 [16/60] Running with alpha=0.4, num_clusters=16, pca_dim=25
[Alpha=0.40] Top-1 Accuracy: 0.11%
[Alpha=0.40] Top-5 Accuracy: 0.51%
✅ Result: Top-1: 0.11%, Top-5: 0.51%

🔄 [17/60] Running with alpha=0.4, num_clusters=16, pca_dim=50
[Alpha=0.40] Top-1 Accuracy: 0.11%
[Alpha=0.40] Top-5 Accuracy: 0.51%
✅ Result: Top-1: 0.11%, Top-5: 0.51%

🔄 [18/60] Running with alpha=0.4, num_clusters=16, pca_dim=100
[Alpha=0.40] Top-1 Accuracy: 0.11%
[Alpha=0.40] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%

🔄 [19/60] Running with alpha=0.4, num_clusters=32, pca_dim=25
[Alpha=0.40] Top-1 Accuracy: 0.11%
[Alpha=0.40] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%

🔄 [20/60] Running with alpha=0.4, num_clusters=32, pca_dim=50
[Alpha=0.40] Top-1 Accuracy: 0.11%
[Alpha=0.40] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%
💾 Saving intermediate results... (20 total combinations)
Results saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_results_20250818_160033.csv
💾 Recovery checkpoint saved: adaround_fixed_regnet_x_3_2gf_20250818_142005/recovery_checkpoint.json

🔄 [21/60] Running with alpha=0.4, num_clusters=32, pca_dim=100
[Alpha=0.40] Top-1 Accuracy: 0.10%
[Alpha=0.40] Top-5 Accuracy: 0.51%
✅ Result: Top-1: 0.10%, Top-5: 0.51%

🔄 [22/60] Running with alpha=0.4, num_clusters=64, pca_dim=25
[Alpha=0.40] Top-1 Accuracy: 0.10%
[Alpha=0.40] Top-5 Accuracy: 0.48%
✅ Result: Top-1: 0.10%, Top-5: 0.48%

🔄 [23/60] Running with alpha=0.4, num_clusters=64, pca_dim=50
[Alpha=0.40] Top-1 Accuracy: 0.11%
[Alpha=0.40] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%

🔄 [24/60] Running with alpha=0.4, num_clusters=64, pca_dim=100
[Alpha=0.40] Top-1 Accuracy: 0.10%
[Alpha=0.40] Top-5 Accuracy: 0.51%
✅ Result: Top-1: 0.10%, Top-5: 0.51%

🔄 [25/60] Running with alpha=0.6, num_clusters=8, pca_dim=25
[Alpha=0.60] Top-1 Accuracy: 0.12%
[Alpha=0.60] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.12%, Top-5: 0.50%
💾 Saving intermediate results... (25 total combinations)
Results saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_results_20250818_160526.csv
💾 Recovery checkpoint saved: adaround_fixed_regnet_x_3_2gf_20250818_142005/recovery_checkpoint.json

🔄 [26/60] Running with alpha=0.6, num_clusters=8, pca_dim=50
[Alpha=0.60] Top-1 Accuracy: 0.11%
[Alpha=0.60] Top-5 Accuracy: 0.49%
✅ Result: Top-1: 0.11%, Top-5: 0.49%

🔄 [27/60] Running with alpha=0.6, num_clusters=8, pca_dim=100
[Alpha=0.60] Top-1 Accuracy: 0.11%
[Alpha=0.60] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%

🔄 [28/60] Running with alpha=0.6, num_clusters=16, pca_dim=25
[Alpha=0.60] Top-1 Accuracy: 0.11%
[Alpha=0.60] Top-5 Accuracy: 0.52%
✅ Result: Top-1: 0.11%, Top-5: 0.52%

🔄 [29/60] Running with alpha=0.6, num_clusters=16, pca_dim=50
[Alpha=0.60] Top-1 Accuracy: 0.11%
[Alpha=0.60] Top-5 Accuracy: 0.51%
✅ Result: Top-1: 0.11%, Top-5: 0.51%

🔄 [30/60] Running with alpha=0.6, num_clusters=16, pca_dim=100
[Alpha=0.60] Top-1 Accuracy: 0.11%
[Alpha=0.60] Top-5 Accuracy: 0.52%
✅ Result: Top-1: 0.11%, Top-5: 0.52%
💾 Saving intermediate results... (30 total combinations)
Results saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_results_20250818_161021.csv
💾 Recovery checkpoint saved: adaround_fixed_regnet_x_3_2gf_20250818_142005/recovery_checkpoint.json

🔄 [31/60] Running with alpha=0.6, num_clusters=32, pca_dim=25
[Alpha=0.60] Top-1 Accuracy: 0.11%
[Alpha=0.60] Top-5 Accuracy: 0.52%
✅ Result: Top-1: 0.11%, Top-5: 0.52%

🔄 [32/60] Running with alpha=0.6, num_clusters=32, pca_dim=50
[Alpha=0.60] Top-1 Accuracy: 0.11%
[Alpha=0.60] Top-5 Accuracy: 0.51%
✅ Result: Top-1: 0.11%, Top-5: 0.51%

🔄 [33/60] Running with alpha=0.6, num_clusters=32, pca_dim=100
[Alpha=0.60] Top-1 Accuracy: 0.09%
[Alpha=0.60] Top-5 Accuracy: 0.51%
✅ Result: Top-1: 0.09%, Top-5: 0.51%

🔄 [34/60] Running with alpha=0.6, num_clusters=64, pca_dim=25
[Alpha=0.60] Top-1 Accuracy: 0.11%
[Alpha=0.60] Top-5 Accuracy: 0.51%
✅ Result: Top-1: 0.11%, Top-5: 0.51%

🔄 [35/60] Running with alpha=0.6, num_clusters=64, pca_dim=50
[Alpha=0.60] Top-1 Accuracy: 0.11%
[Alpha=0.60] Top-5 Accuracy: 0.50%
✅ Result: Top-1: 0.11%, Top-5: 0.50%
💾 Saving intermediate results... (35 total combinations)
Results saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_results_20250818_161515.csv
💾 Recovery checkpoint saved: adaround_fixed_regnet_x_3_2gf_20250818_142005/recovery_checkpoint.json

🔄 [36/60] Running with alpha=0.6, num_clusters=64, pca_dim=100
[Alpha=0.60] Top-1 Accuracy: 0.10%
[Alpha=0.60] Top-5 Accuracy: 0.54%
✅ Result: Top-1: 0.10%, Top-5: 0.54%

🔄 [37/60] Running with alpha=0.8, num_clusters=8, pca_dim=25
[Alpha=0.80] Top-1 Accuracy: 0.12%
[Alpha=0.80] Top-5 Accuracy: 0.52%
✅ Result: Top-1: 0.12%, Top-5: 0.52%

🔄 [38/60] Running with alpha=0.8, num_clusters=8, pca_dim=50
[Alpha=0.80] Top-1 Accuracy: 0.11%
[Alpha=0.80] Top-5 Accuracy: 0.53%
✅ Result: Top-1: 0.11%, Top-5: 0.53%

🔄 [39/60] Running with alpha=0.8, num_clusters=8, pca_dim=100
[Alpha=0.80] Top-1 Accuracy: 0.12%
[Alpha=0.80] Top-5 Accuracy: 0.52%
✅ Result: Top-1: 0.12%, Top-5: 0.52%

🔄 [40/60] Running with alpha=0.8, num_clusters=16, pca_dim=25
[Alpha=0.80] Top-1 Accuracy: 0.11%
[Alpha=0.80] Top-5 Accuracy: 0.54%
✅ Result: Top-1: 0.11%, Top-5: 0.54%
💾 Saving intermediate results... (40 total combinations)
Results saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_results_20250818_162008.csv
💾 Recovery checkpoint saved: adaround_fixed_regnet_x_3_2gf_20250818_142005/recovery_checkpoint.json

🔄 [41/60] Running with alpha=0.8, num_clusters=16, pca_dim=50
[Alpha=0.80] Top-1 Accuracy: 0.12%
[Alpha=0.80] Top-5 Accuracy: 0.56%
✅ Result: Top-1: 0.12%, Top-5: 0.56%

🔄 [42/60] Running with alpha=0.8, num_clusters=16, pca_dim=100
[Alpha=0.80] Top-1 Accuracy: 0.12%
[Alpha=0.80] Top-5 Accuracy: 0.57%
✅ Result: Top-1: 0.12%, Top-5: 0.57%

🔄 [43/60] Running with alpha=0.8, num_clusters=32, pca_dim=25
[Alpha=0.80] Top-1 Accuracy: 0.12%
[Alpha=0.80] Top-5 Accuracy: 0.53%
✅ Result: Top-1: 0.12%, Top-5: 0.53%

🔄 [44/60] Running with alpha=0.8, num_clusters=32, pca_dim=50
[Alpha=0.80] Top-1 Accuracy: 0.13%
[Alpha=0.80] Top-5 Accuracy: 0.59%
✅ Result: Top-1: 0.13%, Top-5: 0.59%

🔄 [45/60] Running with alpha=0.8, num_clusters=32, pca_dim=100
[Alpha=0.80] Top-1 Accuracy: 0.11%
[Alpha=0.80] Top-5 Accuracy: 0.53%
✅ Result: Top-1: 0.11%, Top-5: 0.53%
💾 Saving intermediate results... (45 total combinations)
Results saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_results_20250818_162502.csv
💾 Recovery checkpoint saved: adaround_fixed_regnet_x_3_2gf_20250818_142005/recovery_checkpoint.json

🔄 [46/60] Running with alpha=0.8, num_clusters=64, pca_dim=25
[Alpha=0.80] Top-1 Accuracy: 0.11%
[Alpha=0.80] Top-5 Accuracy: 0.58%
✅ Result: Top-1: 0.11%, Top-5: 0.58%

🔄 [47/60] Running with alpha=0.8, num_clusters=64, pca_dim=50
[Alpha=0.80] Top-1 Accuracy: 0.09%
[Alpha=0.80] Top-5 Accuracy: 0.55%
✅ Result: Top-1: 0.09%, Top-5: 0.55%

🔄 [48/60] Running with alpha=0.8, num_clusters=64, pca_dim=100
[Alpha=0.80] Top-1 Accuracy: 0.10%
[Alpha=0.80] Top-5 Accuracy: 0.52%
✅ Result: Top-1: 0.10%, Top-5: 0.52%

🔄 [49/60] Running with alpha=1.0, num_clusters=8, pca_dim=25
[Alpha=1.00] Top-1 Accuracy: 0.08%
[Alpha=1.00] Top-5 Accuracy: 0.58%
✅ Result: Top-1: 0.08%, Top-5: 0.58%

🔄 [50/60] Running with alpha=1.0, num_clusters=8, pca_dim=50
[Alpha=1.00] Top-1 Accuracy: 0.12%
[Alpha=1.00] Top-5 Accuracy: 0.57%
✅ Result: Top-1: 0.12%, Top-5: 0.57%
💾 Saving intermediate results... (50 total combinations)
Results saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_results_20250818_163223.csv
💾 Recovery checkpoint saved: adaround_fixed_regnet_x_3_2gf_20250818_142005/recovery_checkpoint.json

🔄 [51/60] Running with alpha=1.0, num_clusters=8, pca_dim=100
[Alpha=1.00] Top-1 Accuracy: 0.09%
[Alpha=1.00] Top-5 Accuracy: 0.58%
✅ Result: Top-1: 0.09%, Top-5: 0.58%

🔄 [52/60] Running with alpha=1.0, num_clusters=16, pca_dim=25
[Alpha=1.00] Top-1 Accuracy: 0.15%
[Alpha=1.00] Top-5 Accuracy: 0.61%
✅ Result: Top-1: 0.15%, Top-5: 0.61%

🔄 [53/60] Running with alpha=1.0, num_clusters=16, pca_dim=50
[Alpha=1.00] Top-1 Accuracy: 0.11%
[Alpha=1.00] Top-5 Accuracy: 0.60%
✅ Result: Top-1: 0.11%, Top-5: 0.60%

🔄 [54/60] Running with alpha=1.0, num_clusters=16, pca_dim=100
[Alpha=1.00] Top-1 Accuracy: 0.13%
[Alpha=1.00] Top-5 Accuracy: 0.56%
✅ Result: Top-1: 0.13%, Top-5: 0.56%

🔄 [55/60] Running with alpha=1.0, num_clusters=32, pca_dim=25
[Alpha=1.00] Top-1 Accuracy: 0.09%
[Alpha=1.00] Top-5 Accuracy: 0.49%
✅ Result: Top-1: 0.09%, Top-5: 0.49%
💾 Saving intermediate results... (55 total combinations)
Results saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_results_20250818_163717.csv
💾 Recovery checkpoint saved: adaround_fixed_regnet_x_3_2gf_20250818_142005/recovery_checkpoint.json

🔄 [56/60] Running with alpha=1.0, num_clusters=32, pca_dim=50
[Alpha=1.00] Top-1 Accuracy: 0.12%
[Alpha=1.00] Top-5 Accuracy: 0.56%
✅ Result: Top-1: 0.12%, Top-5: 0.56%

🔄 [57/60] Running with alpha=1.0, num_clusters=32, pca_dim=100
[Alpha=1.00] Top-1 Accuracy: 0.09%
[Alpha=1.00] Top-5 Accuracy: 0.53%
✅ Result: Top-1: 0.09%, Top-5: 0.53%

🔄 [58/60] Running with alpha=1.0, num_clusters=64, pca_dim=25
[Alpha=1.00] Top-1 Accuracy: 0.11%
[Alpha=1.00] Top-5 Accuracy: 0.59%
✅ Result: Top-1: 0.11%, Top-5: 0.59%

🔄 [59/60] Running with alpha=1.0, num_clusters=64, pca_dim=50
[Alpha=1.00] Top-1 Accuracy: 0.10%
[Alpha=1.00] Top-5 Accuracy: 0.53%
✅ Result: Top-1: 0.10%, Top-5: 0.53%

🔄 [60/60] Running with alpha=1.0, num_clusters=64, pca_dim=100
2025-08-18 16:42:10,171 | INFO | ✔ END: extract model logits (elapsed 3675.05s)
[Alpha=1.00] Top-1 Accuracy: 0.11%
[Alpha=1.00] Top-5 Accuracy: 0.52%
✅ Result: Top-1: 0.11%, Top-5: 0.52%
💾 Saving intermediate results... (60 total combinations)
Results saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_results_20250818_164210.csv
💾 Recovery checkpoint saved: adaround_fixed_regnet_x_3_2gf_20250818_142005/recovery_checkpoint.json

================================================================================
SUMMARY OF ALL RESULTS
================================================================================
Alpha    Clusters   PCA_dim    Top-1      Top-5     
--------------------------------------------------
0.20     8          25         0.11       0.50      
0.20     8          50         0.11       0.50      
0.20     8          100        0.11       0.50      
0.20     16         25         0.11       0.51      
0.20     16         50         0.11       0.51      
0.20     16         100        0.11       0.50      
0.20     32         25         0.11       0.50      
0.20     32         50         0.11       0.50      
0.20     32         100        0.11       0.51      
0.20     64         25         0.11       0.49      
0.20     64         50         0.11       0.50      
0.20     64         100        0.11       0.50      
0.40     8          25         0.11       0.50      
0.40     8          50         0.11       0.51      
0.40     8          100        0.11       0.51      
0.40     16         25         0.11       0.51      
0.40     16         50         0.11       0.51      
0.40     16         100        0.11       0.50      
0.40     32         25         0.11       0.50      
0.40     32         50         0.11       0.50      
0.40     32         100        0.10       0.51      
0.40     64         25         0.10       0.48      
0.40     64         50         0.11       0.50      
0.40     64         100        0.10       0.51      
0.60     8          25         0.12       0.50      
0.60     8          50         0.11       0.49      
0.60     8          100        0.11       0.50      
0.60     16         25         0.11       0.52      
0.60     16         50         0.11       0.51      
0.60     16         100        0.11       0.52      
0.60     32         25         0.11       0.52      
0.60     32         50         0.11       0.51      
0.60     32         100        0.09       0.51      
0.60     64         25         0.11       0.51      
0.60     64         50         0.11       0.50      
0.60     64         100        0.10       0.54      
0.80     8          25         0.12       0.52      
0.80     8          50         0.11       0.53      
0.80     8          100        0.12       0.52      
0.80     16         25         0.11       0.54      
0.80     16         50         0.12       0.56      
0.80     16         100        0.12       0.57      
0.80     32         25         0.12       0.53      
0.80     32         50         0.13       0.59      
0.80     32         100        0.11       0.53      
0.80     64         25         0.11       0.58      
0.80     64         50         0.09       0.55      
0.80     64         100        0.10       0.52      
1.00     8          25         0.08       0.58      
1.00     8          50         0.12       0.57      
1.00     8          100        0.09       0.58      
1.00     16         25         0.15       0.61      
1.00     16         50         0.11       0.60      
1.00     16         100        0.13       0.56      
1.00     32         25         0.09       0.49      
1.00     32         50         0.12       0.56      
1.00     32         100        0.09       0.53      
1.00     64         25         0.11       0.59      
1.00     64         50         0.10       0.53      
1.00     64         100        0.11       0.52      

BEST RESULT:
  Alpha: 1.0
  Clusters: 16
  PCA_dim: 25
  Top-1 Accuracy: 0.15%
  Top-5 Accuracy: 0.61%

ACCURACY COMPARISON:
  FP32 Model: 81.18%
  Baseline PTQ: 0.11%
  Best Clustering: 0.15%
  PTQ Degradation: 81.07%
  Clustering Recovery: 0.04%
  Final Gap to FP32: 81.03%
Results saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_results_20250818_164210.csv
Summary saved to: adaround_fixed_regnet_x_3_2gf_20250818_142005/ptq_summary_20250818_164210.csv
✅ Experiment completed successfully!
Results saved in: adaround_fixed_regnet_x_3_2gf_20250818_142005
------------------------------------------
🎉 Experiment finished!
Results directory: adaround_fixed_regnet_x_3_2gf_20250818_142005
