ðŸš€ Starting PTQ Experiment: adaround + lsq + mobilenet_v2
==========================================
Parameters:
  Model: mobilenet_v2
  Advanced Mode: adaround
  Quant Model: lsq
  Weight Bits: 8
  Activation Bits: 8
  Alpha: 0.5
  Clusters: 16
  PCA dim: 50
  Batch Size: 64
  Calib Batches: 32
  Logits Batches: 10
==========================================
ðŸ”„ Running experiment...
Time: Sun Aug 17 01:41:16 PM CEST 2025
------------------------------------------
2025-08-17 13:41:19,113 | INFO | Environment: {'torch': '2.8.0+cu128', 'torchvision': '0.23.0+cu128', 'cuda_available': True, 'device_name': 'NVIDIA L40S', 'capability': '8.9', 'cudnn_version': 91002}
2025-08-17 13:41:19,113 | INFO | â–¶ START: load fp32 model (torchvision weights API)
2025-08-17 13:41:19,287 | INFO | Model: mobilenet_v2 | Weights: MobileNet_V2_Weights.IMAGENET1K_V2 | Params: 3.50M | Ref acc@1=None
2025-08-17 13:41:19,287 | INFO | âœ” END: load fp32 model (torchvision weights API) (elapsed 0.17s)
2025-08-17 13:41:19,287 | INFO | â–¶ START: build & check loaders
2025-08-17 13:41:19,293 | INFO | Val structure looks OK (1000 synset folders).
2025-08-17 13:41:19,300 | INFO | Train structure looks OK (1000 synset folders).
2025-08-17 13:41:42,386 | INFO | ImageNet train: 1281167 images, 1000 classes
2025-08-17 13:41:44,693 | INFO | ImageNet val: 50000 images, 1000 classes
2025-08-17 13:41:44,694 | INFO | Val dataset size: 50000 | batch_size=64 | calib_batches=32
2025-08-17 13:41:47,357 | INFO | [SANITY] Batch[0] stats: mean=-0.2055, std=1.1229, min=-2.118, max=2.640
2025-08-17 13:41:47,357 | INFO | âœ” END: build & check loaders (elapsed 28.07s)
2025-08-17 13:41:47,364 | INFO | â–¶ START: prepare_by_platform(Academic)
2025-08-17 13:41:47,365 | INFO | [Academic extra_config] {'extra_qconfig_dict': {'w_observer': 'MinMaxObserver', 'a_observer': 'EMAMinMaxObserver', 'w_fakequantize': 'AdaRoundFakeQuantize', 'a_fakequantize': 'FixedFakeQuantize', 'w_qscheme': {'bit': 8, 'symmetry': True, 'per_channel': True, 'pot_scale': False}, 'a_qscheme': {'bit': 8, 'symmetry': False, 'per_channel': False, 'pot_scale': False}}}
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MinMaxObserver Params: Symmetric: True / Bitwidth: 8 / Per channel: True / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: FixedFakeQuantize Params: {}
    Oberver:      EMAMinMaxObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer classifier.1 to 8 bit.
[MQBENCH] INFO: Set layer features.0.0 to 8 bit.
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_1_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_1_conv_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_2_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_2_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_2_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_3_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_3_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_4_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_4_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_4_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_5_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_5_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_6_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_6_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_7_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_7_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_7_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_8_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_8_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_9_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_9_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_4_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_10_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_10_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_5_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_11_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_11_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_11_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_12_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_12_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_6_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_13_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_13_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_7_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_14_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_14_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_14_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_15_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_15_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_8_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_16_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_16_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_9_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_17_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_17_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_17_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Set classifier_0 post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant classifier_0_post_act_fake_quantizer
2025-08-17 13:41:47,587 | INFO | Modules (total): 213 -> 425
2025-08-17 13:41:47,587 | INFO | 'Quantish' modules detected after prepare: 212
2025-08-17 13:41:47,587 | INFO | â–¶ START: calibration (enable_calibration + forward)
[MQBENCH] INFO: Enable observer and Disable quantize.
2025-08-17 13:41:49,061 | INFO | [CALIB] step=1/32 seen=64 (43.5 img/s)
2025-08-17 13:41:49,514 | INFO | [CALIB] step=10/32 seen=640 (332.6 img/s)
2025-08-17 13:41:50,337 | INFO | [CALIB] step=20/32 seen=1280 (465.9 img/s)
2025-08-17 13:41:51,280 | INFO | [CALIB] step=30/32 seen=1920 (520.3 img/s)
2025-08-17 13:41:52,445 | INFO | [CALIB] total images seen: 2048
2025-08-17 13:41:52,446 | INFO | âœ” END: calibration (enable_calibration + forward) (elapsed 4.86s)
2025-08-17 13:41:52,446 | INFO | â–¶ START: advanced PTQ reconstruction
2025-08-17 13:41:54,724 | INFO | [ADV] cfg={'pattern': 'block', 'scale_lr': 4e-05, 'warm_up': 0.2, 'weight': 0.01, 'max_count': 20000, 'b_range': [20, 2], 'keep_gpu': True, 'round_mode': 'learned_hard_sigmoid', 'prob': 1.0}
2025-08-17 13:41:54,725 | INFO | [ADV] stacked tensors: 32 | total calib images: 2048
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare block reconstruction for features_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [x_post_act_fake_quantizer, features_0_0, features_0_1, features_0_2, features_0_2_post_act_fake_quantizer, features_1_conv_0_0, features_1_conv_0_1, features_1_conv_0_2, features_1_conv_0_2_post_act_fake_quantizer, features_1_conv_1, features_1_conv_2, features_1_conv_2_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, x_post_act_fake_quantizer):
    features_0_0 = getattr(getattr(self.features, "0"), "0")(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None
    features_0_1 = getattr(getattr(self.features, "0"), "1")(features_0_0);  features_0_0 = None
    features_0_2 = getattr(getattr(self.features, "0"), "2")(features_0_1);  features_0_1 = None
    features_0_2_post_act_fake_quantizer = self.features_0_2_post_act_fake_quantizer(features_0_2);  features_0_2 = None
    features_1_conv_0_0 = getattr(getattr(getattr(self.features, "1").conv, "0"), "0")(features_0_2_post_act_fake_quantizer);  features_0_2_post_act_fake_quantizer = None
    features_1_conv_0_1 = getattr(getattr(getattr(self.features, "1").conv, "0"), "1")(features_1_conv_0_0);  features_1_conv_0_0 = None
    features_1_conv_0_2 = getattr(getattr(getattr(self.features, "1").conv, "0"), "2")(features_1_conv_0_1);  features_1_conv_0_1 = None
    features_1_conv_0_2_post_act_fake_quantizer = self.features_1_conv_0_2_post_act_fake_quantizer(features_1_conv_0_2);  features_1_conv_0_2 = None
    features_1_conv_1 = getattr(getattr(self.features, "1").conv, "1")(features_1_conv_0_2_post_act_fake_quantizer);  features_1_conv_0_2_post_act_fake_quantizer = None
    features_1_conv_2 = getattr(getattr(self.features, "1").conv, "2")(features_1_conv_1);  features_1_conv_1 = None
    features_1_conv_2_post_act_fake_quantizer = self.features_1_conv_2_post_act_fake_quantizer(features_1_conv_2);  features_1_conv_2 = None
    return features_1_conv_2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_1_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_1_conv_2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
2025-08-17 13:42:02,282 | WARNING | /home/alz07xz/project/kmeans_results/MQBench/mqbench/advanced_ptq.py:220: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)
  float(total_loss), float(rec_loss), float(round_loss), b, self.count))

[MQBENCH] INFO: Total loss:	0.931 (rec:0.931, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.949 (rec:0.949, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.928 (rec:0.928, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.928 (rec:0.928, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.935 (rec:0.935, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.980 (rec:0.980, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.948 (rec:0.948, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	15.435 (rec:0.966, round:14.469)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	10.652 (rec:0.975, round:9.677)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	10.027 (rec:0.975, round:9.053)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	9.650 (rec:0.929, round:8.721)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	9.232 (rec:0.950, round:8.282)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	8.966 (rec:0.930, round:8.036)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	8.646 (rec:0.930, round:7.717)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	8.413 (rec:0.966, round:7.446)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	8.070 (rec:0.940, round:7.130)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	7.874 (rec:0.961, round:6.914)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	7.655 (rec:0.964, round:6.691)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	7.404 (rec:0.937, round:6.466)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	7.159 (rec:0.951, round:6.208)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	6.901 (rec:0.953, round:5.947)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	6.643 (rec:0.947, round:5.696)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	6.393 (rec:0.976, round:5.417)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	6.146 (rec:0.964, round:5.183)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	5.755 (rec:0.950, round:4.805)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	5.540 (rec:0.962, round:4.578)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	5.236 (rec:0.937, round:4.299)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	4.960 (rec:0.905, round:4.054)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	4.626 (rec:0.949, round:3.677)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	4.323 (rec:0.958, round:3.365)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	3.905 (rec:0.944, round:2.962)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	3.597 (rec:0.955, round:2.642)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	3.078 (rec:0.917, round:2.161)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	2.792 (rec:0.997, round:1.794)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	2.446 (rec:0.998, round:1.448)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	2.176 (rec:0.995, round:1.180)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1.914 (rec:1.010, round:0.904)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	1.680 (rec:1.013, round:0.667)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	1.342 (rec:1.023, round:0.319)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	1.138 (rec:1.019, round:0.119)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_2_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_1_conv_2_post_act_fake_quantizer, features_2_conv_0_0, features_2_conv_0_1, features_2_conv_0_2, features_2_conv_0_2_post_act_fake_quantizer, features_2_conv_1_0, features_2_conv_1_1, features_2_conv_1_2, features_2_conv_1_2_post_act_fake_quantizer, features_2_conv_2, features_2_conv_3, features_2_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_1_conv_2_post_act_fake_quantizer):
    features_2_conv_0_0 = getattr(getattr(getattr(self.features, "2").conv, "0"), "0")(features_1_conv_2_post_act_fake_quantizer);  features_1_conv_2_post_act_fake_quantizer = None
    features_2_conv_0_1 = getattr(getattr(getattr(self.features, "2").conv, "0"), "1")(features_2_conv_0_0);  features_2_conv_0_0 = None
    features_2_conv_0_2 = getattr(getattr(getattr(self.features, "2").conv, "0"), "2")(features_2_conv_0_1);  features_2_conv_0_1 = None
    features_2_conv_0_2_post_act_fake_quantizer = self.features_2_conv_0_2_post_act_fake_quantizer(features_2_conv_0_2);  features_2_conv_0_2 = None
    features_2_conv_1_0 = getattr(getattr(getattr(self.features, "2").conv, "1"), "0")(features_2_conv_0_2_post_act_fake_quantizer);  features_2_conv_0_2_post_act_fake_quantizer = None
    features_2_conv_1_1 = getattr(getattr(getattr(self.features, "2").conv, "1"), "1")(features_2_conv_1_0);  features_2_conv_1_0 = None
    features_2_conv_1_2 = getattr(getattr(getattr(self.features, "2").conv, "1"), "2")(features_2_conv_1_1);  features_2_conv_1_1 = None
    features_2_conv_1_2_post_act_fake_quantizer = self.features_2_conv_1_2_post_act_fake_quantizer(features_2_conv_1_2);  features_2_conv_1_2 = None
    features_2_conv_2 = getattr(getattr(self.features, "2").conv, "2")(features_2_conv_1_2_post_act_fake_quantizer);  features_2_conv_1_2_post_act_fake_quantizer = None
    features_2_conv_3 = getattr(getattr(self.features, "2").conv, "3")(features_2_conv_2);  features_2_conv_2 = None
    features_2_conv_3_post_act_fake_quantizer = self.features_2_conv_3_post_act_fake_quantizer(features_2_conv_3);  features_2_conv_3 = None
    return features_2_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_2_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_2_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_2_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	2.389 (rec:2.389, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	2.053 (rec:2.053, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	2.357 (rec:2.357, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	2.178 (rec:2.178, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	2.369 (rec:2.369, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	2.302 (rec:2.302, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	1.980 (rec:1.980, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	41.474 (rec:2.024, round:39.450)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	28.012 (rec:2.256, round:25.756)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	26.271 (rec:2.294, round:23.977)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	25.269 (rec:2.537, round:22.733)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	23.632 (rec:2.029, round:21.603)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	22.775 (rec:2.020, round:20.755)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	22.509 (rec:2.551, round:19.959)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	21.260 (rec:2.048, round:19.211)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	20.838 (rec:2.361, round:18.477)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	19.726 (rec:2.049, round:17.678)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	19.021 (rec:1.966, round:17.055)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	18.274 (rec:2.033, round:16.242)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	17.482 (rec:2.033, round:15.449)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	17.312 (rec:2.536, round:14.776)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	16.087 (rec:2.111, round:13.976)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	15.313 (rec:2.029, round:13.284)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	14.733 (rec:2.091, round:12.642)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	14.465 (rec:2.538, round:11.928)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	13.390 (rec:2.210, round:11.180)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	12.410 (rec:2.032, round:10.378)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	11.619 (rec:2.283, round:9.337)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	10.485 (rec:2.176, round:8.309)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	9.910 (rec:2.368, round:7.542)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	8.566 (rec:1.912, round:6.654)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	8.104 (rec:2.373, round:5.731)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	7.190 (rec:2.303, round:4.886)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	6.118 (rec:2.307, round:3.811)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	4.780 (rec:1.996, round:2.784)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	4.354 (rec:2.282, round:2.072)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	4.014 (rec:2.383, round:1.631)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	3.373 (rec:1.987, round:1.386)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	2.864 (rec:2.206, round:0.658)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	2.302 (rec:2.113, round:0.189)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_3_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_2_conv_3_post_act_fake_quantizer, features_3_conv_0_0, features_3_conv_0_1, features_3_conv_0_2, features_3_conv_0_2_post_act_fake_quantizer, features_3_conv_1_0, features_3_conv_1_1, features_3_conv_1_2, features_3_conv_1_2_post_act_fake_quantizer, features_3_conv_2, features_3_conv_3, add, add_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_2_conv_3_post_act_fake_quantizer):
    features_3_conv_0_0 = getattr(getattr(getattr(self.features, "3").conv, "0"), "0")(features_2_conv_3_post_act_fake_quantizer)
    features_3_conv_0_1 = getattr(getattr(getattr(self.features, "3").conv, "0"), "1")(features_3_conv_0_0);  features_3_conv_0_0 = None
    features_3_conv_0_2 = getattr(getattr(getattr(self.features, "3").conv, "0"), "2")(features_3_conv_0_1);  features_3_conv_0_1 = None
    features_3_conv_0_2_post_act_fake_quantizer = self.features_3_conv_0_2_post_act_fake_quantizer(features_3_conv_0_2);  features_3_conv_0_2 = None
    features_3_conv_1_0 = getattr(getattr(getattr(self.features, "3").conv, "1"), "0")(features_3_conv_0_2_post_act_fake_quantizer);  features_3_conv_0_2_post_act_fake_quantizer = None
    features_3_conv_1_1 = getattr(getattr(getattr(self.features, "3").conv, "1"), "1")(features_3_conv_1_0);  features_3_conv_1_0 = None
    features_3_conv_1_2 = getattr(getattr(getattr(self.features, "3").conv, "1"), "2")(features_3_conv_1_1);  features_3_conv_1_1 = None
    features_3_conv_1_2_post_act_fake_quantizer = self.features_3_conv_1_2_post_act_fake_quantizer(features_3_conv_1_2);  features_3_conv_1_2 = None
    features_3_conv_2 = getattr(getattr(self.features, "3").conv, "2")(features_3_conv_1_2_post_act_fake_quantizer);  features_3_conv_1_2_post_act_fake_quantizer = None
    features_3_conv_3 = getattr(getattr(self.features, "3").conv, "3")(features_3_conv_2);  features_3_conv_2 = None
    add = features_2_conv_3_post_act_fake_quantizer + features_3_conv_3;  features_2_conv_3_post_act_fake_quantizer = features_3_conv_3 = None
    add_post_act_fake_quantizer = self.add_post_act_fake_quantizer(add);  add = None
    return add_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_3_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_3_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	11.754 (rec:11.754, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	11.109 (rec:11.109, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	10.996 (rec:10.996, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	9.309 (rec:9.309, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	11.635 (rec:11.635, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	8.042 (rec:8.042, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	12.504 (rec:12.504, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	79.705 (rec:12.249, round:67.456)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	51.568 (rec:9.548, round:42.020)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	49.089 (rec:10.050, round:39.039)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	48.028 (rec:10.927, round:37.101)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	44.320 (rec:8.687, round:35.634)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	43.307 (rec:9.329, round:33.979)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	41.096 (rec:8.265, round:32.831)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	42.343 (rec:10.925, round:31.419)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	41.177 (rec:10.997, round:30.181)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	41.081 (rec:12.168, round:28.913)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	39.370 (rec:11.600, round:27.771)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	35.279 (rec:8.522, round:26.757)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	37.192 (rec:11.599, round:25.593)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	36.863 (rec:12.228, round:24.635)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	32.348 (rec:8.685, round:23.663)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	30.893 (rec:8.270, round:22.623)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	29.982 (rec:8.626, round:21.356)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	32.367 (rec:12.479, round:19.889)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	30.766 (rec:12.174, round:18.592)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	28.780 (rec:11.602, round:17.178)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	25.218 (rec:9.512, round:15.706)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	24.192 (rec:9.961, round:14.231)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	22.085 (rec:9.282, round:12.803)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	23.510 (rec:12.173, round:11.337)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	19.409 (rec:9.557, round:9.853)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	17.748 (rec:9.289, round:8.459)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	16.075 (rec:9.352, round:6.723)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	14.335 (rec:9.356, round:4.979)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	12.182 (rec:8.554, round:3.628)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	12.309 (rec:9.577, round:2.732)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	14.408 (rec:12.259, round:2.149)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	10.968 (rec:9.993, round:0.975)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	11.246 (rec:10.973, round:0.273)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_4_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_post_act_fake_quantizer, features_4_conv_0_0, features_4_conv_0_1, features_4_conv_0_2, features_4_conv_0_2_post_act_fake_quantizer, features_4_conv_1_0, features_4_conv_1_1, features_4_conv_1_2, features_4_conv_1_2_post_act_fake_quantizer, features_4_conv_2, features_4_conv_3, features_4_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_post_act_fake_quantizer):
    features_4_conv_0_0 = getattr(getattr(getattr(self.features, "4").conv, "0"), "0")(add_post_act_fake_quantizer);  add_post_act_fake_quantizer = None
    features_4_conv_0_1 = getattr(getattr(getattr(self.features, "4").conv, "0"), "1")(features_4_conv_0_0);  features_4_conv_0_0 = None
    features_4_conv_0_2 = getattr(getattr(getattr(self.features, "4").conv, "0"), "2")(features_4_conv_0_1);  features_4_conv_0_1 = None
    features_4_conv_0_2_post_act_fake_quantizer = self.features_4_conv_0_2_post_act_fake_quantizer(features_4_conv_0_2);  features_4_conv_0_2 = None
    features_4_conv_1_0 = getattr(getattr(getattr(self.features, "4").conv, "1"), "0")(features_4_conv_0_2_post_act_fake_quantizer);  features_4_conv_0_2_post_act_fake_quantizer = None
    features_4_conv_1_1 = getattr(getattr(getattr(self.features, "4").conv, "1"), "1")(features_4_conv_1_0);  features_4_conv_1_0 = None
    features_4_conv_1_2 = getattr(getattr(getattr(self.features, "4").conv, "1"), "2")(features_4_conv_1_1);  features_4_conv_1_1 = None
    features_4_conv_1_2_post_act_fake_quantizer = self.features_4_conv_1_2_post_act_fake_quantizer(features_4_conv_1_2);  features_4_conv_1_2 = None
    features_4_conv_2 = getattr(getattr(self.features, "4").conv, "2")(features_4_conv_1_2_post_act_fake_quantizer);  features_4_conv_1_2_post_act_fake_quantizer = None
    features_4_conv_3 = getattr(getattr(self.features, "4").conv, "3")(features_4_conv_2);  features_4_conv_2 = None
    features_4_conv_3_post_act_fake_quantizer = self.features_4_conv_3_post_act_fake_quantizer(features_4_conv_3);  features_4_conv_3 = None
    return features_4_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_4_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_4_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_4_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	4.729 (rec:4.729, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	4.408 (rec:4.408, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	3.581 (rec:3.581, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	5.640 (rec:5.640, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	4.287 (rec:4.287, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	6.528 (rec:6.528, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	5.628 (rec:5.628, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	85.907 (rec:5.100, round:80.807)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	54.713 (rec:3.264, round:51.449)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	53.019 (rec:5.094, round:47.925)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	50.872 (rec:5.214, round:45.658)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	48.113 (rec:4.642, round:43.471)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	47.347 (rec:5.619, round:41.728)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	46.351 (rec:6.206, round:40.145)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	45.151 (rec:6.728, round:38.423)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	41.062 (rec:4.196, round:36.866)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	40.651 (rec:5.207, round:35.444)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	38.248 (rec:4.197, round:34.051)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	36.701 (rec:4.196, round:32.505)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	35.328 (rec:4.188, round:31.139)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	35.086 (rec:5.286, round:29.800)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	32.670 (rec:4.186, round:28.485)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	30.943 (rec:3.736, round:27.207)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	30.152 (rec:4.342, round:25.810)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	27.916 (rec:3.502, round:24.414)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	27.135 (rec:4.201, round:22.934)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	27.986 (rec:6.495, round:21.490)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	24.120 (rec:4.135, round:19.985)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	22.679 (rec:4.387, round:18.292)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	21.976 (rec:5.293, round:16.683)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	19.579 (rec:4.653, round:14.926)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	19.746 (rec:6.501, round:13.245)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	15.599 (rec:4.197, round:11.403)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	13.935 (rec:4.397, round:9.538)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	12.234 (rec:4.662, round:7.572)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	9.438 (rec:3.930, round:5.508)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	9.116 (rec:5.309, round:3.807)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	6.458 (rec:3.765, round:2.693)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	7.834 (rec:6.749, round:1.086)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	4.666 (rec:4.428, round:0.238)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_5_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_4_conv_3_post_act_fake_quantizer, features_5_conv_0_0, features_5_conv_0_1, features_5_conv_0_2, features_5_conv_0_2_post_act_fake_quantizer, features_5_conv_1_0, features_5_conv_1_1, features_5_conv_1_2, features_5_conv_1_2_post_act_fake_quantizer, features_5_conv_2, features_5_conv_3, add_1, add_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_4_conv_3_post_act_fake_quantizer):
    features_5_conv_0_0 = getattr(getattr(getattr(self.features, "5").conv, "0"), "0")(features_4_conv_3_post_act_fake_quantizer)
    features_5_conv_0_1 = getattr(getattr(getattr(self.features, "5").conv, "0"), "1")(features_5_conv_0_0);  features_5_conv_0_0 = None
    features_5_conv_0_2 = getattr(getattr(getattr(self.features, "5").conv, "0"), "2")(features_5_conv_0_1);  features_5_conv_0_1 = None
    features_5_conv_0_2_post_act_fake_quantizer = self.features_5_conv_0_2_post_act_fake_quantizer(features_5_conv_0_2);  features_5_conv_0_2 = None
    features_5_conv_1_0 = getattr(getattr(getattr(self.features, "5").conv, "1"), "0")(features_5_conv_0_2_post_act_fake_quantizer);  features_5_conv_0_2_post_act_fake_quantizer = None
    features_5_conv_1_1 = getattr(getattr(getattr(self.features, "5").conv, "1"), "1")(features_5_conv_1_0);  features_5_conv_1_0 = None
    features_5_conv_1_2 = getattr(getattr(getattr(self.features, "5").conv, "1"), "2")(features_5_conv_1_1);  features_5_conv_1_1 = None
    features_5_conv_1_2_post_act_fake_quantizer = self.features_5_conv_1_2_post_act_fake_quantizer(features_5_conv_1_2);  features_5_conv_1_2 = None
    features_5_conv_2 = getattr(getattr(self.features, "5").conv, "2")(features_5_conv_1_2_post_act_fake_quantizer);  features_5_conv_1_2_post_act_fake_quantizer = None
    features_5_conv_3 = getattr(getattr(self.features, "5").conv, "3")(features_5_conv_2);  features_5_conv_2 = None
    add_1 = features_4_conv_3_post_act_fake_quantizer + features_5_conv_3;  features_4_conv_3_post_act_fake_quantizer = features_5_conv_3 = None
    add_1_post_act_fake_quantizer = self.add_1_post_act_fake_quantizer(add_1);  add_1 = None
    return add_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_5_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_5_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	8.779 (rec:8.779, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	10.317 (rec:10.317, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	9.639 (rec:9.639, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	10.929 (rec:10.929, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	8.933 (rec:8.933, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	7.305 (rec:7.305, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	8.926 (rec:8.926, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	124.902 (rec:7.158, round:117.744)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	77.041 (rec:6.822, round:70.219)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	72.070 (rec:6.822, round:65.248)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	73.533 (rec:11.994, round:61.539)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	65.713 (rec:7.299, round:58.413)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	65.719 (rec:9.922, round:55.797)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	61.974 (rec:8.444, round:53.530)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	63.937 (rec:12.372, round:51.565)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	58.914 (rec:9.729, round:49.185)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	57.841 (rec:10.882, round:46.959)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	52.929 (rec:8.107, round:44.822)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	54.899 (rec:12.087, round:42.812)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	53.094 (rec:12.087, round:41.007)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	47.572 (rec:8.593, round:38.979)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	44.924 (rec:7.972, round:36.952)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	42.097 (rec:7.161, round:34.936)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	43.204 (rec:10.268, round:32.936)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	43.239 (rec:12.370, round:30.869)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	38.373 (rec:9.607, round:28.766)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	33.929 (rec:7.162, round:26.767)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	31.426 (rec:7.092, round:24.334)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	28.791 (rec:6.608, round:22.183)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	27.218 (rec:7.600, round:19.618)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	29.272 (rec:11.989, round:17.283)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	25.694 (rec:10.861, round:14.833)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	18.689 (rec:6.195, round:12.495)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	17.631 (rec:7.551, round:10.081)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	16.535 (rec:8.924, round:7.611)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	17.453 (rec:12.095, round:5.358)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	11.829 (rec:8.003, round:3.826)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	12.572 (rec:9.630, round:2.942)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	8.188 (rec:6.849, round:1.339)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	11.301 (rec:10.904, round:0.397)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_6_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_1_post_act_fake_quantizer, features_6_conv_0_0, features_6_conv_0_1, features_6_conv_0_2, features_6_conv_0_2_post_act_fake_quantizer, features_6_conv_1_0, features_6_conv_1_1, features_6_conv_1_2, features_6_conv_1_2_post_act_fake_quantizer, features_6_conv_2, features_6_conv_3, add_2, add_2_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_1_post_act_fake_quantizer):
    features_6_conv_0_0 = getattr(getattr(getattr(self.features, "6").conv, "0"), "0")(add_1_post_act_fake_quantizer)
    features_6_conv_0_1 = getattr(getattr(getattr(self.features, "6").conv, "0"), "1")(features_6_conv_0_0);  features_6_conv_0_0 = None
    features_6_conv_0_2 = getattr(getattr(getattr(self.features, "6").conv, "0"), "2")(features_6_conv_0_1);  features_6_conv_0_1 = None
    features_6_conv_0_2_post_act_fake_quantizer = self.features_6_conv_0_2_post_act_fake_quantizer(features_6_conv_0_2);  features_6_conv_0_2 = None
    features_6_conv_1_0 = getattr(getattr(getattr(self.features, "6").conv, "1"), "0")(features_6_conv_0_2_post_act_fake_quantizer);  features_6_conv_0_2_post_act_fake_quantizer = None
    features_6_conv_1_1 = getattr(getattr(getattr(self.features, "6").conv, "1"), "1")(features_6_conv_1_0);  features_6_conv_1_0 = None
    features_6_conv_1_2 = getattr(getattr(getattr(self.features, "6").conv, "1"), "2")(features_6_conv_1_1);  features_6_conv_1_1 = None
    features_6_conv_1_2_post_act_fake_quantizer = self.features_6_conv_1_2_post_act_fake_quantizer(features_6_conv_1_2);  features_6_conv_1_2 = None
    features_6_conv_2 = getattr(getattr(self.features, "6").conv, "2")(features_6_conv_1_2_post_act_fake_quantizer);  features_6_conv_1_2_post_act_fake_quantizer = None
    features_6_conv_3 = getattr(getattr(self.features, "6").conv, "3")(features_6_conv_2);  features_6_conv_2 = None
    add_2 = add_1_post_act_fake_quantizer + features_6_conv_3;  add_1_post_act_fake_quantizer = features_6_conv_3 = None
    add_2_post_act_fake_quantizer = self.add_2_post_act_fake_quantizer(add_2);  add_2 = None
    return add_2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_6_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_6_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	8.927 (rec:8.927, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	18.614 (rec:18.614, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	10.878 (rec:10.878, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	9.466 (rec:9.466, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	14.770 (rec:14.770, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	18.555 (rec:18.555, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	10.352 (rec:10.352, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	135.791 (rec:15.706, round:120.085)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	85.962 (rec:14.757, round:71.205)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	81.496 (rec:15.704, round:65.792)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	77.859 (rec:15.704, round:62.155)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	69.792 (rec:10.704, round:59.087)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	67.448 (rec:11.272, round:56.177)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	62.580 (rec:8.901, round:53.679)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	61.142 (rec:9.751, round:51.391)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	63.883 (rec:14.725, round:49.158)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	61.601 (rec:14.725, round:46.876)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	58.204 (rec:13.579, round:44.625)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	56.014 (rec:13.581, round:42.433)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	50.541 (rec:10.071, round:40.470)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	48.860 (rec:10.417, round:38.443)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	52.347 (rec:15.690, round:36.657)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	48.244 (rec:13.415, round:34.828)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	42.304 (rec:9.466, round:32.839)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	41.046 (rec:10.070, round:30.976)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	40.885 (rec:12.041, round:28.844)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	37.730 (rec:10.874, round:26.856)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	35.935 (rec:11.272, round:24.663)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	35.972 (rec:13.421, round:22.551)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	32.687 (rec:12.424, round:20.263)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	32.643 (rec:14.724, round:17.919)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	34.030 (rec:18.503, round:15.528)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	23.283 (rec:10.197, round:13.086)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	22.021 (rec:11.509, round:10.512)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	19.325 (rec:11.222, round:8.103)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	17.260 (rec:11.515, round:5.745)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	17.486 (rec:13.591, round:3.896)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	15.239 (rec:12.364, round:2.875)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	16.425 (rec:15.127, round:1.297)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	9.350 (rec:8.927, round:0.424)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_7_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_2_post_act_fake_quantizer, features_7_conv_0_0, features_7_conv_0_1, features_7_conv_0_2, features_7_conv_0_2_post_act_fake_quantizer, features_7_conv_1_0, features_7_conv_1_1, features_7_conv_1_2, features_7_conv_1_2_post_act_fake_quantizer, features_7_conv_2, features_7_conv_3, features_7_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_2_post_act_fake_quantizer):
    features_7_conv_0_0 = getattr(getattr(getattr(self.features, "7").conv, "0"), "0")(add_2_post_act_fake_quantizer);  add_2_post_act_fake_quantizer = None
    features_7_conv_0_1 = getattr(getattr(getattr(self.features, "7").conv, "0"), "1")(features_7_conv_0_0);  features_7_conv_0_0 = None
    features_7_conv_0_2 = getattr(getattr(getattr(self.features, "7").conv, "0"), "2")(features_7_conv_0_1);  features_7_conv_0_1 = None
    features_7_conv_0_2_post_act_fake_quantizer = self.features_7_conv_0_2_post_act_fake_quantizer(features_7_conv_0_2);  features_7_conv_0_2 = None
    features_7_conv_1_0 = getattr(getattr(getattr(self.features, "7").conv, "1"), "0")(features_7_conv_0_2_post_act_fake_quantizer);  features_7_conv_0_2_post_act_fake_quantizer = None
    features_7_conv_1_1 = getattr(getattr(getattr(self.features, "7").conv, "1"), "1")(features_7_conv_1_0);  features_7_conv_1_0 = None
    features_7_conv_1_2 = getattr(getattr(getattr(self.features, "7").conv, "1"), "2")(features_7_conv_1_1);  features_7_conv_1_1 = None
    features_7_conv_1_2_post_act_fake_quantizer = self.features_7_conv_1_2_post_act_fake_quantizer(features_7_conv_1_2);  features_7_conv_1_2 = None
    features_7_conv_2 = getattr(getattr(self.features, "7").conv, "2")(features_7_conv_1_2_post_act_fake_quantizer);  features_7_conv_1_2_post_act_fake_quantizer = None
    features_7_conv_3 = getattr(getattr(self.features, "7").conv, "3")(features_7_conv_2);  features_7_conv_2 = None
    features_7_conv_3_post_act_fake_quantizer = self.features_7_conv_3_post_act_fake_quantizer(features_7_conv_3);  features_7_conv_3 = None
    return features_7_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_7_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_7_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_7_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	5.785 (rec:5.785, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	7.781 (rec:7.781, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	7.781 (rec:7.781, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	7.558 (rec:7.558, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	5.366 (rec:5.366, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	7.759 (rec:7.759, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	5.573 (rec:5.573, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	189.858 (rec:6.211, round:183.647)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	127.443 (rec:7.603, round:119.839)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	119.818 (rec:7.549, round:112.269)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	114.347 (rec:7.062, round:107.285)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	109.656 (rec:6.291, round:103.365)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	105.821 (rec:5.715, round:100.106)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	101.548 (rec:4.803, round:96.746)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	99.559 (rec:5.572, round:93.987)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	97.091 (rec:6.074, round:91.018)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	93.636 (rec:5.424, round:88.212)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	90.761 (rec:5.706, round:85.055)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	88.997 (rec:7.054, round:81.943)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	84.837 (rec:6.074, round:78.763)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	84.626 (rec:8.934, round:75.692)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	79.778 (rec:7.209, round:72.569)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	74.597 (rec:5.424, round:69.172)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	74.387 (rec:8.426, round:65.961)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	69.321 (rec:7.054, round:62.267)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	64.112 (rec:5.484, round:58.628)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	62.034 (rec:7.054, round:54.980)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	56.292 (rec:5.376, round:50.916)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	51.653 (rec:4.813, round:46.841)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	48.020 (rec:5.729, round:42.292)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	43.502 (rec:5.488, round:38.014)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	38.925 (rec:5.580, round:33.345)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	34.686 (rec:6.137, round:28.549)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	28.652 (rec:5.433, round:23.219)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	23.249 (rec:5.391, round:17.858)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	19.530 (rec:6.343, round:13.186)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	14.286 (rec:5.448, round:8.838)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	12.903 (rec:7.808, round:5.095)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	7.281 (rec:5.434, round:1.847)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	6.645 (rec:6.205, round:0.440)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_8_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_7_conv_3_post_act_fake_quantizer, features_8_conv_0_0, features_8_conv_0_1, features_8_conv_0_2, features_8_conv_0_2_post_act_fake_quantizer, features_8_conv_1_0, features_8_conv_1_1, features_8_conv_1_2, features_8_conv_1_2_post_act_fake_quantizer, features_8_conv_2, features_8_conv_3, add_3, add_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_7_conv_3_post_act_fake_quantizer):
    features_8_conv_0_0 = getattr(getattr(getattr(self.features, "8").conv, "0"), "0")(features_7_conv_3_post_act_fake_quantizer)
    features_8_conv_0_1 = getattr(getattr(getattr(self.features, "8").conv, "0"), "1")(features_8_conv_0_0);  features_8_conv_0_0 = None
    features_8_conv_0_2 = getattr(getattr(getattr(self.features, "8").conv, "0"), "2")(features_8_conv_0_1);  features_8_conv_0_1 = None
    features_8_conv_0_2_post_act_fake_quantizer = self.features_8_conv_0_2_post_act_fake_quantizer(features_8_conv_0_2);  features_8_conv_0_2 = None
    features_8_conv_1_0 = getattr(getattr(getattr(self.features, "8").conv, "1"), "0")(features_8_conv_0_2_post_act_fake_quantizer);  features_8_conv_0_2_post_act_fake_quantizer = None
    features_8_conv_1_1 = getattr(getattr(getattr(self.features, "8").conv, "1"), "1")(features_8_conv_1_0);  features_8_conv_1_0 = None
    features_8_conv_1_2 = getattr(getattr(getattr(self.features, "8").conv, "1"), "2")(features_8_conv_1_1);  features_8_conv_1_1 = None
    features_8_conv_1_2_post_act_fake_quantizer = self.features_8_conv_1_2_post_act_fake_quantizer(features_8_conv_1_2);  features_8_conv_1_2 = None
    features_8_conv_2 = getattr(getattr(self.features, "8").conv, "2")(features_8_conv_1_2_post_act_fake_quantizer);  features_8_conv_1_2_post_act_fake_quantizer = None
    features_8_conv_3 = getattr(getattr(self.features, "8").conv, "3")(features_8_conv_2);  features_8_conv_2 = None
    add_3 = features_7_conv_3_post_act_fake_quantizer + features_8_conv_3;  features_7_conv_3_post_act_fake_quantizer = features_8_conv_3 = None
    add_3_post_act_fake_quantizer = self.add_3_post_act_fake_quantizer(add_3);  add_3 = None
    return add_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_8_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_8_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	8.732 (rec:8.732, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	7.907 (rec:7.907, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	9.063 (rec:9.063, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	7.864 (rec:7.864, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	10.085 (rec:10.085, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	11.009 (rec:11.009, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	8.989 (rec:8.989, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	475.523 (rec:8.155, round:467.367)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	285.307 (rec:9.279, round:276.028)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	265.327 (rec:7.815, round:257.512)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	251.656 (rec:7.546, round:244.111)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	241.618 (rec:8.153, round:233.465)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	231.473 (rec:8.080, round:223.393)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	221.800 (rec:8.151, round:213.649)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	213.011 (rec:8.970, round:204.041)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	202.490 (rec:8.149, round:194.341)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	198.025 (rec:13.326, round:184.699)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	184.050 (rec:7.859, round:176.192)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	176.209 (rec:8.228, round:167.980)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	169.227 (rec:9.502, round:159.725)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	158.798 (rec:7.816, round:150.982)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	150.526 (rec:8.266, round:142.261)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	141.720 (rec:8.149, round:133.571)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	138.023 (rec:12.777, round:125.246)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	126.745 (rec:10.061, round:116.684)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	118.622 (rec:10.855, round:107.767)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	108.878 (rec:9.470, round:99.408)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	97.890 (rec:7.842, round:90.048)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	87.915 (rec:6.993, round:80.922)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	83.539 (rec:11.976, round:71.563)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	70.959 (rec:9.046, round:61.912)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	60.753 (rec:7.849, round:52.904)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	54.905 (rec:11.173, round:43.732)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	44.899 (rec:10.102, round:34.797)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	34.261 (rec:8.853, round:25.407)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	26.194 (rec:9.523, round:16.671)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	20.417 (rec:10.662, round:9.756)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	13.884 (rec:7.872, round:6.012)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	10.095 (rec:7.573, round:2.522)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	10.267 (rec:9.495, round:0.771)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_9_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_3_post_act_fake_quantizer, features_9_conv_0_0, features_9_conv_0_1, features_9_conv_0_2, features_9_conv_0_2_post_act_fake_quantizer, features_9_conv_1_0, features_9_conv_1_1, features_9_conv_1_2, features_9_conv_1_2_post_act_fake_quantizer, features_9_conv_2, features_9_conv_3, add_4, add_4_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_3_post_act_fake_quantizer):
    features_9_conv_0_0 = getattr(getattr(getattr(self.features, "9").conv, "0"), "0")(add_3_post_act_fake_quantizer)
    features_9_conv_0_1 = getattr(getattr(getattr(self.features, "9").conv, "0"), "1")(features_9_conv_0_0);  features_9_conv_0_0 = None
    features_9_conv_0_2 = getattr(getattr(getattr(self.features, "9").conv, "0"), "2")(features_9_conv_0_1);  features_9_conv_0_1 = None
    features_9_conv_0_2_post_act_fake_quantizer = self.features_9_conv_0_2_post_act_fake_quantizer(features_9_conv_0_2);  features_9_conv_0_2 = None
    features_9_conv_1_0 = getattr(getattr(getattr(self.features, "9").conv, "1"), "0")(features_9_conv_0_2_post_act_fake_quantizer);  features_9_conv_0_2_post_act_fake_quantizer = None
    features_9_conv_1_1 = getattr(getattr(getattr(self.features, "9").conv, "1"), "1")(features_9_conv_1_0);  features_9_conv_1_0 = None
    features_9_conv_1_2 = getattr(getattr(getattr(self.features, "9").conv, "1"), "2")(features_9_conv_1_1);  features_9_conv_1_1 = None
    features_9_conv_1_2_post_act_fake_quantizer = self.features_9_conv_1_2_post_act_fake_quantizer(features_9_conv_1_2);  features_9_conv_1_2 = None
    features_9_conv_2 = getattr(getattr(self.features, "9").conv, "2")(features_9_conv_1_2_post_act_fake_quantizer);  features_9_conv_1_2_post_act_fake_quantizer = None
    features_9_conv_3 = getattr(getattr(self.features, "9").conv, "3")(features_9_conv_2);  features_9_conv_2 = None
    add_4 = add_3_post_act_fake_quantizer + features_9_conv_3;  add_3_post_act_fake_quantizer = features_9_conv_3 = None
    add_4_post_act_fake_quantizer = self.add_4_post_act_fake_quantizer(add_4);  add_4 = None
    return add_4_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_9_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_9_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_4_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	12.790 (rec:12.790, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	11.555 (rec:11.555, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	14.258 (rec:14.258, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	14.249 (rec:14.249, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	10.501 (rec:10.501, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	12.610 (rec:12.610, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	18.158 (rec:18.158, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	482.395 (rec:11.540, round:470.855)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	294.954 (rec:12.598, round:282.356)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	273.535 (rec:11.333, round:262.203)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	263.103 (rec:13.926, round:249.177)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	256.101 (rec:18.121, round:237.980)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	242.823 (rec:15.337, round:227.486)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	228.884 (rec:10.874, round:218.011)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	221.337 (rec:12.171, round:209.166)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	211.051 (rec:11.176, round:199.875)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	203.146 (rec:12.165, round:190.982)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	199.273 (rec:16.609, round:182.664)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	186.850 (rec:12.327, round:174.523)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	177.644 (rec:11.318, round:166.326)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	168.336 (rec:10.732, round:157.604)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	160.392 (rec:11.083, round:149.309)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	150.548 (rec:9.533, round:141.015)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	145.362 (rec:12.592, round:132.770)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	138.412 (rec:14.052, round:124.360)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	128.298 (rec:12.505, round:115.793)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	125.382 (rec:18.105, round:107.277)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	107.029 (rec:9.539, round:97.490)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	102.959 (rec:14.978, round:87.981)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	96.138 (rec:17.585, round:78.553)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	80.560 (rec:11.097, round:69.463)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	73.757 (rec:14.251, round:59.506)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	60.577 (rec:10.960, round:49.616)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	51.454 (rec:12.349, round:39.105)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	38.675 (rec:9.556, round:29.119)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	35.362 (rec:15.581, round:19.781)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	27.082 (rec:15.121, round:11.962)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	19.279 (rec:12.198, round:7.082)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	20.914 (rec:18.130, round:2.784)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	15.151 (rec:14.274, round:0.877)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_10_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_4_post_act_fake_quantizer, features_10_conv_0_0, features_10_conv_0_1, features_10_conv_0_2, features_10_conv_0_2_post_act_fake_quantizer, features_10_conv_1_0, features_10_conv_1_1, features_10_conv_1_2, features_10_conv_1_2_post_act_fake_quantizer, features_10_conv_2, features_10_conv_3, add_5, add_5_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_4_post_act_fake_quantizer):
    features_10_conv_0_0 = getattr(getattr(getattr(self.features, "10").conv, "0"), "0")(add_4_post_act_fake_quantizer)
    features_10_conv_0_1 = getattr(getattr(getattr(self.features, "10").conv, "0"), "1")(features_10_conv_0_0);  features_10_conv_0_0 = None
    features_10_conv_0_2 = getattr(getattr(getattr(self.features, "10").conv, "0"), "2")(features_10_conv_0_1);  features_10_conv_0_1 = None
    features_10_conv_0_2_post_act_fake_quantizer = self.features_10_conv_0_2_post_act_fake_quantizer(features_10_conv_0_2);  features_10_conv_0_2 = None
    features_10_conv_1_0 = getattr(getattr(getattr(self.features, "10").conv, "1"), "0")(features_10_conv_0_2_post_act_fake_quantizer);  features_10_conv_0_2_post_act_fake_quantizer = None
    features_10_conv_1_1 = getattr(getattr(getattr(self.features, "10").conv, "1"), "1")(features_10_conv_1_0);  features_10_conv_1_0 = None
    features_10_conv_1_2 = getattr(getattr(getattr(self.features, "10").conv, "1"), "2")(features_10_conv_1_1);  features_10_conv_1_1 = None
    features_10_conv_1_2_post_act_fake_quantizer = self.features_10_conv_1_2_post_act_fake_quantizer(features_10_conv_1_2);  features_10_conv_1_2 = None
    features_10_conv_2 = getattr(getattr(self.features, "10").conv, "2")(features_10_conv_1_2_post_act_fake_quantizer);  features_10_conv_1_2_post_act_fake_quantizer = None
    features_10_conv_3 = getattr(getattr(self.features, "10").conv, "3")(features_10_conv_2);  features_10_conv_2 = None
    add_5 = add_4_post_act_fake_quantizer + features_10_conv_3;  add_4_post_act_fake_quantizer = features_10_conv_3 = None
    add_5_post_act_fake_quantizer = self.add_5_post_act_fake_quantizer(add_5);  add_5 = None
    return add_5_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_10_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_10_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_5_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	13.383 (rec:13.383, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	21.615 (rec:21.615, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	18.702 (rec:18.702, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	11.389 (rec:11.389, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	15.232 (rec:15.232, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	17.972 (rec:17.972, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	12.665 (rec:12.665, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	489.323 (rec:15.073, round:474.250)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	300.727 (rec:13.910, round:286.817)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	285.729 (rec:17.960, round:267.769)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	269.695 (rec:15.212, round:254.483)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	256.719 (rec:13.530, round:243.189)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	245.412 (rec:13.114, round:232.298)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	241.379 (rec:18.673, round:222.706)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	235.403 (rec:22.131, round:213.273)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	222.566 (rec:17.954, round:204.612)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	210.722 (rec:15.140, round:195.582)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	202.866 (rec:15.926, round:186.940)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	195.943 (rec:17.952, round:177.991)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	182.949 (rec:13.334, round:169.614)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	173.729 (rec:13.111, round:160.618)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	171.790 (rec:19.892, round:151.898)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	158.777 (rec:15.199, round:143.578)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	148.297 (rec:13.341, round:134.956)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	140.541 (rec:13.673, round:126.868)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	136.650 (rec:18.175, round:118.475)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	128.453 (rec:18.174, round:110.278)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	120.939 (rec:19.876, round:101.063)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	107.040 (rec:15.211, round:91.829)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	101.048 (rec:18.680, round:82.368)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	85.787 (rec:13.123, round:72.664)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	81.202 (rec:18.521, round:62.680)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	67.225 (rec:14.738, round:52.487)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	56.225 (rec:13.926, round:42.299)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	48.656 (rec:16.767, round:31.889)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	35.158 (rec:13.439, round:21.720)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	29.704 (rec:16.550, round:13.154)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	18.938 (rec:11.419, round:7.518)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	18.076 (rec:15.189, round:2.887)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	14.265 (rec:13.474, round:0.791)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_11_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_5_post_act_fake_quantizer, features_11_conv_0_0, features_11_conv_0_1, features_11_conv_0_2, features_11_conv_0_2_post_act_fake_quantizer, features_11_conv_1_0, features_11_conv_1_1, features_11_conv_1_2, features_11_conv_1_2_post_act_fake_quantizer, features_11_conv_2, features_11_conv_3, features_11_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_5_post_act_fake_quantizer):
    features_11_conv_0_0 = getattr(getattr(getattr(self.features, "11").conv, "0"), "0")(add_5_post_act_fake_quantizer);  add_5_post_act_fake_quantizer = None
    features_11_conv_0_1 = getattr(getattr(getattr(self.features, "11").conv, "0"), "1")(features_11_conv_0_0);  features_11_conv_0_0 = None
    features_11_conv_0_2 = getattr(getattr(getattr(self.features, "11").conv, "0"), "2")(features_11_conv_0_1);  features_11_conv_0_1 = None
    features_11_conv_0_2_post_act_fake_quantizer = self.features_11_conv_0_2_post_act_fake_quantizer(features_11_conv_0_2);  features_11_conv_0_2 = None
    features_11_conv_1_0 = getattr(getattr(getattr(self.features, "11").conv, "1"), "0")(features_11_conv_0_2_post_act_fake_quantizer);  features_11_conv_0_2_post_act_fake_quantizer = None
    features_11_conv_1_1 = getattr(getattr(getattr(self.features, "11").conv, "1"), "1")(features_11_conv_1_0);  features_11_conv_1_0 = None
    features_11_conv_1_2 = getattr(getattr(getattr(self.features, "11").conv, "1"), "2")(features_11_conv_1_1);  features_11_conv_1_1 = None
    features_11_conv_1_2_post_act_fake_quantizer = self.features_11_conv_1_2_post_act_fake_quantizer(features_11_conv_1_2);  features_11_conv_1_2 = None
    features_11_conv_2 = getattr(getattr(self.features, "11").conv, "2")(features_11_conv_1_2_post_act_fake_quantizer);  features_11_conv_1_2_post_act_fake_quantizer = None
    features_11_conv_3 = getattr(getattr(self.features, "11").conv, "3")(features_11_conv_2);  features_11_conv_2 = None
    features_11_conv_3_post_act_fake_quantizer = self.features_11_conv_3_post_act_fake_quantizer(features_11_conv_3);  features_11_conv_3 = None
    return features_11_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_11_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_11_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_11_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	11.401 (rec:11.401, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	7.979 (rec:7.979, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	9.047 (rec:9.047, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	9.317 (rec:9.317, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	8.467 (rec:8.467, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	8.682 (rec:8.682, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	7.966 (rec:7.966, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	603.750 (rec:8.679, round:595.072)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	371.608 (rec:10.043, round:361.565)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	347.382 (rec:9.638, round:337.744)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	331.271 (rec:7.816, round:323.455)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	320.842 (rec:9.641, round:311.201)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	308.304 (rec:7.854, round:300.450)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	298.254 (rec:7.975, round:290.279)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	289.719 (rec:9.180, round:280.538)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	281.810 (rec:11.347, round:270.462)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	270.566 (rec:10.039, round:260.527)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	259.303 (rec:8.325, round:250.979)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	252.390 (rec:11.342, round:241.048)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	240.165 (rec:8.682, round:231.484)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	231.679 (rec:10.462, round:221.217)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	220.075 (rec:9.305, round:210.771)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	209.524 (rec:9.362, round:200.162)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	199.230 (rec:10.049, round:189.181)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	188.884 (rec:11.344, round:177.540)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	174.191 (rec:8.473, round:165.717)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	161.968 (rec:8.686, round:153.283)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	148.312 (rec:7.851, round:140.461)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	135.196 (rec:7.975, round:127.221)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	121.092 (rec:7.830, round:113.261)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	108.210 (rec:8.946, round:99.264)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	92.488 (rec:7.982, round:84.505)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	77.622 (rec:8.030, round:69.592)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	65.453 (rec:11.277, round:54.176)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	51.054 (rec:11.366, round:39.687)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	33.312 (rec:7.854, round:25.458)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	22.101 (rec:8.508, round:13.593)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	14.987 (rec:8.379, round:6.608)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	9.842 (rec:7.475, round:2.367)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	8.470 (rec:7.883, round:0.588)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_12_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_11_conv_3_post_act_fake_quantizer, features_12_conv_0_0, features_12_conv_0_1, features_12_conv_0_2, features_12_conv_0_2_post_act_fake_quantizer, features_12_conv_1_0, features_12_conv_1_1, features_12_conv_1_2, features_12_conv_1_2_post_act_fake_quantizer, features_12_conv_2, features_12_conv_3, add_6, add_6_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_11_conv_3_post_act_fake_quantizer):
    features_12_conv_0_0 = getattr(getattr(getattr(self.features, "12").conv, "0"), "0")(features_11_conv_3_post_act_fake_quantizer)
    features_12_conv_0_1 = getattr(getattr(getattr(self.features, "12").conv, "0"), "1")(features_12_conv_0_0);  features_12_conv_0_0 = None
    features_12_conv_0_2 = getattr(getattr(getattr(self.features, "12").conv, "0"), "2")(features_12_conv_0_1);  features_12_conv_0_1 = None
    features_12_conv_0_2_post_act_fake_quantizer = self.features_12_conv_0_2_post_act_fake_quantizer(features_12_conv_0_2);  features_12_conv_0_2 = None
    features_12_conv_1_0 = getattr(getattr(getattr(self.features, "12").conv, "1"), "0")(features_12_conv_0_2_post_act_fake_quantizer);  features_12_conv_0_2_post_act_fake_quantizer = None
    features_12_conv_1_1 = getattr(getattr(getattr(self.features, "12").conv, "1"), "1")(features_12_conv_1_0);  features_12_conv_1_0 = None
    features_12_conv_1_2 = getattr(getattr(getattr(self.features, "12").conv, "1"), "2")(features_12_conv_1_1);  features_12_conv_1_1 = None
    features_12_conv_1_2_post_act_fake_quantizer = self.features_12_conv_1_2_post_act_fake_quantizer(features_12_conv_1_2);  features_12_conv_1_2 = None
    features_12_conv_2 = getattr(getattr(self.features, "12").conv, "2")(features_12_conv_1_2_post_act_fake_quantizer);  features_12_conv_1_2_post_act_fake_quantizer = None
    features_12_conv_3 = getattr(getattr(self.features, "12").conv, "3")(features_12_conv_2);  features_12_conv_2 = None
    add_6 = features_11_conv_3_post_act_fake_quantizer + features_12_conv_3;  features_11_conv_3_post_act_fake_quantizer = features_12_conv_3 = None
    add_6_post_act_fake_quantizer = self.add_6_post_act_fake_quantizer(add_6);  add_6 = None
    return add_6_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_12_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_12_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_6_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	15.220 (rec:15.220, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	12.047 (rec:12.047, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	15.545 (rec:15.545, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	11.980 (rec:11.980, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	13.460 (rec:13.460, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	15.192 (rec:15.192, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	12.037 (rec:12.037, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	1061.390 (rec:15.100, round:1046.290)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	622.317 (rec:12.317, round:610.000)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	579.880 (rec:11.729, round:568.151)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	554.105 (rec:13.706, round:540.399)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	531.716 (rec:14.243, round:517.473)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	508.174 (rec:12.531, round:495.644)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	488.498 (rec:13.689, round:474.809)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	466.185 (rec:11.723, round:454.462)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	451.228 (rec:16.872, round:434.356)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	427.431 (rec:11.721, round:415.711)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	408.685 (rec:11.824, round:396.861)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	390.800 (rec:11.956, round:378.844)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	375.656 (rec:16.939, round:358.717)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	352.385 (rec:11.757, round:340.628)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	334.766 (rec:12.926, round:321.840)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	315.018 (rec:12.636, round:302.382)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	294.632 (rec:11.754, round:282.878)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	275.673 (rec:12.421, round:263.252)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	257.116 (rec:13.796, round:243.319)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	237.874 (rec:15.094, round:222.780)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	214.921 (rec:12.530, round:202.391)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	195.126 (rec:13.444, round:181.682)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	174.575 (rec:13.803, round:160.772)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	151.341 (rec:11.762, round:139.579)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	131.063 (rec:13.349, round:117.714)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	107.764 (rec:11.840, round:95.924)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	86.424 (rec:11.842, round:74.582)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	66.401 (rec:12.936, round:53.465)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	46.467 (rec:12.654, round:33.814)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	30.135 (rec:12.952, round:17.183)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	24.472 (rec:15.522, round:8.950)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	15.207 (rec:11.715, round:3.493)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	13.386 (rec:12.448, round:0.938)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_13_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_6_post_act_fake_quantizer, features_13_conv_0_0, features_13_conv_0_1, features_13_conv_0_2, features_13_conv_0_2_post_act_fake_quantizer, features_13_conv_1_0, features_13_conv_1_1, features_13_conv_1_2, features_13_conv_1_2_post_act_fake_quantizer, features_13_conv_2, features_13_conv_3, add_7, add_7_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_6_post_act_fake_quantizer):
    features_13_conv_0_0 = getattr(getattr(getattr(self.features, "13").conv, "0"), "0")(add_6_post_act_fake_quantizer)
    features_13_conv_0_1 = getattr(getattr(getattr(self.features, "13").conv, "0"), "1")(features_13_conv_0_0);  features_13_conv_0_0 = None
    features_13_conv_0_2 = getattr(getattr(getattr(self.features, "13").conv, "0"), "2")(features_13_conv_0_1);  features_13_conv_0_1 = None
    features_13_conv_0_2_post_act_fake_quantizer = self.features_13_conv_0_2_post_act_fake_quantizer(features_13_conv_0_2);  features_13_conv_0_2 = None
    features_13_conv_1_0 = getattr(getattr(getattr(self.features, "13").conv, "1"), "0")(features_13_conv_0_2_post_act_fake_quantizer);  features_13_conv_0_2_post_act_fake_quantizer = None
    features_13_conv_1_1 = getattr(getattr(getattr(self.features, "13").conv, "1"), "1")(features_13_conv_1_0);  features_13_conv_1_0 = None
    features_13_conv_1_2 = getattr(getattr(getattr(self.features, "13").conv, "1"), "2")(features_13_conv_1_1);  features_13_conv_1_1 = None
    features_13_conv_1_2_post_act_fake_quantizer = self.features_13_conv_1_2_post_act_fake_quantizer(features_13_conv_1_2);  features_13_conv_1_2 = None
    features_13_conv_2 = getattr(getattr(self.features, "13").conv, "2")(features_13_conv_1_2_post_act_fake_quantizer);  features_13_conv_1_2_post_act_fake_quantizer = None
    features_13_conv_3 = getattr(getattr(self.features, "13").conv, "3")(features_13_conv_2);  features_13_conv_2 = None
    add_7 = add_6_post_act_fake_quantizer + features_13_conv_3;  add_6_post_act_fake_quantizer = features_13_conv_3 = None
    add_7_post_act_fake_quantizer = self.add_7_post_act_fake_quantizer(add_7);  add_7 = None
    return add_7_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_13_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_13_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_7_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	21.567 (rec:21.567, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	19.036 (rec:19.036, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	23.797 (rec:23.797, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	20.492 (rec:20.492, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	19.002 (rec:19.002, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	16.952 (rec:16.952, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	17.045 (rec:17.045, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	1071.884 (rec:19.648, round:1052.236)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	649.739 (rec:20.713, round:629.026)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	604.904 (rec:18.991, round:585.914)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	576.197 (rec:18.046, round:558.151)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	554.539 (rec:19.375, round:535.164)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	535.922 (rec:22.095, round:513.827)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	515.595 (rec:22.093, round:493.502)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	490.252 (rec:16.948, round:473.305)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	469.959 (rec:15.921, round:454.037)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	455.931 (rec:21.348, round:434.582)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	434.108 (rec:19.376, round:414.732)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	415.175 (rec:18.965, round:396.210)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	396.858 (rec:19.498, round:377.360)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	375.455 (rec:17.194, round:358.261)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	357.315 (rec:17.941, round:339.374)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	342.666 (rec:22.101, round:320.565)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	318.494 (rec:16.920, round:301.574)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	302.949 (rec:20.702, round:282.247)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	278.548 (rec:15.933, round:262.616)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	261.350 (rec:19.134, round:242.216)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	241.356 (rec:19.778, round:221.578)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	219.000 (rec:18.123, round:200.878)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	198.965 (rec:19.370, round:179.595)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	174.583 (rec:17.390, round:157.193)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	152.855 (rec:17.860, round:134.995)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	129.790 (rec:17.396, round:112.394)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	113.172 (rec:23.709, round:89.463)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	83.696 (rec:16.939, round:66.757)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	64.013 (rec:19.234, round:44.778)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	45.794 (rec:20.565, round:25.229)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	33.829 (rec:21.385, round:12.444)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	27.920 (rec:23.797, round:4.123)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	22.475 (rec:21.548, round:0.927)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_14_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_7_post_act_fake_quantizer, features_14_conv_0_0, features_14_conv_0_1, features_14_conv_0_2, features_14_conv_0_2_post_act_fake_quantizer, features_14_conv_1_0, features_14_conv_1_1, features_14_conv_1_2, features_14_conv_1_2_post_act_fake_quantizer, features_14_conv_2, features_14_conv_3, features_14_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_7_post_act_fake_quantizer):
    features_14_conv_0_0 = getattr(getattr(getattr(self.features, "14").conv, "0"), "0")(add_7_post_act_fake_quantizer);  add_7_post_act_fake_quantizer = None
    features_14_conv_0_1 = getattr(getattr(getattr(self.features, "14").conv, "0"), "1")(features_14_conv_0_0);  features_14_conv_0_0 = None
    features_14_conv_0_2 = getattr(getattr(getattr(self.features, "14").conv, "0"), "2")(features_14_conv_0_1);  features_14_conv_0_1 = None
    features_14_conv_0_2_post_act_fake_quantizer = self.features_14_conv_0_2_post_act_fake_quantizer(features_14_conv_0_2);  features_14_conv_0_2 = None
    features_14_conv_1_0 = getattr(getattr(getattr(self.features, "14").conv, "1"), "0")(features_14_conv_0_2_post_act_fake_quantizer);  features_14_conv_0_2_post_act_fake_quantizer = None
    features_14_conv_1_1 = getattr(getattr(getattr(self.features, "14").conv, "1"), "1")(features_14_conv_1_0);  features_14_conv_1_0 = None
    features_14_conv_1_2 = getattr(getattr(getattr(self.features, "14").conv, "1"), "2")(features_14_conv_1_1);  features_14_conv_1_1 = None
    features_14_conv_1_2_post_act_fake_quantizer = self.features_14_conv_1_2_post_act_fake_quantizer(features_14_conv_1_2);  features_14_conv_1_2 = None
    features_14_conv_2 = getattr(getattr(self.features, "14").conv, "2")(features_14_conv_1_2_post_act_fake_quantizer);  features_14_conv_1_2_post_act_fake_quantizer = None
    features_14_conv_3 = getattr(getattr(self.features, "14").conv, "3")(features_14_conv_2);  features_14_conv_2 = None
    features_14_conv_3_post_act_fake_quantizer = self.features_14_conv_3_post_act_fake_quantizer(features_14_conv_3);  features_14_conv_3 = None
    return features_14_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_14_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_14_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_14_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	7.025 (rec:7.025, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	6.585 (rec:6.585, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	7.483 (rec:7.483, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	7.047 (rec:7.047, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	7.235 (rec:7.235, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	7.007 (rec:7.007, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	7.344 (rec:7.344, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	1415.935 (rec:6.977, round:1408.958)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	861.714 (rec:6.526, round:855.188)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	807.334 (rec:7.101, round:800.233)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	772.049 (rec:7.335, round:764.714)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	742.759 (rec:6.721, round:736.038)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	715.673 (rec:5.778, round:709.895)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	692.211 (rec:6.977, round:685.234)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	668.199 (rec:7.024, round:661.175)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	643.029 (rec:6.366, round:636.663)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	619.178 (rec:6.537, round:612.641)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	594.984 (rec:5.977, round:589.007)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	572.548 (rec:7.683, round:564.865)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	548.162 (rec:6.976, round:541.186)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	522.492 (rec:6.853, round:515.638)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	497.501 (rec:7.221, round:490.280)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	473.655 (rec:9.514, round:464.141)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	446.015 (rec:7.673, round:438.343)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	418.218 (rec:6.353, round:411.865)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	392.564 (rec:7.660, round:384.904)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	363.768 (rec:7.417, round:356.351)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	334.728 (rec:7.212, round:327.515)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	304.540 (rec:7.111, round:297.428)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	275.938 (rec:8.765, round:267.172)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	242.759 (rec:6.903, round:235.856)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	211.376 (rec:8.101, round:203.275)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	177.354 (rec:6.993, round:170.361)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	144.113 (rec:7.182, round:136.931)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	110.623 (rec:7.333, round:103.290)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	77.653 (rec:7.003, round:70.650)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	45.512 (rec:6.018, round:39.494)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	22.568 (rec:6.416, round:16.152)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	11.252 (rec:6.842, round:4.411)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	7.708 (rec:6.670, round:1.038)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_15_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_14_conv_3_post_act_fake_quantizer, features_15_conv_0_0, features_15_conv_0_1, features_15_conv_0_2, features_15_conv_0_2_post_act_fake_quantizer, features_15_conv_1_0, features_15_conv_1_1, features_15_conv_1_2, features_15_conv_1_2_post_act_fake_quantizer, features_15_conv_2, features_15_conv_3, add_8, add_8_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_14_conv_3_post_act_fake_quantizer):
    features_15_conv_0_0 = getattr(getattr(getattr(self.features, "15").conv, "0"), "0")(features_14_conv_3_post_act_fake_quantizer)
    features_15_conv_0_1 = getattr(getattr(getattr(self.features, "15").conv, "0"), "1")(features_15_conv_0_0);  features_15_conv_0_0 = None
    features_15_conv_0_2 = getattr(getattr(getattr(self.features, "15").conv, "0"), "2")(features_15_conv_0_1);  features_15_conv_0_1 = None
    features_15_conv_0_2_post_act_fake_quantizer = self.features_15_conv_0_2_post_act_fake_quantizer(features_15_conv_0_2);  features_15_conv_0_2 = None
    features_15_conv_1_0 = getattr(getattr(getattr(self.features, "15").conv, "1"), "0")(features_15_conv_0_2_post_act_fake_quantizer);  features_15_conv_0_2_post_act_fake_quantizer = None
    features_15_conv_1_1 = getattr(getattr(getattr(self.features, "15").conv, "1"), "1")(features_15_conv_1_0);  features_15_conv_1_0 = None
    features_15_conv_1_2 = getattr(getattr(getattr(self.features, "15").conv, "1"), "2")(features_15_conv_1_1);  features_15_conv_1_1 = None
    features_15_conv_1_2_post_act_fake_quantizer = self.features_15_conv_1_2_post_act_fake_quantizer(features_15_conv_1_2);  features_15_conv_1_2 = None
    features_15_conv_2 = getattr(getattr(self.features, "15").conv, "2")(features_15_conv_1_2_post_act_fake_quantizer);  features_15_conv_1_2_post_act_fake_quantizer = None
    features_15_conv_3 = getattr(getattr(self.features, "15").conv, "3")(features_15_conv_2);  features_15_conv_2 = None
    add_8 = features_14_conv_3_post_act_fake_quantizer + features_15_conv_3;  features_14_conv_3_post_act_fake_quantizer = features_15_conv_3 = None
    add_8_post_act_fake_quantizer = self.add_8_post_act_fake_quantizer(add_8);  add_8 = None
    return add_8_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_15_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_15_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_8_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	11.413 (rec:11.413, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	14.340 (rec:14.340, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	11.624 (rec:11.624, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	10.312 (rec:10.312, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	11.610 (rec:11.610, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	12.141 (rec:12.141, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	10.297 (rec:10.297, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	2891.413 (rec:13.220, round:2878.192)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	1687.770 (rec:11.698, round:1676.073)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	1574.094 (rec:12.325, round:1561.770)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	1495.690 (rec:11.962, round:1483.728)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	1429.715 (rec:13.352, round:1416.363)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	1365.307 (rec:12.836, round:1352.472)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	1303.382 (rec:12.141, round:1291.241)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	1244.847 (rec:11.543, round:1233.304)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	1188.396 (rec:12.128, round:1176.268)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1130.821 (rec:11.862, round:1118.959)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	1074.556 (rec:11.661, round:1062.895)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	1018.857 (rec:12.120, round:1006.738)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	961.180 (rec:11.208, round:949.972)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	906.584 (rec:12.402, round:894.182)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	850.637 (rec:12.472, round:838.165)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	793.030 (rec:10.175, round:782.855)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	741.853 (rec:13.122, round:728.731)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	684.845 (rec:11.425, round:673.420)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	631.036 (rec:11.656, round:619.380)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	577.613 (rec:12.066, round:565.547)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	524.105 (rec:12.286, round:511.818)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	470.651 (rec:12.562, round:458.089)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	415.323 (rec:10.733, round:404.591)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	362.709 (rec:11.774, round:350.936)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	307.143 (rec:10.184, round:296.959)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	255.374 (rec:12.525, round:242.850)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	202.996 (rec:12.305, round:190.691)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	150.790 (rec:11.781, round:139.009)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	101.985 (rec:12.319, round:89.666)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	60.526 (rec:13.714, round:46.812)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	32.474 (rec:12.120, round:20.354)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	18.779 (rec:12.349, round:6.431)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	12.957 (rec:11.274, round:1.683)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_16_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_8_post_act_fake_quantizer, features_16_conv_0_0, features_16_conv_0_1, features_16_conv_0_2, features_16_conv_0_2_post_act_fake_quantizer, features_16_conv_1_0, features_16_conv_1_1, features_16_conv_1_2, features_16_conv_1_2_post_act_fake_quantizer, features_16_conv_2, features_16_conv_3, add_9, add_9_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_8_post_act_fake_quantizer):
    features_16_conv_0_0 = getattr(getattr(getattr(self.features, "16").conv, "0"), "0")(add_8_post_act_fake_quantizer)
    features_16_conv_0_1 = getattr(getattr(getattr(self.features, "16").conv, "0"), "1")(features_16_conv_0_0);  features_16_conv_0_0 = None
    features_16_conv_0_2 = getattr(getattr(getattr(self.features, "16").conv, "0"), "2")(features_16_conv_0_1);  features_16_conv_0_1 = None
    features_16_conv_0_2_post_act_fake_quantizer = self.features_16_conv_0_2_post_act_fake_quantizer(features_16_conv_0_2);  features_16_conv_0_2 = None
    features_16_conv_1_0 = getattr(getattr(getattr(self.features, "16").conv, "1"), "0")(features_16_conv_0_2_post_act_fake_quantizer);  features_16_conv_0_2_post_act_fake_quantizer = None
    features_16_conv_1_1 = getattr(getattr(getattr(self.features, "16").conv, "1"), "1")(features_16_conv_1_0);  features_16_conv_1_0 = None
    features_16_conv_1_2 = getattr(getattr(getattr(self.features, "16").conv, "1"), "2")(features_16_conv_1_1);  features_16_conv_1_1 = None
    features_16_conv_1_2_post_act_fake_quantizer = self.features_16_conv_1_2_post_act_fake_quantizer(features_16_conv_1_2);  features_16_conv_1_2 = None
    features_16_conv_2 = getattr(getattr(self.features, "16").conv, "2")(features_16_conv_1_2_post_act_fake_quantizer);  features_16_conv_1_2_post_act_fake_quantizer = None
    features_16_conv_3 = getattr(getattr(self.features, "16").conv, "3")(features_16_conv_2);  features_16_conv_2 = None
    add_9 = add_8_post_act_fake_quantizer + features_16_conv_3;  add_8_post_act_fake_quantizer = features_16_conv_3 = None
    add_9_post_act_fake_quantizer = self.add_9_post_act_fake_quantizer(add_9);  add_9 = None
    return add_9_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_16_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_16_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_9_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	19.646 (rec:19.646, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	21.718 (rec:21.718, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	20.322 (rec:20.322, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	20.334 (rec:20.334, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	19.421 (rec:19.421, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	24.921 (rec:24.921, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	20.832 (rec:20.832, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	2905.629 (rec:23.892, round:2881.738)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	1756.307 (rec:22.402, round:1733.904)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	1635.351 (rec:20.765, round:1614.586)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	1562.205 (rec:27.607, round:1534.598)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	1485.730 (rec:20.140, round:1465.590)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	1421.277 (rec:19.955, round:1401.322)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	1361.702 (rec:22.219, round:1339.483)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	1299.714 (rec:20.725, round:1278.989)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	1240.516 (rec:21.526, round:1218.990)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1177.506 (rec:17.772, round:1159.734)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	1120.427 (rec:17.906, round:1102.521)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	1066.261 (rec:20.625, round:1045.636)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	1009.996 (rec:21.451, round:988.545)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	954.266 (rec:20.514, round:933.752)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	900.690 (rec:21.462, round:879.228)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	845.537 (rec:21.330, round:824.207)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	791.416 (rec:20.814, round:770.602)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	735.208 (rec:19.377, round:715.831)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	679.801 (rec:18.517, round:661.284)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	628.058 (rec:20.614, round:607.444)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	573.279 (rec:19.822, round:553.457)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	524.349 (rec:23.608, round:500.741)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	464.855 (rec:17.812, round:447.042)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	419.943 (rec:27.448, round:392.495)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	359.271 (rec:21.143, round:338.128)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	304.648 (rec:20.506, round:284.143)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	247.714 (rec:18.213, round:229.501)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	196.914 (rec:20.887, round:176.026)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	144.709 (rec:20.494, round:124.215)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	96.640 (rec:21.343, round:75.297)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	56.682 (rec:20.629, round:36.054)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	31.487 (rec:20.902, round:10.586)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	23.367 (rec:21.342, round:2.024)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_17_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_9_post_act_fake_quantizer, features_17_conv_0_0, features_17_conv_0_1, features_17_conv_0_2, features_17_conv_0_2_post_act_fake_quantizer, features_17_conv_1_0, features_17_conv_1_1, features_17_conv_1_2, features_17_conv_1_2_post_act_fake_quantizer, features_17_conv_2, features_17_conv_3, features_17_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_9_post_act_fake_quantizer):
    features_17_conv_0_0 = getattr(getattr(getattr(self.features, "17").conv, "0"), "0")(add_9_post_act_fake_quantizer);  add_9_post_act_fake_quantizer = None
    features_17_conv_0_1 = getattr(getattr(getattr(self.features, "17").conv, "0"), "1")(features_17_conv_0_0);  features_17_conv_0_0 = None
    features_17_conv_0_2 = getattr(getattr(getattr(self.features, "17").conv, "0"), "2")(features_17_conv_0_1);  features_17_conv_0_1 = None
    features_17_conv_0_2_post_act_fake_quantizer = self.features_17_conv_0_2_post_act_fake_quantizer(features_17_conv_0_2);  features_17_conv_0_2 = None
    features_17_conv_1_0 = getattr(getattr(getattr(self.features, "17").conv, "1"), "0")(features_17_conv_0_2_post_act_fake_quantizer);  features_17_conv_0_2_post_act_fake_quantizer = None
    features_17_conv_1_1 = getattr(getattr(getattr(self.features, "17").conv, "1"), "1")(features_17_conv_1_0);  features_17_conv_1_0 = None
    features_17_conv_1_2 = getattr(getattr(getattr(self.features, "17").conv, "1"), "2")(features_17_conv_1_1);  features_17_conv_1_1 = None
    features_17_conv_1_2_post_act_fake_quantizer = self.features_17_conv_1_2_post_act_fake_quantizer(features_17_conv_1_2);  features_17_conv_1_2 = None
    features_17_conv_2 = getattr(getattr(self.features, "17").conv, "2")(features_17_conv_1_2_post_act_fake_quantizer);  features_17_conv_1_2_post_act_fake_quantizer = None
    features_17_conv_3 = getattr(getattr(self.features, "17").conv, "3")(features_17_conv_2);  features_17_conv_2 = None
    features_17_conv_3_post_act_fake_quantizer = self.features_17_conv_3_post_act_fake_quantizer(features_17_conv_3);  features_17_conv_3 = None
    return features_17_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_17_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_17_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_17_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	11.397 (rec:11.397, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	10.821 (rec:10.821, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	13.089 (rec:13.089, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	11.429 (rec:11.429, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	11.322 (rec:11.322, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	11.260 (rec:11.260, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	11.974 (rec:11.974, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	4357.977 (rec:12.380, round:4345.597)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	2647.176 (rec:11.680, round:2635.496)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	2476.138 (rec:10.586, round:2465.551)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	2365.806 (rec:9.505, round:2356.302)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2278.825 (rec:13.164, round:2265.661)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	2192.414 (rec:12.030, round:2180.384)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2110.148 (rec:10.651, round:2099.497)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	2029.537 (rec:9.587, round:2019.951)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	1953.050 (rec:12.237, round:1940.814)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1872.557 (rec:9.481, round:1863.077)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	1798.830 (rec:12.358, round:1786.472)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	1718.235 (rec:9.467, round:1708.768)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	1645.108 (rec:14.299, round:1630.809)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	1560.660 (rec:9.118, round:1551.542)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	1480.524 (rec:9.435, round:1471.089)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	1401.619 (rec:11.122, round:1390.497)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1315.989 (rec:9.427, round:1306.562)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	1234.111 (rec:10.360, round:1223.751)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	1155.639 (rec:14.580, round:1141.059)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	1065.184 (rec:9.444, round:1055.740)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	980.851 (rec:14.251, round:966.600)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	888.581 (rec:11.807, round:876.773)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	797.226 (rec:12.191, round:785.035)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	703.298 (rec:10.436, round:692.862)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	612.299 (rec:14.576, round:597.723)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	513.445 (rec:12.339, round:501.106)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	415.656 (rec:12.558, round:403.098)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	314.351 (rec:9.092, round:305.260)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	220.614 (rec:12.145, round:208.469)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	126.339 (rec:9.683, round:116.656)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	54.232 (rec:10.450, round:43.782)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	24.291 (rec:14.375, round:9.916)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	11.575 (rec:9.588, round:1.987)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_18_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_17_conv_3_post_act_fake_quantizer, features_18_0, features_18_1, features_18_2, adaptive_avg_pool2d, flatten, classifier_0, classifier_0_post_act_fake_quantizer, classifier_1]
[MQBENCH] INFO: 


def forward(self, features_17_conv_3_post_act_fake_quantizer):
    features_18_0 = getattr(getattr(self.features, "18"), "0")(features_17_conv_3_post_act_fake_quantizer);  features_17_conv_3_post_act_fake_quantizer = None
    features_18_1 = getattr(getattr(self.features, "18"), "1")(features_18_0);  features_18_0 = None
    features_18_2 = getattr(getattr(self.features, "18"), "2")(features_18_1);  features_18_1 = None
    adaptive_avg_pool2d = torch.nn.functional.adaptive_avg_pool2d(features_18_2, (1, 1));  features_18_2 = None
    flatten = torch.flatten(adaptive_avg_pool2d, 1);  adaptive_avg_pool2d = None
    classifier_0 = getattr(self.classifier, "0")(flatten);  flatten = None
    classifier_0_post_act_fake_quantizer = self.classifier_0_post_act_fake_quantizer(classifier_0);  classifier_0 = None
    classifier_1 = getattr(self.classifier, "1")(classifier_0_post_act_fake_quantizer);  classifier_0_post_act_fake_quantizer = None
    return classifier_1
    
[MQBENCH] INFO: learn the scale for classifier_0_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	1.026 (rec:1.026, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	1.913 (rec:1.913, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.972 (rec:0.972, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	1.122 (rec:1.122, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	1.351 (rec:1.351, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	1.738 (rec:1.738, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.995 (rec:0.995, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	15401.877 (rec:1.433, round:15400.444)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	8109.442 (rec:1.247, round:8108.195)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	7483.896 (rec:1.232, round:7482.664)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	7041.394 (rec:0.952, round:7040.442)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	6649.669 (rec:1.170, round:6648.499)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	6273.430 (rec:0.690, round:6272.740)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	5902.334 (rec:0.877, round:5901.458)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	5539.577 (rec:1.567, round:5538.010)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	5180.324 (rec:0.904, round:5179.420)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	4827.544 (rec:1.157, round:4826.387)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	4489.217 (rec:1.106, round:4488.111)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	4154.605 (rec:1.098, round:4153.507)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	3828.501 (rec:1.309, round:3827.192)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	3510.822 (rec:1.545, round:3509.277)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	3203.343 (rec:1.450, round:3201.892)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	2903.166 (rec:1.443, round:2901.723)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	2619.312 (rec:0.847, round:2618.466)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	2347.735 (rec:0.843, round:2346.893)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	2089.152 (rec:1.118, round:2088.035)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	1839.975 (rec:1.134, round:1838.841)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	1598.479 (rec:0.947, round:1597.531)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	1371.182 (rec:1.306, round:1369.875)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	1151.725 (rec:0.828, round:1150.898)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	947.041 (rec:0.873, round:946.169)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	756.388 (rec:1.162, round:755.226)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	579.837 (rec:0.863, round:578.974)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	418.355 (rec:0.860, round:417.495)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	274.163 (rec:0.856, round:273.307)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	154.063 (rec:0.842, round:153.221)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	70.415 (rec:1.061, round:69.354)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	28.126 (rec:0.836, round:27.290)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	9.494 (rec:0.842, round:8.651)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	3.370 (rec:0.846, round:2.525)	b=2.00	count=20000
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node x_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.1.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.1.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.1.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_1_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.1.conv.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.1.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_1_conv_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_2_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_2_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.2.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_2_conv_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_3_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_3_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.3.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_4_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_4_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.4.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_4_conv_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_5_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_5_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.5.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_6_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_6_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.6.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_7_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_7_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.7.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_7_conv_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_8_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_8_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.8.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_9_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_9_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.9.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_4_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_10_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_10_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.10.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_5_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_11_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_11_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.11.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_11_conv_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_12_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_12_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.12.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_6_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_13_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_13_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.13.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_7_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_14_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_14_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.14.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_14_conv_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_15_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_15_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.15.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_8_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_16_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_16_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.16.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node add_9_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.0.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.0.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.0.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_17_conv_0_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.1.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.1.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.1.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_17_conv_1_2_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.17.conv.3 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features_17_conv_3_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.18.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.18.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node features.18.2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node classifier.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node classifier_0_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node classifier.1 in quant
2025-08-17 14:27:48,402 | INFO | âœ” END: advanced PTQ reconstruction (elapsed 2755.96s)
2025-08-17 14:27:48,404 | INFO | â–¶ START: enable_quantization (simulate INT8)
[MQBENCH] INFO: Disable observer and Enable quantize.
2025-08-17 14:27:48,408 | INFO | âœ” END: enable_quantization (simulate INT8) (elapsed 0.00s)
2025-08-17 14:27:48,408 | INFO | âœ” END: prepare_by_platform(Academic) (elapsed 2761.04s)
2025-08-17 14:27:48,410 | INFO | â–¶ START: evaluate INT8-sim
2025-08-17 14:27:51,153 | INFO | [EVAL_INT8] progress: 50 batches, running top1=78.31%
2025-08-17 14:27:53,193 | INFO | [EVAL_INT8] progress: 100 batches, running top1=79.27%
2025-08-17 14:27:55,295 | INFO | [EVAL_INT8] progress: 150 batches, running top1=78.80%
2025-08-17 14:27:57,281 | INFO | [EVAL_INT8] progress: 200 batches, running top1=78.05%
2025-08-17 14:27:59,393 | INFO | [EVAL_INT8] progress: 250 batches, running top1=77.92%
2025-08-17 14:28:01,440 | INFO | [EVAL_INT8] progress: 300 batches, running top1=78.28%
2025-08-17 14:28:03,420 | INFO | [EVAL_INT8] progress: 350 batches, running top1=77.20%
2025-08-17 14:28:05,401 | INFO | [EVAL_INT8] progress: 400 batches, running top1=75.70%
2025-08-17 14:28:07,377 | INFO | [EVAL_INT8] progress: 450 batches, running top1=75.09%
2025-08-17 14:28:09,379 | INFO | [EVAL_INT8] progress: 500 batches, running top1=74.16%
2025-08-17 14:28:11,363 | INFO | [EVAL_INT8] progress: 550 batches, running top1=73.53%
2025-08-17 14:28:13,288 | INFO | [EVAL_INT8] progress: 600 batches, running top1=72.96%
2025-08-17 14:28:15,296 | INFO | [EVAL_INT8] progress: 650 batches, running top1=72.58%
2025-08-17 14:28:17,243 | INFO | [EVAL_INT8] progress: 700 batches, running top1=72.04%
2025-08-17 14:28:19,152 | INFO | [EVAL_INT8] progress: 750 batches, running top1=72.07%
2025-08-17 14:28:21,490 | INFO | [EVAL_INT8] done: 782 batches in 33.08s, top1=72.03%
2025-08-17 14:28:21,490 | INFO | [PTQ][mobilenet_v2][Academic] [ADV] Top-1 = 72.03%
2025-08-17 14:28:21,490 | INFO | âœ” END: evaluate INT8-sim (elapsed 33.08s)
2025-08-17 14:28:21,490 | INFO | â–¶ START: evaluate FP32 baseline
2025-08-17 14:28:23,923 | INFO | [EVAL_FP32] progress: 50 batches, running top1=78.28%
2025-08-17 14:28:25,997 | INFO | [EVAL_FP32] progress: 100 batches, running top1=79.39%
2025-08-17 14:28:27,923 | INFO | [EVAL_FP32] progress: 150 batches, running top1=78.90%
2025-08-17 14:28:29,721 | INFO | [EVAL_FP32] progress: 200 batches, running top1=78.18%
2025-08-17 14:28:31,831 | INFO | [EVAL_FP32] progress: 250 batches, running top1=78.02%
2025-08-17 14:28:33,918 | INFO | [EVAL_FP32] progress: 300 batches, running top1=78.35%
2025-08-17 14:28:35,776 | INFO | [EVAL_FP32] progress: 350 batches, running top1=77.30%
2025-08-17 14:28:37,631 | INFO | [EVAL_FP32] progress: 400 batches, running top1=75.82%
2025-08-17 14:28:39,539 | INFO | [EVAL_FP32] progress: 450 batches, running top1=75.21%
2025-08-17 14:28:41,536 | INFO | [EVAL_FP32] progress: 500 batches, running top1=74.27%
2025-08-17 14:28:43,541 | INFO | [EVAL_FP32] progress: 550 batches, running top1=73.65%
2025-08-17 14:28:45,358 | INFO | [EVAL_FP32] progress: 600 batches, running top1=73.09%
2025-08-17 14:28:47,271 | INFO | [EVAL_FP32] progress: 650 batches, running top1=72.71%
2025-08-17 14:28:49,048 | INFO | [EVAL_FP32] progress: 700 batches, running top1=72.16%
2025-08-17 14:28:51,079 | INFO | [EVAL_FP32] progress: 750 batches, running top1=72.19%
2025-08-17 14:28:52,271 | INFO | [EVAL_FP32] done: 782 batches in 30.78s, top1=72.15%
2025-08-17 14:28:52,271 | INFO | [FP32] Top-1 = 72.15% (expected ~None)
2025-08-17 14:28:52,271 | INFO | âœ” END: evaluate FP32 baseline (elapsed 30.78s)
2025-08-17 14:28:52,271 | INFO | â–¶ START: extract model logits
2025-08-17 14:28:52,273 | INFO | Extracting logits from both models...

============================================================
BASELINE ACCURACIES (Before Clustering)
============================================================
  FP32 Model: 72.15%
  Baseline PTQ: 72.03%
  PTQ Degradation: 0.12%
============================================================
Extracting logits from quantized and full-precision models...
2025-08-17 14:28:55,486 | INFO | Processed 5 batches
2025-08-17 14:28:58,731 | INFO | Processed 10 batches
2025-08-17 14:29:01,427 | INFO | Extracted logits: Q=torch.Size([640, 1000]), FP=torch.Size([640, 1000])
Logits extraction complete.
Quantized logits shape: torch.Size([640, 1000])
Full-precision logits shape: torch.Size([640, 1000])
ðŸš€ Running all 1 combinations...

ðŸ”„ [1/1] Running with alpha=0.5, num_clusters=16, pca_dim=50
2025-08-17 14:29:51,829 | INFO | âœ” END: extract model logits (elapsed 59.56s)
[Alpha=0.50] Top-1 Accuracy: 72.05%
[Alpha=0.50] Top-5 Accuracy: 90.79%
âœ… Result: Top-1: 72.05%, Top-5: 90.79%
ðŸ’¾ Saving intermediate results... (1 total combinations)
Results saved to: adaround_lsq_mobilenet_v2_20250817_134116/ptq_results_20250817_142951.csv
ðŸ’¾ Recovery checkpoint saved: adaround_lsq_mobilenet_v2_20250817_134116/recovery_checkpoint.json

================================================================================
SUMMARY OF ALL RESULTS
================================================================================
Alpha    Clusters   PCA_dim    Top-1      Top-5     
--------------------------------------------------
0.50     16         50         72.05      90.79     

BEST RESULT:
  Alpha: 0.5
  Clusters: 16
  PCA_dim: 50
  Top-1 Accuracy: 72.05%
  Top-5 Accuracy: 90.79%

ACCURACY COMPARISON:
  FP32 Model: 72.15%
  Baseline PTQ: 72.03%
  Best Clustering: 72.05%
  PTQ Degradation: 0.12%
  Clustering Recovery: 0.02%
  Final Gap to FP32: 0.11%
Results saved to: adaround_lsq_mobilenet_v2_20250817_134116/ptq_results_20250817_142951.csv
Summary saved to: adaround_lsq_mobilenet_v2_20250817_134116/ptq_summary_20250817_142951.csv
âœ… Experiment completed successfully!
Results saved in: adaround_lsq_mobilenet_v2_20250817_134116
------------------------------------------
ðŸŽ‰ Experiment finished!
Results directory: adaround_lsq_mobilenet_v2_20250817_134116
