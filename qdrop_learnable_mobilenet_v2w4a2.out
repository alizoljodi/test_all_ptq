ðŸš€ Starting PTQ Experiment: qdrop + learnable + mobilenet_v2
==========================================
Parameters:
  Model: mobilenet_v2
  Advanced Mode: qdrop
  Quant Model: learnable
  Weight Bits: 4
  Activation Bits: 2
  Alpha: 0.5
  Clusters: 16
  PCA dim: 50
  Batch Size: 64
  Calib Batches: 32
  Logits Batches: 10
  Alpha List: 0.2 0.4 0.6 0.8 1.0
  Clusters List: 8 16 32 64
  PCA Dim List: 25 50 100
==========================================
ðŸ”„ Running experiment...
Time: Mon Aug 18 11:11:11 AM CEST 2025
------------------------------------------
2025-08-18 11:11:15,587 | INFO | Environment: {'torch': '2.8.0+cu128', 'torchvision': '0.23.0+cu128', 'cuda_available': True, 'device_name': 'NVIDIA L40S', 'capability': '8.9', 'cudnn_version': 91002}
2025-08-18 11:11:15,587 | INFO | â–¶ START: load fp32 model (torchvision weights API)
2025-08-18 11:11:15,794 | INFO | Model: mobilenet_v2 | Weights: MobileNet_V2_Weights.IMAGENET1K_V2 | Params: 3.50M | Ref acc@1=None
2025-08-18 11:11:15,794 | INFO | âœ” END: load fp32 model (torchvision weights API) (elapsed 0.21s)
2025-08-18 11:11:15,795 | INFO | â–¶ START: build & check loaders
2025-08-18 11:11:15,801 | INFO | Val structure looks OK (1000 synset folders).
2025-08-18 11:11:15,807 | INFO | Train structure looks OK (1000 synset folders).
2025-08-18 11:12:05,959 | INFO | ImageNet train: 1281167 images, 1000 classes
2025-08-18 11:12:07,653 | INFO | ImageNet val: 50000 images, 1000 classes
2025-08-18 11:12:07,654 | INFO | Val dataset size: 50000 | batch_size=64 | calib_batches=32
2025-08-18 11:12:08,991 | INFO | [SANITY] Batch[0] stats: mean=-0.2055, std=1.1229, min=-2.118, max=2.640
2025-08-18 11:12:08,991 | INFO | âœ” END: build & check loaders (elapsed 53.20s)
2025-08-18 11:12:08,994 | INFO | â–¶ START: prepare_by_platform(Academic)
2025-08-18 11:12:08,995 | INFO | [Academic extra_config] {'extra_qconfig_dict': {'w_observer': 'MinMaxObserver', 'a_observer': 'EMAMinMaxObserver', 'w_fakequantize': 'AdaRoundFakeQuantize', 'a_fakequantize': 'FixedFakeQuantize', 'w_qscheme': {'bit': 4, 'symmetry': True, 'per_channel': True, 'pot_scale': False}, 'a_qscheme': {'bit': 2, 'symmetry': False, 'per_channel': False, 'pot_scale': False}}}
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MinMaxObserver Params: Symmetric: True / Bitwidth: 4 / Per channel: True / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: FixedFakeQuantize Params: {}
    Oberver:      EMAMinMaxObserver Params: Symmetric: False / Bitwidth: 2 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer classifier.1 to 8 bit.
[MQBENCH] INFO: Set layer features.0.0 to 8 bit.
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_1_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_1_conv_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_2_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_2_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_2_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_3_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_3_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_4_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_4_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_4_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_5_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_5_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_6_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_6_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_7_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_7_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_7_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_8_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_8_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_9_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_9_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_4_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_10_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_10_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_5_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_11_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_11_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_11_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_12_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_12_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_6_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_13_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_13_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_7_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_14_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_14_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_14_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_15_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_15_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_8_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_16_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_16_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_9_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_17_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_17_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_17_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Set classifier_0 post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant classifier_0_post_act_fake_quantizer
2025-08-18 11:12:09,162 | INFO | Modules (total): 213 -> 425
2025-08-18 11:12:09,162 | INFO | 'Quantish' modules detected after prepare: 212
2025-08-18 11:12:09,162 | INFO | â–¶ START: calibration (enable_calibration + forward)
[MQBENCH] INFO: Enable observer and Disable quantize.
2025-08-18 11:12:10,554 | INFO | [CALIB] step=1/32 seen=64 (46.1 img/s)
2025-08-18 11:12:11,014 | INFO | [CALIB] step=10/32 seen=640 (346.0 img/s)
2025-08-18 11:12:11,549 | INFO | [CALIB] step=20/32 seen=1280 (536.7 img/s)
2025-08-18 11:12:12,059 | INFO | [CALIB] step=30/32 seen=1920 (663.4 img/s)
2025-08-18 11:12:13,436 | INFO | [CALIB] total images seen: 2048
2025-08-18 11:12:13,436 | INFO | âœ” END: calibration (enable_calibration + forward) (elapsed 4.27s)
2025-08-18 11:12:13,436 | INFO | â–¶ START: advanced PTQ reconstruction
2025-08-18 11:12:15,410 | INFO | [ADV] cfg={'pattern': 'block', 'scale_lr': 4e-05, 'warm_up': 0.2, 'weight': 0.01, 'max_count': 20000, 'b_range': [20, 2], 'keep_gpu': True, 'round_mode': 'learned_hard_sigmoid', 'prob': 1.0}
2025-08-18 11:12:15,411 | INFO | [ADV] stacked tensors: 32 | total calib images: 2048
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare block reconstruction for features_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [x_post_act_fake_quantizer, features_0_0, features_0_1, features_0_2, features_0_2_post_act_fake_quantizer, features_1_conv_0_0, features_1_conv_0_1, features_1_conv_0_2, features_1_conv_0_2_post_act_fake_quantizer, features_1_conv_1, features_1_conv_2, features_1_conv_2_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, x_post_act_fake_quantizer):
    features_0_0 = getattr(getattr(self.features, "0"), "0")(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None
    features_0_1 = getattr(getattr(self.features, "0"), "1")(features_0_0);  features_0_0 = None
    features_0_2 = getattr(getattr(self.features, "0"), "2")(features_0_1);  features_0_1 = None
    features_0_2_post_act_fake_quantizer = self.features_0_2_post_act_fake_quantizer(features_0_2);  features_0_2 = None
    features_1_conv_0_0 = getattr(getattr(getattr(self.features, "1").conv, "0"), "0")(features_0_2_post_act_fake_quantizer);  features_0_2_post_act_fake_quantizer = None
    features_1_conv_0_1 = getattr(getattr(getattr(self.features, "1").conv, "0"), "1")(features_1_conv_0_0);  features_1_conv_0_0 = None
    features_1_conv_0_2 = getattr(getattr(getattr(self.features, "1").conv, "0"), "2")(features_1_conv_0_1);  features_1_conv_0_1 = None
    features_1_conv_0_2_post_act_fake_quantizer = self.features_1_conv_0_2_post_act_fake_quantizer(features_1_conv_0_2);  features_1_conv_0_2 = None
    features_1_conv_1 = getattr(getattr(self.features, "1").conv, "1")(features_1_conv_0_2_post_act_fake_quantizer);  features_1_conv_0_2_post_act_fake_quantizer = None
    features_1_conv_2 = getattr(getattr(self.features, "1").conv, "2")(features_1_conv_1);  features_1_conv_1 = None
    features_1_conv_2_post_act_fake_quantizer = self.features_1_conv_2_post_act_fake_quantizer(features_1_conv_2);  features_1_conv_2 = None
    return features_1_conv_2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_1_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_1_conv_2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
2025-08-18 11:12:22,495 | WARNING | /home/alz07xz/project/kmeans_results/MQBench/mqbench/advanced_ptq.py:220: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)
  float(total_loss), float(rec_loss), float(round_loss), b, self.count))

[MQBENCH] INFO: Total loss:	625.493 (rec:625.493, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	567.058 (rec:567.058, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	683.615 (rec:683.615, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	699.665 (rec:699.665, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	492.685 (rec:492.685, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	405.705 (rec:405.705, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	654.860 (rec:654.860, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	664.398 (rec:657.062, round:7.336)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	561.957 (rec:556.521, round:5.435)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	563.333 (rec:558.510, round:4.823)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	743.432 (rec:738.970, round:4.462)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	678.344 (rec:674.165, round:4.179)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	741.884 (rec:737.978, round:3.906)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	740.784 (rec:737.105, round:3.679)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	583.007 (rec:579.456, round:3.552)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	820.189 (rec:816.848, round:3.341)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	615.879 (rec:612.695, round:3.185)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	735.306 (rec:732.259, round:3.047)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	536.343 (rec:533.397, round:2.946)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	676.546 (rec:673.678, round:2.868)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	713.195 (rec:710.394, round:2.801)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	766.485 (rec:763.760, round:2.724)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	532.744 (rec:530.110, round:2.633)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	622.181 (rec:619.613, round:2.569)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	764.112 (rec:761.604, round:2.508)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	697.195 (rec:694.743, round:2.452)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	772.716 (rec:770.320, round:2.396)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	559.980 (rec:557.642, round:2.338)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	820.670 (rec:818.372, round:2.298)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	689.743 (rec:687.489, round:2.254)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	745.961 (rec:743.750, round:2.211)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	809.217 (rec:807.050, round:2.167)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	613.935 (rec:611.807, round:2.127)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	453.679 (rec:451.598, round:2.081)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	457.636 (rec:455.603, round:2.033)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	589.864 (rec:587.886, round:1.978)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	391.158 (rec:389.251, round:1.907)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	578.472 (rec:576.649, round:1.823)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	615.290 (rec:613.676, round:1.614)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	729.452 (rec:728.151, round:1.301)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_2_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_1_conv_2_post_act_fake_quantizer, features_2_conv_0_0, features_2_conv_0_1, features_2_conv_0_2, features_2_conv_0_2_post_act_fake_quantizer, features_2_conv_1_0, features_2_conv_1_1, features_2_conv_1_2, features_2_conv_1_2_post_act_fake_quantizer, features_2_conv_2, features_2_conv_3, features_2_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_1_conv_2_post_act_fake_quantizer):
    features_2_conv_0_0 = getattr(getattr(getattr(self.features, "2").conv, "0"), "0")(features_1_conv_2_post_act_fake_quantizer);  features_1_conv_2_post_act_fake_quantizer = None
    features_2_conv_0_1 = getattr(getattr(getattr(self.features, "2").conv, "0"), "1")(features_2_conv_0_0);  features_2_conv_0_0 = None
    features_2_conv_0_2 = getattr(getattr(getattr(self.features, "2").conv, "0"), "2")(features_2_conv_0_1);  features_2_conv_0_1 = None
    features_2_conv_0_2_post_act_fake_quantizer = self.features_2_conv_0_2_post_act_fake_quantizer(features_2_conv_0_2);  features_2_conv_0_2 = None
    features_2_conv_1_0 = getattr(getattr(getattr(self.features, "2").conv, "1"), "0")(features_2_conv_0_2_post_act_fake_quantizer);  features_2_conv_0_2_post_act_fake_quantizer = None
    features_2_conv_1_1 = getattr(getattr(getattr(self.features, "2").conv, "1"), "1")(features_2_conv_1_0);  features_2_conv_1_0 = None
    features_2_conv_1_2 = getattr(getattr(getattr(self.features, "2").conv, "1"), "2")(features_2_conv_1_1);  features_2_conv_1_1 = None
    features_2_conv_1_2_post_act_fake_quantizer = self.features_2_conv_1_2_post_act_fake_quantizer(features_2_conv_1_2);  features_2_conv_1_2 = None
    features_2_conv_2 = getattr(getattr(self.features, "2").conv, "2")(features_2_conv_1_2_post_act_fake_quantizer);  features_2_conv_1_2_post_act_fake_quantizer = None
    features_2_conv_3 = getattr(getattr(self.features, "2").conv, "3")(features_2_conv_2);  features_2_conv_2 = None
    features_2_conv_3_post_act_fake_quantizer = self.features_2_conv_3_post_act_fake_quantizer(features_2_conv_3);  features_2_conv_3 = None
    return features_2_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_2_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_2_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_2_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	881.387 (rec:881.387, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	1155.129 (rec:1155.129, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	954.436 (rec:954.436, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	1119.723 (rec:1119.723, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	1170.451 (rec:1170.451, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	983.367 (rec:983.367, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	1262.865 (rec:1262.865, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	1346.790 (rec:1316.838, round:29.953)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	1089.618 (rec:1065.010, round:24.608)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	1068.476 (rec:1046.528, round:21.948)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	1332.371 (rec:1312.490, round:19.881)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	1164.257 (rec:1146.073, round:18.184)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	1342.786 (rec:1325.909, round:16.877)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	1089.549 (rec:1073.551, round:15.998)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	1173.942 (rec:1158.655, round:15.288)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	1197.182 (rec:1182.550, round:14.632)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1410.538 (rec:1396.482, round:14.056)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	1393.806 (rec:1380.287, round:13.518)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	1145.092 (rec:1132.051, round:13.041)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	1452.294 (rec:1439.609, round:12.684)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	1047.035 (rec:1034.708, round:12.327)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	1208.401 (rec:1196.437, round:11.964)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	1546.790 (rec:1535.132, round:11.659)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1283.424 (rec:1272.056, round:11.369)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	1046.870 (rec:1035.805, round:11.065)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	1182.740 (rec:1171.999, round:10.741)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	1340.246 (rec:1329.819, round:10.427)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	1450.048 (rec:1439.884, round:10.164)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	1168.751 (rec:1158.816, round:9.935)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	1190.607 (rec:1180.917, round:9.690)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	1548.085 (rec:1538.633, round:9.452)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	1130.068 (rec:1120.897, round:9.171)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	1073.772 (rec:1064.863, round:8.909)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	1076.224 (rec:1067.590, round:8.634)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	1320.342 (rec:1312.001, round:8.340)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	1038.501 (rec:1030.458, round:8.043)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1195.444 (rec:1187.684, round:7.760)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	1375.973 (rec:1368.532, round:7.440)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	1233.859 (rec:1227.006, round:6.853)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	1295.429 (rec:1289.759, round:5.671)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_3_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_2_conv_3_post_act_fake_quantizer, features_3_conv_0_0, features_3_conv_0_1, features_3_conv_0_2, features_3_conv_0_2_post_act_fake_quantizer, features_3_conv_1_0, features_3_conv_1_1, features_3_conv_1_2, features_3_conv_1_2_post_act_fake_quantizer, features_3_conv_2, features_3_conv_3, add, add_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_2_conv_3_post_act_fake_quantizer):
    features_3_conv_0_0 = getattr(getattr(getattr(self.features, "3").conv, "0"), "0")(features_2_conv_3_post_act_fake_quantizer)
    features_3_conv_0_1 = getattr(getattr(getattr(self.features, "3").conv, "0"), "1")(features_3_conv_0_0);  features_3_conv_0_0 = None
    features_3_conv_0_2 = getattr(getattr(getattr(self.features, "3").conv, "0"), "2")(features_3_conv_0_1);  features_3_conv_0_1 = None
    features_3_conv_0_2_post_act_fake_quantizer = self.features_3_conv_0_2_post_act_fake_quantizer(features_3_conv_0_2);  features_3_conv_0_2 = None
    features_3_conv_1_0 = getattr(getattr(getattr(self.features, "3").conv, "1"), "0")(features_3_conv_0_2_post_act_fake_quantizer);  features_3_conv_0_2_post_act_fake_quantizer = None
    features_3_conv_1_1 = getattr(getattr(getattr(self.features, "3").conv, "1"), "1")(features_3_conv_1_0);  features_3_conv_1_0 = None
    features_3_conv_1_2 = getattr(getattr(getattr(self.features, "3").conv, "1"), "2")(features_3_conv_1_1);  features_3_conv_1_1 = None
    features_3_conv_1_2_post_act_fake_quantizer = self.features_3_conv_1_2_post_act_fake_quantizer(features_3_conv_1_2);  features_3_conv_1_2 = None
    features_3_conv_2 = getattr(getattr(self.features, "3").conv, "2")(features_3_conv_1_2_post_act_fake_quantizer);  features_3_conv_1_2_post_act_fake_quantizer = None
    features_3_conv_3 = getattr(getattr(self.features, "3").conv, "3")(features_3_conv_2);  features_3_conv_2 = None
    add = features_2_conv_3_post_act_fake_quantizer + features_3_conv_3;  features_2_conv_3_post_act_fake_quantizer = features_3_conv_3 = None
    add_post_act_fake_quantizer = self.add_post_act_fake_quantizer(add);  add = None
    return add_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_3_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_3_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	5661.283 (rec:5661.283, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	4639.248 (rec:4639.248, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	3955.245 (rec:3955.245, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	3879.285 (rec:3879.285, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	2961.106 (rec:2961.106, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	3467.468 (rec:3467.468, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	2501.013 (rec:2501.013, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	2518.753 (rec:2467.584, round:51.170)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	2668.752 (rec:2626.532, round:42.220)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	2534.809 (rec:2497.558, round:37.251)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	2549.467 (rec:2516.171, round:33.296)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2762.384 (rec:2732.426, round:29.957)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	2823.931 (rec:2796.844, round:27.087)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2889.292 (rec:2864.776, round:24.516)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	2445.383 (rec:2423.078, round:22.305)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	2449.342 (rec:2428.731, round:20.611)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	2256.450 (rec:2237.326, round:19.124)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	2225.317 (rec:2207.574, round:17.743)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	2753.131 (rec:2736.557, round:16.574)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	2227.106 (rec:2211.485, round:15.621)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	2219.578 (rec:2204.893, round:14.685)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	2678.430 (rec:2664.611, round:13.819)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	2869.950 (rec:2856.734, round:13.216)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	2710.834 (rec:2698.210, round:12.624)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	2166.346 (rec:2154.229, round:12.117)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	2273.463 (rec:2261.859, round:11.604)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	2284.731 (rec:2273.637, round:11.095)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	2605.198 (rec:2594.547, round:10.651)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	2557.563 (rec:2547.309, round:10.254)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	2931.792 (rec:2921.859, round:9.934)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	2256.294 (rec:2246.704, round:9.590)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	2492.807 (rec:2483.552, round:9.255)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	2908.733 (rec:2899.810, round:8.923)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	2790.183 (rec:2781.596, round:8.587)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	2786.398 (rec:2778.152, round:8.245)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	2679.035 (rec:2671.131, round:7.904)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	2473.840 (rec:2466.277, round:7.563)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2189.788 (rec:2182.600, round:7.187)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	2547.931 (rec:2541.409, round:6.522)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	2487.999 (rec:2482.818, round:5.180)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_4_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_post_act_fake_quantizer, features_4_conv_0_0, features_4_conv_0_1, features_4_conv_0_2, features_4_conv_0_2_post_act_fake_quantizer, features_4_conv_1_0, features_4_conv_1_1, features_4_conv_1_2, features_4_conv_1_2_post_act_fake_quantizer, features_4_conv_2, features_4_conv_3, features_4_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_post_act_fake_quantizer):
    features_4_conv_0_0 = getattr(getattr(getattr(self.features, "4").conv, "0"), "0")(add_post_act_fake_quantizer);  add_post_act_fake_quantizer = None
    features_4_conv_0_1 = getattr(getattr(getattr(self.features, "4").conv, "0"), "1")(features_4_conv_0_0);  features_4_conv_0_0 = None
    features_4_conv_0_2 = getattr(getattr(getattr(self.features, "4").conv, "0"), "2")(features_4_conv_0_1);  features_4_conv_0_1 = None
    features_4_conv_0_2_post_act_fake_quantizer = self.features_4_conv_0_2_post_act_fake_quantizer(features_4_conv_0_2);  features_4_conv_0_2 = None
    features_4_conv_1_0 = getattr(getattr(getattr(self.features, "4").conv, "1"), "0")(features_4_conv_0_2_post_act_fake_quantizer);  features_4_conv_0_2_post_act_fake_quantizer = None
    features_4_conv_1_1 = getattr(getattr(getattr(self.features, "4").conv, "1"), "1")(features_4_conv_1_0);  features_4_conv_1_0 = None
    features_4_conv_1_2 = getattr(getattr(getattr(self.features, "4").conv, "1"), "2")(features_4_conv_1_1);  features_4_conv_1_1 = None
    features_4_conv_1_2_post_act_fake_quantizer = self.features_4_conv_1_2_post_act_fake_quantizer(features_4_conv_1_2);  features_4_conv_1_2 = None
    features_4_conv_2 = getattr(getattr(self.features, "4").conv, "2")(features_4_conv_1_2_post_act_fake_quantizer);  features_4_conv_1_2_post_act_fake_quantizer = None
    features_4_conv_3 = getattr(getattr(self.features, "4").conv, "3")(features_4_conv_2);  features_4_conv_2 = None
    features_4_conv_3_post_act_fake_quantizer = self.features_4_conv_3_post_act_fake_quantizer(features_4_conv_3);  features_4_conv_3 = None
    return features_4_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_4_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_4_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_4_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	1615.766 (rec:1615.766, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	1523.965 (rec:1523.965, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	1527.824 (rec:1527.824, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	1308.766 (rec:1308.766, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	1303.689 (rec:1303.689, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	1314.065 (rec:1314.065, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	1250.760 (rec:1250.760, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	1409.634 (rec:1341.240, round:68.394)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	1490.976 (rec:1433.069, round:57.907)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	1396.025 (rec:1343.362, round:52.663)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	1389.429 (rec:1341.005, round:48.424)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	1278.874 (rec:1234.011, round:44.863)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	1289.157 (rec:1247.716, round:41.442)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	1347.237 (rec:1308.679, round:38.558)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	1334.108 (rec:1297.997, round:36.111)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	1481.347 (rec:1447.258, round:34.088)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1358.422 (rec:1326.198, round:32.225)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	1511.318 (rec:1480.619, round:30.700)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	1469.507 (rec:1440.226, round:29.281)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	1291.353 (rec:1263.330, round:28.023)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	1330.755 (rec:1303.866, round:26.889)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	1279.023 (rec:1253.257, round:25.766)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	1368.160 (rec:1343.403, round:24.757)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1504.161 (rec:1480.182, round:23.979)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	1361.457 (rec:1338.277, round:23.181)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	1456.121 (rec:1433.711, round:22.410)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	1254.111 (rec:1232.475, round:21.636)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	1369.495 (rec:1348.610, round:20.885)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	1343.167 (rec:1322.932, round:20.234)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	1365.251 (rec:1345.706, round:19.546)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	1321.199 (rec:1302.363, round:18.836)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	1247.566 (rec:1229.372, round:18.194)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	1305.029 (rec:1287.481, round:17.549)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	1338.705 (rec:1321.787, round:16.918)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	1316.653 (rec:1300.389, round:16.265)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	1282.530 (rec:1266.938, round:15.592)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1356.996 (rec:1342.155, round:14.842)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	1358.244 (rec:1344.289, round:13.955)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	1307.149 (rec:1294.466, round:12.683)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	1328.653 (rec:1318.069, round:10.584)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_5_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_4_conv_3_post_act_fake_quantizer, features_5_conv_0_0, features_5_conv_0_1, features_5_conv_0_2, features_5_conv_0_2_post_act_fake_quantizer, features_5_conv_1_0, features_5_conv_1_1, features_5_conv_1_2, features_5_conv_1_2_post_act_fake_quantizer, features_5_conv_2, features_5_conv_3, add_1, add_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_4_conv_3_post_act_fake_quantizer):
    features_5_conv_0_0 = getattr(getattr(getattr(self.features, "5").conv, "0"), "0")(features_4_conv_3_post_act_fake_quantizer)
    features_5_conv_0_1 = getattr(getattr(getattr(self.features, "5").conv, "0"), "1")(features_5_conv_0_0);  features_5_conv_0_0 = None
    features_5_conv_0_2 = getattr(getattr(getattr(self.features, "5").conv, "0"), "2")(features_5_conv_0_1);  features_5_conv_0_1 = None
    features_5_conv_0_2_post_act_fake_quantizer = self.features_5_conv_0_2_post_act_fake_quantizer(features_5_conv_0_2);  features_5_conv_0_2 = None
    features_5_conv_1_0 = getattr(getattr(getattr(self.features, "5").conv, "1"), "0")(features_5_conv_0_2_post_act_fake_quantizer);  features_5_conv_0_2_post_act_fake_quantizer = None
    features_5_conv_1_1 = getattr(getattr(getattr(self.features, "5").conv, "1"), "1")(features_5_conv_1_0);  features_5_conv_1_0 = None
    features_5_conv_1_2 = getattr(getattr(getattr(self.features, "5").conv, "1"), "2")(features_5_conv_1_1);  features_5_conv_1_1 = None
    features_5_conv_1_2_post_act_fake_quantizer = self.features_5_conv_1_2_post_act_fake_quantizer(features_5_conv_1_2);  features_5_conv_1_2 = None
    features_5_conv_2 = getattr(getattr(self.features, "5").conv, "2")(features_5_conv_1_2_post_act_fake_quantizer);  features_5_conv_1_2_post_act_fake_quantizer = None
    features_5_conv_3 = getattr(getattr(self.features, "5").conv, "3")(features_5_conv_2);  features_5_conv_2 = None
    add_1 = features_4_conv_3_post_act_fake_quantizer + features_5_conv_3;  features_4_conv_3_post_act_fake_quantizer = features_5_conv_3 = None
    add_1_post_act_fake_quantizer = self.add_1_post_act_fake_quantizer(add_1);  add_1 = None
    return add_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_5_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_5_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	2950.184 (rec:2950.184, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	2688.038 (rec:2688.038, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	2433.796 (rec:2433.796, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	2424.408 (rec:2424.408, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	2374.628 (rec:2374.628, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	2338.033 (rec:2338.033, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	2275.669 (rec:2275.669, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	2385.504 (rec:2300.870, round:84.634)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	2356.488 (rec:2293.978, round:62.510)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	2336.448 (rec:2282.621, round:53.828)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	2324.466 (rec:2276.810, round:47.656)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2352.864 (rec:2309.679, round:43.185)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	2350.225 (rec:2311.285, round:38.940)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2287.401 (rec:2251.642, round:35.759)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	2163.529 (rec:2130.267, round:33.262)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	2378.651 (rec:2347.569, round:31.082)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	2228.464 (rec:2199.223, round:29.241)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	2215.176 (rec:2187.509, round:27.667)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	2219.820 (rec:2193.662, round:26.159)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	2211.195 (rec:2186.422, round:24.772)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	2371.501 (rec:2347.775, round:23.726)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	2483.319 (rec:2460.630, round:22.689)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	2324.547 (rec:2302.895, round:21.652)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	2278.678 (rec:2258.051, round:20.627)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	2194.438 (rec:2174.666, round:19.772)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	2228.367 (rec:2209.358, round:19.009)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	2361.262 (rec:2343.074, round:18.188)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	2341.741 (rec:2324.372, round:17.370)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	2332.535 (rec:2315.909, round:16.627)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	2243.970 (rec:2228.113, round:15.857)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	2309.424 (rec:2294.229, round:15.195)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	2179.698 (rec:2165.209, round:14.490)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	2420.592 (rec:2406.796, round:13.795)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	2151.764 (rec:2138.645, round:13.119)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	2263.811 (rec:2251.309, round:12.502)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	2297.214 (rec:2285.345, round:11.869)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	2487.368 (rec:2476.191, round:11.177)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2141.435 (rec:2130.949, round:10.486)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	2276.214 (rec:2266.825, round:9.388)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	2238.859 (rec:2231.562, round:7.297)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_6_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_1_post_act_fake_quantizer, features_6_conv_0_0, features_6_conv_0_1, features_6_conv_0_2, features_6_conv_0_2_post_act_fake_quantizer, features_6_conv_1_0, features_6_conv_1_1, features_6_conv_1_2, features_6_conv_1_2_post_act_fake_quantizer, features_6_conv_2, features_6_conv_3, add_2, add_2_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_1_post_act_fake_quantizer):
    features_6_conv_0_0 = getattr(getattr(getattr(self.features, "6").conv, "0"), "0")(add_1_post_act_fake_quantizer)
    features_6_conv_0_1 = getattr(getattr(getattr(self.features, "6").conv, "0"), "1")(features_6_conv_0_0);  features_6_conv_0_0 = None
    features_6_conv_0_2 = getattr(getattr(getattr(self.features, "6").conv, "0"), "2")(features_6_conv_0_1);  features_6_conv_0_1 = None
    features_6_conv_0_2_post_act_fake_quantizer = self.features_6_conv_0_2_post_act_fake_quantizer(features_6_conv_0_2);  features_6_conv_0_2 = None
    features_6_conv_1_0 = getattr(getattr(getattr(self.features, "6").conv, "1"), "0")(features_6_conv_0_2_post_act_fake_quantizer);  features_6_conv_0_2_post_act_fake_quantizer = None
    features_6_conv_1_1 = getattr(getattr(getattr(self.features, "6").conv, "1"), "1")(features_6_conv_1_0);  features_6_conv_1_0 = None
    features_6_conv_1_2 = getattr(getattr(getattr(self.features, "6").conv, "1"), "2")(features_6_conv_1_1);  features_6_conv_1_1 = None
    features_6_conv_1_2_post_act_fake_quantizer = self.features_6_conv_1_2_post_act_fake_quantizer(features_6_conv_1_2);  features_6_conv_1_2 = None
    features_6_conv_2 = getattr(getattr(self.features, "6").conv, "2")(features_6_conv_1_2_post_act_fake_quantizer);  features_6_conv_1_2_post_act_fake_quantizer = None
    features_6_conv_3 = getattr(getattr(self.features, "6").conv, "3")(features_6_conv_2);  features_6_conv_2 = None
    add_2 = add_1_post_act_fake_quantizer + features_6_conv_3;  add_1_post_act_fake_quantizer = features_6_conv_3 = None
    add_2_post_act_fake_quantizer = self.add_2_post_act_fake_quantizer(add_2);  add_2 = None
    return add_2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_6_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_6_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	3961.501 (rec:3961.501, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	3752.890 (rec:3752.890, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	3585.042 (rec:3585.042, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	3738.132 (rec:3738.132, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	3442.901 (rec:3442.901, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	3446.896 (rec:3446.896, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	3337.434 (rec:3337.434, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	3444.226 (rec:3369.902, round:74.325)	b=20.00	count=4000
