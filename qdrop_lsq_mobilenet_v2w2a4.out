ðŸš€ Starting PTQ Experiment: qdrop + lsq + mobilenet_v2
==========================================
Parameters:
  Model: mobilenet_v2
  Advanced Mode: qdrop
  Quant Model: lsq
  Weight Bits: 2
  Activation Bits: 4
  Alpha: 0.5
  Clusters: 16
  PCA dim: 50
  Batch Size: 64
  Calib Batches: 32
  Logits Batches: 10
  Alpha List: 0.2 0.4 0.6 0.8 1.0
  Clusters List: 8 16 32 64
  PCA Dim List: 25 50 100
==========================================
ðŸ”„ Running experiment...
Time: Mon Aug 18 11:13:47 AM CEST 2025
------------------------------------------
2025-08-18 11:13:59,831 | INFO | Environment: {'torch': '2.8.0+cu128', 'torchvision': '0.23.0+cu128', 'cuda_available': True, 'device_name': 'NVIDIA L40S', 'capability': '8.9', 'cudnn_version': 91002}
2025-08-18 11:13:59,832 | INFO | â–¶ START: load fp32 model (torchvision weights API)
2025-08-18 11:14:00,435 | INFO | Model: mobilenet_v2 | Weights: MobileNet_V2_Weights.IMAGENET1K_V2 | Params: 3.50M | Ref acc@1=None
2025-08-18 11:14:00,436 | INFO | âœ” END: load fp32 model (torchvision weights API) (elapsed 0.60s)
2025-08-18 11:14:00,436 | INFO | â–¶ START: build & check loaders
2025-08-18 11:14:00,446 | INFO | Val structure looks OK (1000 synset folders).
2025-08-18 11:14:00,457 | INFO | Train structure looks OK (1000 synset folders).
2025-08-18 11:14:25,044 | INFO | ImageNet train: 1281167 images, 1000 classes
2025-08-18 11:14:26,924 | INFO | ImageNet val: 50000 images, 1000 classes
2025-08-18 11:14:26,924 | INFO | Val dataset size: 50000 | batch_size=64 | calib_batches=32
2025-08-18 11:14:33,774 | INFO | [SANITY] Batch[0] stats: mean=-0.2055, std=1.1229, min=-2.118, max=2.640
2025-08-18 11:14:33,774 | INFO | âœ” END: build & check loaders (elapsed 33.34s)
2025-08-18 11:14:33,782 | INFO | â–¶ START: prepare_by_platform(Academic)
2025-08-18 11:14:33,783 | INFO | [Academic extra_config] {'extra_qconfig_dict': {'w_observer': 'MinMaxObserver', 'a_observer': 'EMAMinMaxObserver', 'w_fakequantize': 'AdaRoundFakeQuantize', 'a_fakequantize': 'FixedFakeQuantize', 'w_qscheme': {'bit': 2, 'symmetry': True, 'per_channel': True, 'pot_scale': False}, 'a_qscheme': {'bit': 4, 'symmetry': False, 'per_channel': False, 'pot_scale': False}}}
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MinMaxObserver Params: Symmetric: True / Bitwidth: 2 / Per channel: True / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: FixedFakeQuantize Params: {}
    Oberver:      EMAMinMaxObserver Params: Symmetric: False / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer classifier.1 to 8 bit.
[MQBENCH] INFO: Set layer features.0.0 to 8 bit.
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_1_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_1_conv_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_2_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_2_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_2_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_3_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_3_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_4_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_4_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_4_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_5_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_5_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_6_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_6_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_7_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_7_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_7_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_8_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_8_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_9_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_9_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_4_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_10_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_10_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_5_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_11_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_11_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_11_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_12_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_12_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_6_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_13_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_13_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_7_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_14_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_14_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_14_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_15_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_15_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_8_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_16_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_16_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant add_9_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_17_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_17_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant features_17_conv_3_post_act_fake_quantizer
[MQBENCH] INFO: Set classifier_0 post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant classifier_0_post_act_fake_quantizer
2025-08-18 11:14:34,404 | INFO | Modules (total): 213 -> 425
2025-08-18 11:14:34,404 | INFO | 'Quantish' modules detected after prepare: 212
2025-08-18 11:14:34,404 | INFO | â–¶ START: calibration (enable_calibration + forward)
[MQBENCH] INFO: Enable observer and Disable quantize.
2025-08-18 11:14:43,587 | INFO | [CALIB] step=1/32 seen=64 (7.0 img/s)
2025-08-18 11:14:44,116 | INFO | [CALIB] step=10/32 seen=640 (65.9 img/s)
2025-08-18 11:14:47,121 | INFO | [CALIB] step=20/32 seen=1280 (100.7 img/s)
2025-08-18 11:14:49,001 | INFO | [CALIB] step=30/32 seen=1920 (131.6 img/s)
2025-08-18 11:14:52,693 | INFO | [CALIB] total images seen: 2048
2025-08-18 11:14:52,693 | INFO | âœ” END: calibration (enable_calibration + forward) (elapsed 18.29s)
2025-08-18 11:14:52,693 | INFO | â–¶ START: advanced PTQ reconstruction
2025-08-18 11:14:54,705 | INFO | [ADV] cfg={'pattern': 'block', 'scale_lr': 4e-05, 'warm_up': 0.2, 'weight': 0.01, 'max_count': 20000, 'b_range': [20, 2], 'keep_gpu': True, 'round_mode': 'learned_hard_sigmoid', 'prob': 1.0}
2025-08-18 11:14:54,705 | INFO | [ADV] stacked tensors: 32 | total calib images: 2048
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare block reconstruction for features_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [x_post_act_fake_quantizer, features_0_0, features_0_1, features_0_2, features_0_2_post_act_fake_quantizer, features_1_conv_0_0, features_1_conv_0_1, features_1_conv_0_2, features_1_conv_0_2_post_act_fake_quantizer, features_1_conv_1, features_1_conv_2, features_1_conv_2_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, x_post_act_fake_quantizer):
    features_0_0 = getattr(getattr(self.features, "0"), "0")(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None
    features_0_1 = getattr(getattr(self.features, "0"), "1")(features_0_0);  features_0_0 = None
    features_0_2 = getattr(getattr(self.features, "0"), "2")(features_0_1);  features_0_1 = None
    features_0_2_post_act_fake_quantizer = self.features_0_2_post_act_fake_quantizer(features_0_2);  features_0_2 = None
    features_1_conv_0_0 = getattr(getattr(getattr(self.features, "1").conv, "0"), "0")(features_0_2_post_act_fake_quantizer);  features_0_2_post_act_fake_quantizer = None
    features_1_conv_0_1 = getattr(getattr(getattr(self.features, "1").conv, "0"), "1")(features_1_conv_0_0);  features_1_conv_0_0 = None
    features_1_conv_0_2 = getattr(getattr(getattr(self.features, "1").conv, "0"), "2")(features_1_conv_0_1);  features_1_conv_0_1 = None
    features_1_conv_0_2_post_act_fake_quantizer = self.features_1_conv_0_2_post_act_fake_quantizer(features_1_conv_0_2);  features_1_conv_0_2 = None
    features_1_conv_1 = getattr(getattr(self.features, "1").conv, "1")(features_1_conv_0_2_post_act_fake_quantizer);  features_1_conv_0_2_post_act_fake_quantizer = None
    features_1_conv_2 = getattr(getattr(self.features, "1").conv, "2")(features_1_conv_1);  features_1_conv_1 = None
    features_1_conv_2_post_act_fake_quantizer = self.features_1_conv_2_post_act_fake_quantizer(features_1_conv_2);  features_1_conv_2 = None
    return features_1_conv_2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_1_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_1_conv_2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
2025-08-18 11:15:05,618 | WARNING | /home/alz07xz/project/kmeans_results/MQBench/mqbench/advanced_ptq.py:220: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)
  float(total_loss), float(rec_loss), float(round_loss), b, self.count))

[MQBENCH] INFO: Total loss:	154.191 (rec:154.191, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	137.916 (rec:137.916, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	139.845 (rec:139.845, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	137.399 (rec:137.399, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	116.319 (rec:116.319, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	116.874 (rec:116.874, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	129.702 (rec:129.702, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	142.234 (rec:130.072, round:12.162)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	128.611 (rec:118.755, round:9.856)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	127.742 (rec:118.513, round:9.229)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	143.102 (rec:134.528, round:8.574)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	136.816 (rec:128.756, round:8.060)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	141.647 (rec:133.954, round:7.692)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	142.217 (rec:134.830, round:7.388)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	133.999 (rec:126.889, round:7.110)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	144.822 (rec:137.932, round:6.890)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	132.184 (rec:125.467, round:6.717)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	141.034 (rec:134.475, round:6.559)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	119.646 (rec:113.211, round:6.436)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	133.762 (rec:127.484, round:6.277)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	137.729 (rec:131.557, round:6.172)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	143.885 (rec:137.822, round:6.063)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	120.391 (rec:114.408, round:5.983)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	131.215 (rec:125.335, round:5.880)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	143.531 (rec:137.800, round:5.732)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	138.290 (rec:132.676, round:5.614)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	137.442 (rec:131.911, round:5.531)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	119.386 (rec:113.958, round:5.427)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	142.316 (rec:136.984, round:5.332)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	136.527 (rec:131.281, round:5.246)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	138.252 (rec:133.125, round:5.127)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	142.401 (rec:137.405, round:4.996)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	127.064 (rec:122.197, round:4.867)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	112.985 (rec:108.277, round:4.708)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	120.088 (rec:115.601, round:4.486)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	130.852 (rec:126.550, round:4.302)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	112.258 (rec:108.160, round:4.098)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	130.137 (rec:126.260, round:3.877)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	126.166 (rec:122.625, round:3.541)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	134.053 (rec:130.985, round:3.068)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_2_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_1_conv_2_post_act_fake_quantizer, features_2_conv_0_0, features_2_conv_0_1, features_2_conv_0_2, features_2_conv_0_2_post_act_fake_quantizer, features_2_conv_1_0, features_2_conv_1_1, features_2_conv_1_2, features_2_conv_1_2_post_act_fake_quantizer, features_2_conv_2, features_2_conv_3, features_2_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_1_conv_2_post_act_fake_quantizer):
    features_2_conv_0_0 = getattr(getattr(getattr(self.features, "2").conv, "0"), "0")(features_1_conv_2_post_act_fake_quantizer);  features_1_conv_2_post_act_fake_quantizer = None
    features_2_conv_0_1 = getattr(getattr(getattr(self.features, "2").conv, "0"), "1")(features_2_conv_0_0);  features_2_conv_0_0 = None
    features_2_conv_0_2 = getattr(getattr(getattr(self.features, "2").conv, "0"), "2")(features_2_conv_0_1);  features_2_conv_0_1 = None
    features_2_conv_0_2_post_act_fake_quantizer = self.features_2_conv_0_2_post_act_fake_quantizer(features_2_conv_0_2);  features_2_conv_0_2 = None
    features_2_conv_1_0 = getattr(getattr(getattr(self.features, "2").conv, "1"), "0")(features_2_conv_0_2_post_act_fake_quantizer);  features_2_conv_0_2_post_act_fake_quantizer = None
    features_2_conv_1_1 = getattr(getattr(getattr(self.features, "2").conv, "1"), "1")(features_2_conv_1_0);  features_2_conv_1_0 = None
    features_2_conv_1_2 = getattr(getattr(getattr(self.features, "2").conv, "1"), "2")(features_2_conv_1_1);  features_2_conv_1_1 = None
    features_2_conv_1_2_post_act_fake_quantizer = self.features_2_conv_1_2_post_act_fake_quantizer(features_2_conv_1_2);  features_2_conv_1_2 = None
    features_2_conv_2 = getattr(getattr(self.features, "2").conv, "2")(features_2_conv_1_2_post_act_fake_quantizer);  features_2_conv_1_2_post_act_fake_quantizer = None
    features_2_conv_3 = getattr(getattr(self.features, "2").conv, "3")(features_2_conv_2);  features_2_conv_2 = None
    features_2_conv_3_post_act_fake_quantizer = self.features_2_conv_3_post_act_fake_quantizer(features_2_conv_3);  features_2_conv_3 = None
    return features_2_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_2_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_2_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_2_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	340.874 (rec:340.874, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	316.898 (rec:316.898, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	269.399 (rec:269.399, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	263.005 (rec:263.005, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	253.025 (rec:253.025, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	234.089 (rec:234.089, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	261.619 (rec:261.619, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	297.914 (rec:259.781, round:38.133)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	271.439 (rec:237.021, round:34.418)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	259.519 (rec:226.524, round:32.995)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	244.870 (rec:212.779, round:32.091)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	275.996 (rec:244.683, round:31.313)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	284.168 (rec:253.669, round:30.500)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	242.696 (rec:212.859, round:29.836)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	271.495 (rec:242.174, round:29.321)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	261.922 (rec:233.116, round:28.807)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	269.414 (rec:241.110, round:28.304)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	278.869 (rec:251.055, round:27.814)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	268.001 (rec:240.670, round:27.331)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	267.863 (rec:240.920, round:26.943)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	235.828 (rec:209.244, round:26.584)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	268.148 (rec:241.959, round:26.189)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	274.148 (rec:248.261, round:25.887)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	266.151 (rec:240.589, round:25.562)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	234.213 (rec:208.978, round:25.235)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	263.274 (rec:238.368, round:24.906)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	272.220 (rec:247.691, round:24.530)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	261.627 (rec:237.446, round:24.181)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	262.418 (rec:238.547, round:23.871)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	252.346 (rec:228.833, round:23.514)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	285.888 (rec:262.749, round:23.140)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	255.187 (rec:232.442, round:22.744)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	245.010 (rec:222.721, round:22.289)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	242.138 (rec:220.329, round:21.809)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	266.607 (rec:245.294, round:21.313)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	250.311 (rec:229.558, round:20.753)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	249.486 (rec:229.394, round:20.093)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	272.716 (rec:253.426, round:19.290)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	257.455 (rec:239.182, round:18.272)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	258.812 (rec:242.178, round:16.634)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_3_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [features_2_conv_3_post_act_fake_quantizer, features_3_conv_0_0, features_3_conv_0_1, features_3_conv_0_2, features_3_conv_0_2_post_act_fake_quantizer, features_3_conv_1_0, features_3_conv_1_1, features_3_conv_1_2, features_3_conv_1_2_post_act_fake_quantizer, features_3_conv_2, features_3_conv_3, add, add_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, features_2_conv_3_post_act_fake_quantizer):
    features_3_conv_0_0 = getattr(getattr(getattr(self.features, "3").conv, "0"), "0")(features_2_conv_3_post_act_fake_quantizer)
    features_3_conv_0_1 = getattr(getattr(getattr(self.features, "3").conv, "0"), "1")(features_3_conv_0_0);  features_3_conv_0_0 = None
    features_3_conv_0_2 = getattr(getattr(getattr(self.features, "3").conv, "0"), "2")(features_3_conv_0_1);  features_3_conv_0_1 = None
    features_3_conv_0_2_post_act_fake_quantizer = self.features_3_conv_0_2_post_act_fake_quantizer(features_3_conv_0_2);  features_3_conv_0_2 = None
    features_3_conv_1_0 = getattr(getattr(getattr(self.features, "3").conv, "1"), "0")(features_3_conv_0_2_post_act_fake_quantizer);  features_3_conv_0_2_post_act_fake_quantizer = None
    features_3_conv_1_1 = getattr(getattr(getattr(self.features, "3").conv, "1"), "1")(features_3_conv_1_0);  features_3_conv_1_0 = None
    features_3_conv_1_2 = getattr(getattr(getattr(self.features, "3").conv, "1"), "2")(features_3_conv_1_1);  features_3_conv_1_1 = None
    features_3_conv_1_2_post_act_fake_quantizer = self.features_3_conv_1_2_post_act_fake_quantizer(features_3_conv_1_2);  features_3_conv_1_2 = None
    features_3_conv_2 = getattr(getattr(self.features, "3").conv, "2")(features_3_conv_1_2_post_act_fake_quantizer);  features_3_conv_1_2_post_act_fake_quantizer = None
    features_3_conv_3 = getattr(getattr(self.features, "3").conv, "3")(features_3_conv_2);  features_3_conv_2 = None
    add = features_2_conv_3_post_act_fake_quantizer + features_3_conv_3;  features_2_conv_3_post_act_fake_quantizer = features_3_conv_3 = None
    add_post_act_fake_quantizer = self.add_post_act_fake_quantizer(add);  add = None
    return add_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_3_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_3_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for add_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	1078.452 (rec:1078.452, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	978.416 (rec:978.416, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	820.504 (rec:820.504, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	977.764 (rec:977.764, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	827.295 (rec:827.295, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	975.172 (rec:975.172, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	660.004 (rec:660.004, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	729.747 (rec:664.221, round:65.526)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	901.212 (rec:841.323, round:59.889)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	849.628 (rec:792.247, round:57.381)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	857.523 (rec:801.949, round:55.574)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	891.933 (rec:838.091, round:53.842)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	902.521 (rec:850.311, round:52.210)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	925.481 (rec:874.745, round:50.736)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	836.537 (rec:787.080, round:49.457)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	814.349 (rec:766.134, round:48.216)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	769.286 (rec:722.229, round:47.057)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	794.712 (rec:748.670, round:46.042)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	897.136 (rec:852.148, round:44.988)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	789.980 (rec:746.115, round:43.865)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	662.646 (rec:619.894, round:42.753)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	857.184 (rec:815.467, round:41.716)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	900.472 (rec:859.766, round:40.706)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	893.641 (rec:853.811, round:39.830)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	644.212 (rec:605.196, round:39.016)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	752.625 (rec:714.471, round:38.154)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	777.783 (rec:740.384, round:37.399)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	821.908 (rec:785.213, round:36.695)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	804.199 (rec:768.250, round:35.949)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	881.301 (rec:846.089, round:35.212)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	744.039 (rec:709.610, round:34.429)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	830.570 (rec:796.939, round:33.631)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	879.711 (rec:846.889, round:32.822)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	860.541 (rec:828.609, round:31.932)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	859.560 (rec:828.524, round:31.036)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	871.405 (rec:841.358, round:30.048)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	823.654 (rec:794.743, round:28.911)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	641.562 (rec:613.966, round:27.595)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	792.731 (rec:766.743, round:25.988)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	795.316 (rec:771.793, round:23.523)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for features_4_conv_0_0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [add_post_act_fake_quantizer, features_4_conv_0_0, features_4_conv_0_1, features_4_conv_0_2, features_4_conv_0_2_post_act_fake_quantizer, features_4_conv_1_0, features_4_conv_1_1, features_4_conv_1_2, features_4_conv_1_2_post_act_fake_quantizer, features_4_conv_2, features_4_conv_3, features_4_conv_3_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, add_post_act_fake_quantizer):
    features_4_conv_0_0 = getattr(getattr(getattr(self.features, "4").conv, "0"), "0")(add_post_act_fake_quantizer);  add_post_act_fake_quantizer = None
    features_4_conv_0_1 = getattr(getattr(getattr(self.features, "4").conv, "0"), "1")(features_4_conv_0_0);  features_4_conv_0_0 = None
    features_4_conv_0_2 = getattr(getattr(getattr(self.features, "4").conv, "0"), "2")(features_4_conv_0_1);  features_4_conv_0_1 = None
    features_4_conv_0_2_post_act_fake_quantizer = self.features_4_conv_0_2_post_act_fake_quantizer(features_4_conv_0_2);  features_4_conv_0_2 = None
    features_4_conv_1_0 = getattr(getattr(getattr(self.features, "4").conv, "1"), "0")(features_4_conv_0_2_post_act_fake_quantizer);  features_4_conv_0_2_post_act_fake_quantizer = None
    features_4_conv_1_1 = getattr(getattr(getattr(self.features, "4").conv, "1"), "1")(features_4_conv_1_0);  features_4_conv_1_0 = None
    features_4_conv_1_2 = getattr(getattr(getattr(self.features, "4").conv, "1"), "2")(features_4_conv_1_1);  features_4_conv_1_1 = None
    features_4_conv_1_2_post_act_fake_quantizer = self.features_4_conv_1_2_post_act_fake_quantizer(features_4_conv_1_2);  features_4_conv_1_2 = None
    features_4_conv_2 = getattr(getattr(self.features, "4").conv, "2")(features_4_conv_1_2_post_act_fake_quantizer);  features_4_conv_1_2_post_act_fake_quantizer = None
    features_4_conv_3 = getattr(getattr(self.features, "4").conv, "3")(features_4_conv_2);  features_4_conv_2 = None
    features_4_conv_3_post_act_fake_quantizer = self.features_4_conv_3_post_act_fake_quantizer(features_4_conv_3);  features_4_conv_3 = None
    return features_4_conv_3_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for features_4_conv_0_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_4_conv_1_2_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for features_4_conv_3_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	601.774 (rec:601.774, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	541.738 (rec:541.738, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	547.596 (rec:547.596, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	507.946 (rec:507.946, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	490.108 (rec:490.108, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	428.232 (rec:428.232, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	476.716 (rec:476.716, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	566.572 (rec:486.671, round:79.901)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	580.047 (rec:506.579, round:73.468)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	549.502 (rec:478.325, round:71.177)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	548.856 (rec:479.386, round:69.470)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	518.197 (rec:450.265, round:67.932)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	520.642 (rec:454.099, round:66.543)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	510.797 (rec:445.548, round:65.248)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	490.355 (rec:426.190, round:64.165)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	547.740 (rec:484.683, round:63.057)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	526.069 (rec:464.093, round:61.976)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	541.650 (rec:480.755, round:60.895)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	539.847 (rec:479.944, round:59.903)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	502.807 (rec:443.841, round:58.966)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	495.141 (rec:437.114, round:58.027)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	499.647 (rec:442.602, round:57.045)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	523.886 (rec:467.786, round:56.100)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	531.451 (rec:476.207, round:55.244)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	500.706 (rec:446.302, round:54.405)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	527.635 (rec:474.066, round:53.570)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	440.544 (rec:387.762, round:52.782)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	515.778 (rec:463.899, round:51.879)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	487.946 (rec:436.924, round:51.022)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	481.787 (rec:431.650, round:50.137)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	493.631 (rec:444.385, round:49.246)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	434.642 (rec:386.315, round:48.327)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	485.543 (rec:438.174, round:47.368)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	482.030 (rec:435.738, round:46.292)	b=5.38	count=17000
